{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 16:50:53.313285: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-15 16:50:54.072713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-09-15 16:50:54.746225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 16:50:54.772900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 16:50:54.773115: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 16:50:54.774066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 16:50:54.774279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 16:50:54.774418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 16:50:54.823464: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 16:50:54.823675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 16:50:54.823835: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-15 16:50:54.823949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3127 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "#\n",
    "import tensorflow as tf\n",
    "# limit gpu mem\n",
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.set_logical_device_configuration(gpus[0],[tf.config.LogicalDeviceConfiguration(memory_limit=3700)])\n",
    "#\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation, AveragePooling2D, BatchNormalization, Conv2D, Conv2DTranspose, GaussianNoise,\n",
    "    Dense, Dropout, Flatten, Input, LeakyReLU, ReLU, UpSampling2D)\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "#from keras.models import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Conv2D\n",
    "#\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from tensorflow.keras.utils import load_img,save_img\n",
    "#from keras.utils.vis_utils import plot_model\n",
    "from copy import deepcopy\n",
    "\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2hsv, hsv2rgb\n",
    "from PIL import Image\n",
    "\n",
    "convstride_enc=np.array([[[3,3],[1,1]],[[3,3],[2,2]],[[3,3],[2,2]],[[3,3],[2,2]],[[3,3],[2,2]],[[3,3],[2,2]]])\n",
    "convstride_dec=np.array([[[3,3],[1,1]],[[3,3],[2,2]],[[3,3],[2,2]],[[3,3],[2,2]],[[3,3],[2,2]],[[3,3],[2,2]]])  \n",
    "convstride_dis=np.array([[[3,3],[2,2]],[[3,3],[2,2]],[[3,3],[2,2]],[[3,3],[2,2]],[[3,3],[2,2]],[[3,3],[2,2]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conversion lab & rgb\n",
    "def lab_to_rgb(img_lab):\n",
    "    img_lab = img_lab.numpy()\n",
    "    L = (img_lab[:,:,:,0] + 1.) * 50. \n",
    "    L = L[..., np.newaxis]\n",
    "    ab = img_lab[:,:,:,1:]  * 110\n",
    "    img_lab = np.concatenate((L,ab),axis=3)\n",
    "    #\n",
    "    img = lab2rgb(img_lab).astype(\"float32\")\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "def rgb_to_lab(img):\n",
    "    img_lab = rgb2lab(img).astype(\"float32\") # Converting RGB to L*a*b    # float16 to optimize ?\n",
    "    # Between -1 and 1\n",
    "    L  = img_lab[:,:,0]  / 50. - 1. \n",
    "    ab = img_lab[:,:,1:]  / 110. # Between -1 and 1 \n",
    "    return L, ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJvTBV-p8feY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define an encoder block\n",
    "def define_encoder_block(layer_in,name, n_filters,kernel=(3,3),strides=(2,2), batchnorm=True, pad='same'):\n",
    "        # weight initialization\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        # padding\n",
    "        if (pad == 'valid'):\n",
    "            layer_in = tf.pad(layer_in, [[0, 0], [1, 1], [1, 1], [0, 0]], \"REFLECT\")\n",
    "        # add downsampling layer\n",
    "        g = Conv2D(n_filters, kernel, strides=strides, padding=pad, kernel_initializer=init,name=name)(layer_in)\n",
    "        # conditionally add batch normalization\n",
    "        if batchnorm:\n",
    "            g = BatchNormalization()(g, training=True)\n",
    "        # leaky relu activation\n",
    "        g = LeakyReLU(alpha=0.2)(g)\n",
    "        return g\n",
    "\n",
    "# define a decoder block\n",
    "def decoder_block(layer_in, skip_in,name, n_filters,kernel=(3,3),strides=(2,2), dropout=True):\n",
    "        # weight initialization\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        # add upsampling layer\n",
    "        g = Conv2DTranspose(n_filters, kernel, strides=strides, padding='same', kernel_initializer=init,name=name)(layer_in)\n",
    "        # add batch normalization\n",
    "        g = BatchNormalization()(g, training=True)\n",
    "        # conditionally add dropout\n",
    "        if dropout:\n",
    "            g = Dropout(0.3)(g, training=True)\n",
    "        # merge with skip connection\n",
    "        g = Concatenate()([g, skip_in])\n",
    "        # relu activation\n",
    "        g = Activation('relu')(g)\n",
    "        return g\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(image_shape):   # input L : 480x640x1, output ab : 480x640x1 \n",
    "        convstrideg=convstride_enc.copy()\n",
    "        print(convstrideg.shape)\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        # image input\n",
    "        in_image = Input(shape=image_shape)\n",
    "        # encoder model\n",
    "        (conv,stride),convstrideg=convstrideg[0,:,:],convstrideg[1:,:,:]\n",
    "        e0 = define_encoder_block(in_image,\"encoder_block_0\", 64, kernel=conv,strides=stride, pad='valid') # 64  pad='valid'\n",
    "        \n",
    "        (conv,stride),convstrideg=convstrideg[0,:,:],convstrideg[1:,:,:]\n",
    "        e1 = define_encoder_block(e0,\"encoder_block_1\", 128, kernel=conv,strides=stride) # 64\n",
    "        \n",
    "        (conv,stride),convstrideg=convstrideg[0,:,:],convstrideg[1:,:,:]\n",
    "        e2 = define_encoder_block(e1,\"encoder_block_2\", 256,kernel=conv,strides=stride) # 16\n",
    "        \n",
    "        (conv,stride),convstrideg=convstrideg[0,:,:],convstrideg[1:,:,:]\n",
    "        e3 = define_encoder_block(e2,\"encoder_block_3\", 512,kernel=conv,strides=stride) # 16\n",
    "        \n",
    "        (conv,stride),convstrideg=convstrideg[0,:,:],convstrideg[1:,:,:]\n",
    "        e4 = define_encoder_block(e3,\"encoder_block_4\", 512,kernel=conv,strides=stride) # 16\n",
    "        # bottleneck, no batch norm and relu\n",
    "        (conv,stride),convstrideg=convstrideg[0,:,:],convstrideg[1:,:,:]\n",
    "        b = Conv2D(512, conv,strides=stride, padding='same', kernel_initializer=init,name=\"bottleneck_conv2d\")(e4)\n",
    "        b = Activation('relu')(b)\n",
    "        # decoder model\n",
    "        convstrideg=convstride_dec.copy()\n",
    "        (conv,stride),convstrideg=convstrideg[-1,:,:],convstrideg[:-1,:,:]\n",
    "        d4 = decoder_block(b, e4,\"decoder_block_4\", 512,kernel=conv,strides=stride, dropout=False)\n",
    "        (conv,stride),convstrideg=convstrideg[-1,:,:],convstrideg[:-1,:,:]\n",
    "        d5 = decoder_block(d4, e3,\"decoder_block_3\",512,kernel=conv,strides=stride, dropout=False)\n",
    "        (conv,stride),convstrideg=convstrideg[-1,:,:],convstrideg[:-1,:,:]\n",
    "        d6 = decoder_block(d5, e2,\"decoder_block_2\", 256,kernel=conv,strides=stride, dropout=False)\n",
    "        (conv,stride),convstrideg=convstrideg[-1,:,:],convstrideg[:-1,:,:]\n",
    "        d7 = decoder_block(d6, e1,\"decoder_block_1\", 128,kernel=conv,strides=stride, dropout=False)\n",
    "        (conv,stride),convstrideg=convstrideg[-1,:,:],convstrideg[:-1,:,:]\n",
    "        d8 = decoder_block(d7, e0,\"decoder_block_0\", 64,kernel=conv,strides=stride, dropout=False)\n",
    "        # output\n",
    "        (conv,stride),convstrideg=convstrideg[-1,:,:],convstrideg[:-1,:,:]\n",
    "        g = tf.pad(d8, [[0, 0], [1, 1], [1, 1], [0, 0]], \"REFLECT\")\n",
    "        g = Conv2D(3, conv,strides=stride, padding='valid',kernel_initializer=init, name=\"3rd_last_layer\")(g)    \n",
    "        out_image = Activation('tanh')(g)\n",
    "        \n",
    "        # define model\n",
    "        model = Model(in_image, out_image)\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "\n",
    "# define the discriminator model\n",
    "def define_discriminator(image_shape):    # input concatenate(L,ab) : 480x640x3, output : 1,0\n",
    "        convstrided=convstride_dis.copy()\n",
    "        print(convstrided.shape)\n",
    "        # weight initialization\n",
    "        init = RandomNormal(stddev=0.02)\n",
    "        # source image input\n",
    "        in_src_image = Input(shape=image_shape)\n",
    "        # C64\n",
    "        (conv,stride),convstrided=convstrided[0,:,:],convstrided[1:,:,:]\n",
    "        d = Conv2D(32, conv, strides=stride, padding='same', kernel_initializer=init)(in_src_image)\n",
    "        #d = tf.keras.layers.GaussianNoise(stddev= 0.1)(d)\n",
    "        d = Dropout(0.3)(d, training=True)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        # C128\n",
    "        (conv,stride),convstrided=convstrided[0,:,:],convstrided[1:,:,:]\n",
    "        d = Conv2D(64, conv, strides=stride, padding='same', kernel_initializer=init)(d)\n",
    "        d = Dropout(0.3)(d, training=True)\n",
    "        d = BatchNormalization()(d)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        # C256\n",
    "        (conv,stride),convstrided=convstrided[0,:,:],convstrided[1:,:,:]\n",
    "        d = Conv2D(128, conv, strides=stride, padding='same', kernel_initializer=init)(d)\n",
    "        d = Dropout(0.3)(d, training=True)\n",
    "        d = BatchNormalization()(d)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        # second last output layer\n",
    "        (conv,stride),convstrided=convstrided[0,:,:],convstrided[1:,:,:]\n",
    "        d = Conv2D(256, conv, strides=stride, padding='same', kernel_initializer=init)(d)\n",
    "        d = Dropout(0.3)(d, training=True)\n",
    "        d = BatchNormalization()(d)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        # last output layer\n",
    "        (conv,stride),convstrided=convstrided[0,:,:],convstrided[1:,:,:]\n",
    "        d = Conv2D(512, conv, strides=stride, padding='same', kernel_initializer=init)(d)\n",
    "        d = Dropout(0.3)(d, training=True)\n",
    "        d = BatchNormalization()(d)\n",
    "        d = LeakyReLU(alpha=0.2)(d)\n",
    "        # patch output\n",
    "        (conv,stride),convstrided=convstrided[0,:,:],convstrided[1:,:,:]\n",
    "        d = Conv2D(1, conv, strides=stride, padding='same', kernel_initializer=init)(d)\n",
    "        patch_out = Activation('sigmoid')(d)\n",
    "        # define model\n",
    "        model = Model(in_src_image, patch_out)\n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOSS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GR_ML92o8mP7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "LAMBDA = 5\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(disc_real_output), disc_real_output)\n",
    "    generated_loss = cross_entropy(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    return total_disc_loss\n",
    "\n",
    "def identity_loss(real_image, same_image):\n",
    "    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "    return LAMBDA * 0.4 * loss\n",
    "\n",
    "def calc_cycle_loss(real_image, cycled_image):\n",
    "    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "    return LAMBDA * loss1\n",
    "\n",
    "def gan_loss(disc_generated_A):\n",
    "    gan_loss = cross_entropy(tf.ones_like(disc_generated_A), disc_generated_A)\n",
    "    return gan_loss\n",
    "\n",
    "def GEN_total_loss(GEN_X, GEN_Y, im_X, im_Y, gen_output_X_Y_bar, gen_output_Y_X_bar, disc_generated_X, disc_generated_Y ):\n",
    "    id_X_loss = identity_loss(im_Y, GEN_X(im_Y))\n",
    "    id_Y_loss = identity_loss(im_X, GEN_Y(im_X))\n",
    "            \n",
    "    cycle_X_loss= calc_cycle_loss(gen_output_Y_X_bar, im_X)\n",
    "    cycle_Y_loss= calc_cycle_loss(gen_output_X_Y_bar, im_Y)\n",
    "    cycle_total_loss = cycle_X_loss + cycle_Y_loss\n",
    "            \n",
    "    gan_X_loss = gan_loss(disc_generated_Y)\n",
    "    gan_Y_loss = gan_loss(disc_generated_X)\n",
    "    \n",
    "    total_X = gan_X_loss + cycle_total_loss + id_X_loss\n",
    "    total_Y = gan_Y_loss + cycle_total_loss + id_Y_loss\n",
    "    \n",
    "    return total_X, gan_X_loss, id_X_loss, total_Y, gan_Y_loss, id_Y_loss, cycle_total_loss\n",
    "    \n",
    "# old loss\n",
    "def generator_loss(disc_generated_A, gen_output_A_B_bar, img_B):\n",
    "    gan_loss = cross_entropy(tf.ones_like(disc_generated_A), disc_generated_A)\n",
    "    # mean absolute error\n",
    "    l1_loss = tf.reduce_mean(tf.abs(gen_output_A_B_bar - img_B))\n",
    "    #\n",
    "    total_gen_loss = gan_loss + (LAMBDA*l1_loss)\n",
    "    return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vj2E6Mu58nhh",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 2, 2)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.pad (TFOpLamb  (None, 258, 258, 3)          0         ['input_1[0][0]']             \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " encoder_block_0 (Conv2D)    (None, 256, 256, 64)         1792      ['tf.compat.v1.pad[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 256, 256, 64)         256       ['encoder_block_0[0][0]']     \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, 256, 256, 64)         0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " encoder_block_1 (Conv2D)    (None, 128, 128, 128)        73856     ['leaky_re_lu[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 128, 128, 128)        512       ['encoder_block_1[0][0]']     \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 128, 128, 128)        0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_block_2 (Conv2D)    (None, 64, 64, 256)          295168    ['leaky_re_lu_1[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 64, 64, 256)          1024      ['encoder_block_2[0][0]']     \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 64, 64, 256)          0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_block_3 (Conv2D)    (None, 32, 32, 512)          1180160   ['leaky_re_lu_2[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 32, 32, 512)          2048      ['encoder_block_3[0][0]']     \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 32, 32, 512)          0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " encoder_block_4 (Conv2D)    (None, 16, 16, 512)          2359808   ['leaky_re_lu_3[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 16, 16, 512)          2048      ['encoder_block_4[0][0]']     \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 16, 16, 512)          0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " bottleneck_conv2d (Conv2D)  (None, 8, 8, 512)            2359808   ['leaky_re_lu_4[0][0]']       \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 8, 8, 512)            0         ['bottleneck_conv2d[0][0]']   \n",
      "                                                                                                  \n",
      " decoder_block_4 (Conv2DTra  (None, 16, 16, 512)          2359808   ['activation[0][0]']          \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 16, 16, 512)          2048      ['decoder_block_4[0][0]']     \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 16, 16, 1024)         0         ['batch_normalization_5[0][0]'\n",
      "                                                                    , 'leaky_re_lu_4[0][0]']      \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 16, 16, 1024)         0         ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " decoder_block_3 (Conv2DTra  (None, 32, 32, 512)          4719104   ['activation_1[0][0]']        \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 32, 32, 512)          2048      ['decoder_block_3[0][0]']     \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 32, 32, 1024)         0         ['batch_normalization_6[0][0]'\n",
      " )                                                                  , 'leaky_re_lu_3[0][0]']      \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 32, 32, 1024)         0         ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " decoder_block_2 (Conv2DTra  (None, 64, 64, 256)          2359552   ['activation_2[0][0]']        \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 64, 64, 256)          1024      ['decoder_block_2[0][0]']     \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 64, 64, 512)          0         ['batch_normalization_7[0][0]'\n",
      " )                                                                  , 'leaky_re_lu_2[0][0]']      \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 64, 64, 512)          0         ['concatenate_2[0][0]']       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " decoder_block_1 (Conv2DTra  (None, 128, 128, 128)        589952    ['activation_3[0][0]']        \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 128, 128, 128)        512       ['decoder_block_1[0][0]']     \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 128, 128, 256)        0         ['batch_normalization_8[0][0]'\n",
      " )                                                                  , 'leaky_re_lu_1[0][0]']      \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 128, 128, 256)        0         ['concatenate_3[0][0]']       \n",
      "                                                                                                  \n",
      " decoder_block_0 (Conv2DTra  (None, 256, 256, 64)         147520    ['activation_4[0][0]']        \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 256, 256, 64)         256       ['decoder_block_0[0][0]']     \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate  (None, 256, 256, 128)        0         ['batch_normalization_9[0][0]'\n",
      " )                                                                  , 'leaky_re_lu[0][0]']        \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 256, 256, 128)        0         ['concatenate_4[0][0]']       \n",
      "                                                                                                  \n",
      " tf.compat.v1.pad_1 (TFOpLa  (None, 258, 258, 128)        0         ['activation_5[0][0]']        \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " 3rd_last_layer (Conv2D)     (None, 256, 256, 3)          3459      ['tf.compat.v1.pad_1[0][0]']  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 256, 256, 3)          0         ['3rd_last_layer[0][0]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16461763 (62.80 MB)\n",
      "Trainable params: 16455875 (62.77 MB)\n",
      "Non-trainable params: 5888 (23.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 2, 2)\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 128, 128, 32)      896       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128, 128, 32)      0         \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 128, 128, 32)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 64)        18496     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 64, 64, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 32, 32, 128)       512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 16, 16, 256)       1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 512)         1180160   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 8, 8, 512)         2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 4, 4, 1)           4609      \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 4, 4, 1)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1577025 (6.02 MB)\n",
      "Trainable params: 1575105 (6.01 MB)\n",
      "Non-trainable params: 1920 (7.50 KB)\n",
      "_________________________________________________________________\n",
      "(6, 2, 2)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 256, 256, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.pad_2 (TFOpLa  (None, 258, 258, 3)          0         ['input_3[0][0]']             \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " encoder_block_0 (Conv2D)    (None, 256, 256, 64)         1792      ['tf.compat.v1.pad_2[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 256, 256, 64)         256       ['encoder_block_0[0][0]']     \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 256, 256, 64)         0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " encoder_block_1 (Conv2D)    (None, 128, 128, 128)        73856     ['leaky_re_lu_10[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 128, 128, 128)        512       ['encoder_block_1[0][0]']     \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 128, 128, 128)        0         ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " encoder_block_2 (Conv2D)    (None, 64, 64, 256)          295168    ['leaky_re_lu_11[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 64, 64, 256)          1024      ['encoder_block_2[0][0]']     \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)  (None, 64, 64, 256)          0         ['batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " encoder_block_3 (Conv2D)    (None, 32, 32, 512)          1180160   ['leaky_re_lu_12[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 32, 32, 512)          2048      ['encoder_block_3[0][0]']     \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)  (None, 32, 32, 512)          0         ['batch_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " encoder_block_4 (Conv2D)    (None, 16, 16, 512)          2359808   ['leaky_re_lu_13[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 16, 16, 512)          2048      ['encoder_block_4[0][0]']     \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, 16, 16, 512)          0         ['batch_normalization_18[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " bottleneck_conv2d (Conv2D)  (None, 8, 8, 512)            2359808   ['leaky_re_lu_14[0][0]']      \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 8, 8, 512)            0         ['bottleneck_conv2d[0][0]']   \n",
      "                                                                                                  \n",
      " decoder_block_4 (Conv2DTra  (None, 16, 16, 512)          2359808   ['activation_8[0][0]']        \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 16, 16, 512)          2048      ['decoder_block_4[0][0]']     \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate  (None, 16, 16, 1024)         0         ['batch_normalization_19[0][0]\n",
      " )                                                                  ',                            \n",
      "                                                                     'leaky_re_lu_14[0][0]']      \n",
      "                                                                                                  \n",
      " activation_9 (Activation)   (None, 16, 16, 1024)         0         ['concatenate_5[0][0]']       \n",
      "                                                                                                  \n",
      " decoder_block_3 (Conv2DTra  (None, 32, 32, 512)          4719104   ['activation_9[0][0]']        \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, 32, 32, 512)          2048      ['decoder_block_3[0][0]']     \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate  (None, 32, 32, 1024)         0         ['batch_normalization_20[0][0]\n",
      " )                                                                  ',                            \n",
      "                                                                     'leaky_re_lu_13[0][0]']      \n",
      "                                                                                                  \n",
      " activation_10 (Activation)  (None, 32, 32, 1024)         0         ['concatenate_6[0][0]']       \n",
      "                                                                                                  \n",
      " decoder_block_2 (Conv2DTra  (None, 64, 64, 256)          2359552   ['activation_10[0][0]']       \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (Ba  (None, 64, 64, 256)          1024      ['decoder_block_2[0][0]']     \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate  (None, 64, 64, 512)          0         ['batch_normalization_21[0][0]\n",
      " )                                                                  ',                            \n",
      "                                                                     'leaky_re_lu_12[0][0]']      \n",
      "                                                                                                  \n",
      " activation_11 (Activation)  (None, 64, 64, 512)          0         ['concatenate_7[0][0]']       \n",
      "                                                                                                  \n",
      " decoder_block_1 (Conv2DTra  (None, 128, 128, 128)        589952    ['activation_11[0][0]']       \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (Ba  (None, 128, 128, 128)        512       ['decoder_block_1[0][0]']     \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate  (None, 128, 128, 256)        0         ['batch_normalization_22[0][0]\n",
      " )                                                                  ',                            \n",
      "                                                                     'leaky_re_lu_11[0][0]']      \n",
      "                                                                                                  \n",
      " activation_12 (Activation)  (None, 128, 128, 256)        0         ['concatenate_8[0][0]']       \n",
      "                                                                                                  \n",
      " decoder_block_0 (Conv2DTra  (None, 256, 256, 64)         147520    ['activation_12[0][0]']       \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (Ba  (None, 256, 256, 64)         256       ['decoder_block_0[0][0]']     \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate  (None, 256, 256, 128)        0         ['batch_normalization_23[0][0]\n",
      " )                                                                  ',                            \n",
      "                                                                     'leaky_re_lu_10[0][0]']      \n",
      "                                                                                                  \n",
      " activation_13 (Activation)  (None, 256, 256, 128)        0         ['concatenate_9[0][0]']       \n",
      "                                                                                                  \n",
      " tf.compat.v1.pad_3 (TFOpLa  (None, 258, 258, 128)        0         ['activation_13[0][0]']       \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " 3rd_last_layer (Conv2D)     (None, 256, 256, 3)          3459      ['tf.compat.v1.pad_3[0][0]']  \n",
      "                                                                                                  \n",
      " activation_14 (Activation)  (None, 256, 256, 3)          0         ['3rd_last_layer[0][0]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16461763 (62.80 MB)\n",
      "Trainable params: 16455875 (62.77 MB)\n",
      "Non-trainable params: 5888 (23.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "(6, 2, 2)\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 128, 128, 32)      896       \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128, 128, 32)      0         \n",
      "                                                                 \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, 128, 128, 32)      0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 64, 64, 64)        18496     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_24 (Ba  (None, 64, 64, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_16 (LeakyReLU)  (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_25 (Ba  (None, 32, 32, 128)       512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_17 (LeakyReLU)  (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " batch_normalization_26 (Ba  (None, 16, 16, 256)       1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 8, 8, 512)         1180160   \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      " batch_normalization_27 (Ba  (None, 8, 8, 512)         2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, 8, 8, 512)         0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 4, 4, 1)           4609      \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 4, 4, 1)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1577025 (6.02 MB)\n",
      "Trainable params: 1575105 (6.01 MB)\n",
      "Non-trainable params: 1920 (7.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "GEN_X = define_generator(image_shape=(256,256,3))\n",
    "DISC_X = define_discriminator(image_shape=(256,256,3))\n",
    "\n",
    "GEN_X_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)    #2e-4\n",
    "DISC_X_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)  #2e-4\n",
    "\n",
    "GEN_Y = define_generator(image_shape=(256,256,3))\n",
    "DISC_Y = define_discriminator(image_shape=(256,256,3))\n",
    "\n",
    "GEN_Y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)    #2e-4\n",
    "DISC_Y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)  #2e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UTILS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot real source images\n",
    "def summarize_performance(step, img_X_B, img_X_Y, img_Y_B, img_Y_X):\n",
    "    X_B = (img_X_B[-1] +1.0) / 2.0   # -1\n",
    "    X_Y = (img_X_Y[0] +1.0 ) / 2.0   \n",
    "    Y_B = (img_Y_B[-1] +1.0) / 2.0   # -1\n",
    "    Y_X = (img_Y_X[0] + 1.0) / 2.0\n",
    "\n",
    "    pyplot.subplot(2, 2, 1)\n",
    "    pyplot.axis('off')       \n",
    "    pyplot.imshow(X_B)\n",
    "    pyplot.title('X org')\n",
    "    \n",
    "    pyplot.subplot(2, 2, 2)\n",
    "    pyplot.axis('off')       \n",
    "    pyplot.imshow(X_Y) \n",
    "    pyplot.title('X org -> Y')\n",
    "\n",
    "    pyplot.subplot(2, 2, 3)\n",
    "    pyplot.axis('off')       \n",
    "    pyplot.imshow(Y_B)  #*255   \n",
    "    pyplot.title('Y org')\n",
    "\n",
    "    pyplot.subplot(2, 2, 4)\n",
    "    pyplot.axis('off')       \n",
    "    pyplot.imshow(Y_X) \n",
    "    pyplot.title('Y org -> X')\n",
    "\n",
    "    # save plot to file\n",
    "    filename1 = 'plot_%06d.png' % (step+1)\n",
    "    pyplot.savefig(filename1)\n",
    "    pyplot.close()\n",
    "\n",
    "def test_loss_avg(GEN_X, DISC_Y,path):\n",
    "    Loss_avg = list()\n",
    "    #L1_avg = list()\n",
    "\n",
    "    IMAGE=os.listdir(path)\n",
    "    IMAGE.sort()\n",
    "    \n",
    "    for file_img in IMAGE:\n",
    "        print(file_img)\n",
    "        new_im = os.path.join(path,file_img)\n",
    "\n",
    "        # now read the input image files (difference from the code it is derived from that preloads all images)\n",
    "        pixels = Image.open(new_im).convert(\"RGB\")\n",
    "        pixels = np.array(pixels)\n",
    "        pixels = pixels[np.newaxis, ...]\n",
    "        #\n",
    "        img_Y_fake = GEN_X(pixels, training=False)\n",
    "        #\n",
    "        disc_on_test = DISC_Y(img_Y_fake,training=False)\n",
    "        #\n",
    "        test_loss = gan_loss(disc_on_test)\n",
    "        #test_loss, gan_loss, l1_loss=generator_loss(disc_on_test, img_Y_fake, pixels)\n",
    "        Loss_avg.append(test_loss)\n",
    "        #L1_avg.append(l1_loss)\n",
    "        \n",
    "    avg = np.mean(Loss_avg) \n",
    "    #l1 = np.mean(L1_avg)\n",
    "    return avg\n",
    "\n",
    "def shuffle_and_batch(Dataset, buff_size, SEED):\n",
    "    # SHUFFLE\n",
    "    Dataset_S = Dataset.shuffle(buffer_size = buff_size,seed=SEED)    #10 * BATCH_SIZE   L_dataset.cardinality()\n",
    "    # Batch\n",
    "    Dataset_B = Dataset_S.batch(BATCH_SIZE)\n",
    "    return(Dataset_B)\n",
    "\n",
    "def change_Bright(image,B):        #run_eagerly=True   ???\n",
    "    img = rgb2hsv(image[0:1]*127.5 + 127.5)\n",
    "    img[:,:,:,2] = np.clip(img[:,:,:,2]*B,0,255)\n",
    "    img = (hsv2rgb(img) - 127.5) / 127.5\n",
    "    img_final = tf.keras.layers.concatenate([img, image[1:]],axis=0)\n",
    "    return img_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JsH4W1-x8otE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GEN_X -->  Generator from X to Y\n",
    "# DISC_X --> Discriminator of X images\n",
    "# ...\n",
    "\n",
    "@tf.function\n",
    "def train_step(GEN_X, DISC_X, GEN_Y, DISC_Y, img_X, img_Y, batch):  \n",
    "    GLX=DLX= tf.constant(0.)\n",
    "    GLY=DLY= tf.constant(0.)\n",
    "    GANX=GANY=tf.constant(0.)\n",
    "    IDX=IDY= tf.constant(0.)\n",
    "    CYCLE =  tf.constant(0.)\n",
    "    # for each image of the batch\n",
    "    with tf.GradientTape() as genX_tape, tf.GradientTape() as discX_tape, tf.GradientTape() as genY_tape, tf.GradientTape() as discY_tape:\n",
    "        for i in range(batch):\n",
    "            #  X --> Y --> X_bar  #\n",
    "            gen_output_X_Y = GEN_X(img_X[i:i+1], training=True)\n",
    "            gen_output_Y_X_bar = GEN_Y(gen_output_X_Y, training=True)\n",
    "            #  Y --> X --> Y_bar  #\n",
    "            gen_output_Y_X = GEN_Y(img_Y[i:i+1], training=True)\n",
    "            gen_output_X_Y_bar = GEN_X(gen_output_Y_X, training=True)\n",
    "            #\n",
    "            #  DISCRIMINATOR generated losses  #\n",
    "            disc_generated_X = DISC_X(gen_output_Y_X , training=True)\n",
    "            disc_generated_Y = DISC_Y(gen_output_X_Y, training=True)\n",
    "            #  DISCRIMINATOR Real losses  #\n",
    "            disc_real_X = DISC_X(img_X[i:i+1], training=True)\n",
    "            disc_real_Y = DISC_Y(img_Y[i:i+1], training=True)\n",
    "            #  DISCRIMINATOR total losses  #\n",
    "            disc_loss_X = discriminator_loss(disc_real_X, disc_generated_X)\n",
    "            disc_loss_Y = discriminator_loss(disc_real_Y, disc_generated_Y)\n",
    "            #\n",
    "            TOTAL_X, gan_X_loss, id_X_loss, TOTAL_Y, gan_Y_loss, id_Y_loss, CYCLE_loss = GEN_total_loss(GEN_X, GEN_Y, img_X[i:i+1], img_Y[i:i+1], gen_output_X_Y_bar, gen_output_Y_X_bar, disc_generated_X, disc_generated_Y)\n",
    "            #\n",
    "            DLX=tf.add(DLX,disc_loss_X)\n",
    "            GLX=tf.add(GLX,TOTAL_X)\n",
    "            DLY=tf.add(DLY,disc_loss_Y)\n",
    "            GLY=tf.add(GLY,TOTAL_Y)\n",
    "            #\n",
    "            GANX = tf.add(GANX, gan_X_loss)\n",
    "            IDX = tf.add(IDX, id_X_loss)\n",
    "            GANY = tf.add(GANX, gan_X_loss)\n",
    "            IDY = tf.add(IDX, id_X_loss)\n",
    "            CYCLE = tf.add(CYCLE, CYCLE_loss)\n",
    "            \n",
    "        # mean of batch loss\n",
    "        DLX = tf.math.divide(DLX,batch)\n",
    "        GLX = tf.math.divide(GLX,batch)\n",
    "        DLY = tf.math.divide(DLY,batch)\n",
    "        GLY = tf.math.divide(GLY,batch)\n",
    "        GANX= tf.math.divide(GANX,batch)\n",
    "        IDX = tf.math.divide(IDX,batch)\n",
    "        GANY= tf.math.divide(GANY,batch)\n",
    "        IDY = tf.math.divide(IDY,batch)\n",
    "        CYCLE=tf.math.divide(CYCLE,batch)\n",
    "        \n",
    "        # Update at the end of batch\n",
    "        generator_gradients_X = genX_tape.gradient(GLX,\n",
    "                                                GEN_X.trainable_variables)\n",
    "        discriminator_gradients_X = discX_tape.gradient(DLX,\n",
    "                                                     DISC_X.trainable_variables)\n",
    "        GEN_X_optimizer.apply_gradients(zip(generator_gradients_X,\n",
    "                                              GEN_X.trainable_variables))\n",
    "        DISC_X_optimizer.apply_gradients(zip(discriminator_gradients_X,\n",
    "                                                  DISC_X.trainable_variables))\n",
    "        \n",
    "        generator_gradients_Y = genY_tape.gradient(GLY,\n",
    "                                                GEN_Y.trainable_variables)\n",
    "        discriminator_gradients_Y = discY_tape.gradient(DLY,\n",
    "                                                     DISC_Y.trainable_variables)\n",
    "        GEN_Y_optimizer.apply_gradients(zip(generator_gradients_Y,\n",
    "                                              GEN_Y.trainable_variables))\n",
    "        DISC_Y_optimizer.apply_gradients(zip(discriminator_gradients_Y,\n",
    "                                                  DISC_Y.trainable_variables))\n",
    "\n",
    "    return GLX, DLX, GLY, DLY, gen_output_X_Y, gen_output_Y_X, GANX, IDX, GANY, IDY, CYCLE\n",
    "\n",
    "\n",
    "# train pix2pix models\n",
    "def train(GEN_X, DISC_X, GEN_Y, DISC_Y, dataset, batch=4):\n",
    "    # determine the output square shape of the discriminator\n",
    "    n_patch1 = DISC_X.output_shape[1]\n",
    "    n_patch2 = DISC_X.output_shape[2]\n",
    "    # calculate the number of batches per training epoch\n",
    "    #\n",
    "    GLoss = []\n",
    "    DLoss = []\n",
    "    GanX = []\n",
    "    GanY= []\n",
    "    IdX= []\n",
    "    IdY= []\n",
    "    Cycle= []\n",
    "    #\n",
    "    i=0\n",
    "    # manually enumerate batch\n",
    "    for data in dataset:\n",
    "        img_X = data[:,:,:,:3]\n",
    "        img_Y = data[:,:,:,3:]\n",
    "        # BRIGHT +/-\n",
    "        bright = np.clip(np.random.normal(0.9,0.2),0.8,1.1)  # (0.9,0.2),0.8,1.1)    (0.8,0.25),0.7,1.2) \n",
    "        img_X_B = change_Bright(img_X,bright)\n",
    "        img_Y_B = change_Bright(img_Y,bright)\n",
    "        # train step, update loss, gradient\n",
    "        gen_loss_X, disc_loss_X, gen_loss_Y, disc_loss_Y, img_X_Y, img_Y_X, GANX, IDX, GANY, IDY, CYCLE = train_step(GEN_X, DISC_X, GEN_Y, DISC_Y, img_X_B, img_Y_B, batch)    # tf.constant()\n",
    "        #\n",
    "        print(\"step \"+str(i)+\" gen_loss : \"+str(gen_loss_X.numpy())+\" disc_loss : \"+str(disc_loss_X.numpy())+\" Bright coef : \"+str(bright))\n",
    "        GLoss.append(gen_loss_X.numpy())\n",
    "        DLoss.append(disc_loss_X.numpy())\n",
    "        GanX.append(GANX.numpy())\n",
    "        GanY.append(GANY.numpy())\n",
    "        IdX.append(IDX.numpy())\n",
    "        IdY.append(IDY.numpy())\n",
    "        Cycle.append(CYCLE.numpy())\n",
    "        # summarize model performance every 200 steps\n",
    "        if (i % 20 == 0) and (i !=0):\n",
    "            summarize_performance(i, img_X_B, img_X_Y, img_Y_B, img_Y_X)\n",
    "        i+=1\n",
    "    return(DLoss, GLoss, GEN_X, DISC_X, GEN_Y, DISC_Y, GanX, GanY, IdX, IdY, Cycle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate G : 0.0002 Learning Rate D : 5e-05\n",
      "Seed :  5546\n",
      "\n",
      "Start of epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:1: Invalid control characters encountered in text.\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:3: Expected identifier, got: 4040241663323905875\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:1: Invalid control characters encountered in text.\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:3: Expected identifier, got: 4040241663323905875\n",
      "2023-09-15 16:51:15.602034: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_1/dropout/dropout_1/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-09-15 16:51:17.286892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-09-15 16:51:17.836245: W tensorflow/tsl/framework/bfc_allocator.cc:366] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n",
      "2023-09-15 16:51:18.249865: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f2db7ec2b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-15 16:51:18.249892: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 970, Compute Capability 5.2\n",
      "2023-09-15 16:51:18.254675: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-15 16:51:18.375313: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 gen_loss : 9.782321 disc_loss : 1.3685415 Bright coef : 0.8\n",
      "step 1 gen_loss : 5.9297886 disc_loss : 1.6699625 Bright coef : 0.8\n",
      "step 2 gen_loss : 6.9315243 disc_loss : 1.8194358 Bright coef : 0.8\n",
      "step 3 gen_loss : 4.9911094 disc_loss : 1.5699823 Bright coef : 0.8\n",
      "step 4 gen_loss : 5.3501034 disc_loss : 1.7092617 Bright coef : 0.9699763613557568\n",
      "step 5 gen_loss : 4.6062536 disc_loss : 1.5528995 Bright coef : 0.8\n",
      "step 6 gen_loss : 4.5520453 disc_loss : 1.424107 Bright coef : 1.0361837352927634\n",
      "step 7 gen_loss : 3.680949 disc_loss : 1.5728192 Bright coef : 0.9367371723803376\n",
      "step 8 gen_loss : 3.6534543 disc_loss : 1.592746 Bright coef : 0.8398002022338954\n",
      "step 9 gen_loss : 4.4697514 disc_loss : 1.4450114 Bright coef : 0.9228824494151759\n",
      "step 10 gen_loss : 3.1076367 disc_loss : 1.8102771 Bright coef : 0.850819299476317\n",
      "step 11 gen_loss : 4.5911293 disc_loss : 1.6232517 Bright coef : 1.1\n",
      "step 12 gen_loss : 2.9665954 disc_loss : 1.3038936 Bright coef : 0.8\n",
      "step 13 gen_loss : 4.296647 disc_loss : 1.4470941 Bright coef : 1.0315266462380637\n",
      "step 14 gen_loss : 3.3963478 disc_loss : 1.5920947 Bright coef : 0.8\n",
      "step 15 gen_loss : 3.8124406 disc_loss : 1.5611527 Bright coef : 0.8\n",
      "step 16 gen_loss : 4.467 disc_loss : 1.6091809 Bright coef : 0.8\n",
      "step 17 gen_loss : 2.67343 disc_loss : 1.3936608 Bright coef : 1.1\n",
      "step 18 gen_loss : 3.311329 disc_loss : 1.4437246 Bright coef : 1.1\n",
      "step 19 gen_loss : 3.7220573 disc_loss : 1.5881745 Bright coef : 0.9860724334470803\n",
      "step 20 gen_loss : 3.7128134 disc_loss : 1.7259064 Bright coef : 0.9290325063177938\n",
      "step 21 gen_loss : 4.6999063 disc_loss : 1.4359001 Bright coef : 0.8\n",
      "step 22 gen_loss : 2.5952473 disc_loss : 1.6731825 Bright coef : 0.8\n",
      "step 23 gen_loss : 2.9108558 disc_loss : 1.558002 Bright coef : 0.8\n",
      "step 24 gen_loss : 2.812137 disc_loss : 1.4544235 Bright coef : 0.8\n",
      "step 25 gen_loss : 3.082674 disc_loss : 1.5099139 Bright coef : 0.8\n",
      "step 26 gen_loss : 4.671277 disc_loss : 1.5552037 Bright coef : 0.8689713513071868\n",
      "step 27 gen_loss : 5.0488997 disc_loss : 1.3840933 Bright coef : 1.1\n",
      "step 28 gen_loss : 3.9368415 disc_loss : 1.8441951 Bright coef : 0.9652282688328198\n",
      "step 29 gen_loss : 3.0986514 disc_loss : 1.7798492 Bright coef : 0.8449985374693486\n",
      "step 30 gen_loss : 3.1597214 disc_loss : 1.3002706 Bright coef : 0.8\n",
      "step 31 gen_loss : 2.8079543 disc_loss : 1.6656337 Bright coef : 0.9515654310355626\n",
      "step 32 gen_loss : 2.9727905 disc_loss : 1.5123125 Bright coef : 1.0204023065706727\n",
      "step 33 gen_loss : 4.236814 disc_loss : 1.8808099 Bright coef : 0.9353400049065724\n",
      "step 34 gen_loss : 2.8223739 disc_loss : 1.6991969 Bright coef : 0.8\n",
      "step 35 gen_loss : 4.6299095 disc_loss : 1.6697826 Bright coef : 1.0646167423314643\n",
      "step 36 gen_loss : 2.4339955 disc_loss : 1.7235372 Bright coef : 0.8545186532912409\n",
      "step 37 gen_loss : 3.4196398 disc_loss : 1.5711141 Bright coef : 1.0487415175483035\n",
      "step 38 gen_loss : 2.56825 disc_loss : 1.3868446 Bright coef : 0.8272638882405622\n",
      "step 39 gen_loss : 3.699915 disc_loss : 1.8798838 Bright coef : 0.8988411524491757\n",
      "step 40 gen_loss : 3.2324765 disc_loss : 1.5731261 Bright coef : 0.9031492988771538\n",
      "step 41 gen_loss : 3.3067503 disc_loss : 1.8052299 Bright coef : 0.8\n",
      "step 42 gen_loss : 2.8554204 disc_loss : 1.8071129 Bright coef : 0.8872204265736402\n",
      "step 43 gen_loss : 2.8957648 disc_loss : 1.4639828 Bright coef : 0.8\n",
      "step 44 gen_loss : 2.712778 disc_loss : 1.5488696 Bright coef : 1.062338386533548\n",
      "step 45 gen_loss : 5.794272 disc_loss : 1.6071153 Bright coef : 1.1\n",
      "step 46 gen_loss : 3.1163664 disc_loss : 1.6812425 Bright coef : 0.8\n",
      "step 47 gen_loss : 4.2513433 disc_loss : 1.6285603 Bright coef : 1.1\n",
      "step 48 gen_loss : 3.4392262 disc_loss : 1.3087256 Bright coef : 1.0397736336639622\n",
      "step 49 gen_loss : 2.8940985 disc_loss : 1.4266176 Bright coef : 1.1\n",
      "step 50 gen_loss : 3.4237168 disc_loss : 1.4927869 Bright coef : 1.1\n",
      "step 51 gen_loss : 4.0605607 disc_loss : 1.3023181 Bright coef : 0.8\n",
      "step 52 gen_loss : 2.67214 disc_loss : 1.4957025 Bright coef : 0.8453498222179925\n",
      "step 53 gen_loss : 3.2999768 disc_loss : 1.6755317 Bright coef : 0.8\n",
      "step 54 gen_loss : 3.825102 disc_loss : 1.8116361 Bright coef : 0.8921963513173973\n",
      "step 55 gen_loss : 2.4592705 disc_loss : 1.5949011 Bright coef : 0.8\n",
      "step 56 gen_loss : 3.7080684 disc_loss : 1.5951053 Bright coef : 0.8719182000640844\n",
      "step 57 gen_loss : 2.0408566 disc_loss : 1.5717267 Bright coef : 0.8\n",
      "step 58 gen_loss : 2.336627 disc_loss : 1.7716078 Bright coef : 0.8120724804313115\n",
      "step 59 gen_loss : 3.8079023 disc_loss : 1.4844954 Bright coef : 0.8769099983306192\n",
      "step 60 gen_loss : 2.300322 disc_loss : 1.5123593 Bright coef : 0.8\n",
      "step 61 gen_loss : 2.525661 disc_loss : 1.6753509 Bright coef : 0.8731430004806113\n",
      "step 62 gen_loss : 2.515325 disc_loss : 1.4541875 Bright coef : 0.8\n",
      "step 63 gen_loss : 2.5440965 disc_loss : 1.4434791 Bright coef : 0.8597789712493242\n",
      "step 64 gen_loss : 3.3171487 disc_loss : 1.5682584 Bright coef : 0.8\n",
      "step 65 gen_loss : 2.6499145 disc_loss : 1.7444322 Bright coef : 0.8\n",
      "step 66 gen_loss : 5.117573 disc_loss : 1.3620409 Bright coef : 1.0151377395144328\n",
      "step 67 gen_loss : 2.7186317 disc_loss : 1.4177251 Bright coef : 0.9285253401260296\n",
      "step 68 gen_loss : 3.2147799 disc_loss : 1.4885708 Bright coef : 1.1\n",
      "step 69 gen_loss : 2.1103466 disc_loss : 1.3590885 Bright coef : 0.8419116215155426\n",
      "step 70 gen_loss : 2.695878 disc_loss : 1.5901427 Bright coef : 0.8\n",
      "step 71 gen_loss : 3.0927076 disc_loss : 1.2755437 Bright coef : 0.9571004352071684\n",
      "step 72 gen_loss : 3.2727952 disc_loss : 1.4194063 Bright coef : 1.0774600535105507\n",
      "step 73 gen_loss : 3.4312248 disc_loss : 1.6991076 Bright coef : 0.8946638476356666\n",
      "step 74 gen_loss : 3.4782205 disc_loss : 1.3988814 Bright coef : 0.990053589240765\n",
      "step 75 gen_loss : 2.709246 disc_loss : 1.4728807 Bright coef : 0.8664132605986605\n",
      "step 76 gen_loss : 2.4345746 disc_loss : 1.4471623 Bright coef : 0.996632924624455\n",
      "step 77 gen_loss : 2.348775 disc_loss : 1.5538664 Bright coef : 0.8\n",
      "step 78 gen_loss : 2.497693 disc_loss : 1.4378276 Bright coef : 0.8\n",
      "step 79 gen_loss : 3.211475 disc_loss : 1.6469526 Bright coef : 0.9648636531596255\n",
      "step 80 gen_loss : 3.6403108 disc_loss : 1.6805366 Bright coef : 1.0701483649853696\n",
      "step 81 gen_loss : 4.554802 disc_loss : 1.5928503 Bright coef : 0.8\n",
      "step 82 gen_loss : 4.048284 disc_loss : 1.9769645 Bright coef : 0.8\n",
      "step 83 gen_loss : 3.2674577 disc_loss : 1.4337668 Bright coef : 1.0406484793377504\n",
      "step 84 gen_loss : 2.8633094 disc_loss : 1.8013179 Bright coef : 0.9111359864767169\n",
      "step 85 gen_loss : 6.192064 disc_loss : 1.6725097 Bright coef : 0.9988994065312369\n",
      "step 86 gen_loss : 4.0405254 disc_loss : 1.5328802 Bright coef : 0.8\n",
      "step 87 gen_loss : 4.1907635 disc_loss : 1.6392269 Bright coef : 1.0832499559494473\n",
      "step 88 gen_loss : 3.5615423 disc_loss : 1.482388 Bright coef : 0.9290273709586511\n",
      "step 89 gen_loss : 2.85954 disc_loss : 1.3289738 Bright coef : 1.1\n",
      "step 90 gen_loss : 3.9947011 disc_loss : 1.4659626 Bright coef : 0.9478937448512748\n",
      "step 91 gen_loss : 3.1411111 disc_loss : 1.5532663 Bright coef : 0.8491402069412635\n",
      "step 92 gen_loss : 2.1087334 disc_loss : 1.5487115 Bright coef : 0.9594712831894441\n",
      "step 93 gen_loss : 2.4971972 disc_loss : 1.728096 Bright coef : 1.0174170489616654\n",
      "step 94 gen_loss : 3.1066463 disc_loss : 1.556225 Bright coef : 0.8\n",
      "step 95 gen_loss : 3.4201922 disc_loss : 1.5098503 Bright coef : 1.1\n",
      "step 96 gen_loss : 2.6590893 disc_loss : 1.7653031 Bright coef : 0.9305413395065142\n",
      "step 97 gen_loss : 2.4411845 disc_loss : 1.6377296 Bright coef : 0.8\n",
      "step 98 gen_loss : 4.0657673 disc_loss : 1.530395 Bright coef : 0.8\n",
      "step 99 gen_loss : 2.4483628 disc_loss : 1.3488145 Bright coef : 0.9208722270678253\n",
      "step 100 gen_loss : 3.761164 disc_loss : 1.4934614 Bright coef : 1.0314046756196031\n",
      "step 101 gen_loss : 2.4913156 disc_loss : 1.9544872 Bright coef : 1.005438025302735\n",
      "step 102 gen_loss : 3.1901765 disc_loss : 1.5356944 Bright coef : 1.1\n",
      "step 103 gen_loss : 2.765084 disc_loss : 1.2786944 Bright coef : 0.8662614030869931\n",
      "step 104 gen_loss : 2.7468255 disc_loss : 1.7371125 Bright coef : 1.087762998415282\n",
      "step 105 gen_loss : 2.4282749 disc_loss : 2.0141225 Bright coef : 0.978951768889079\n",
      "step 106 gen_loss : 2.9262886 disc_loss : 1.6691827 Bright coef : 0.8069042407198215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 107 gen_loss : 2.632405 disc_loss : 1.3248172 Bright coef : 0.8\n",
      "step 108 gen_loss : 2.043913 disc_loss : 1.5876848 Bright coef : 0.8715133519559211\n",
      "step 109 gen_loss : 2.4129841 disc_loss : 1.3496885 Bright coef : 1.040002422376771\n",
      "step 110 gen_loss : 2.2081904 disc_loss : 1.4154974 Bright coef : 0.8070873563282246\n",
      "step 111 gen_loss : 2.7592888 disc_loss : 1.6230657 Bright coef : 0.8\n",
      "step 112 gen_loss : 3.8552642 disc_loss : 1.6402551 Bright coef : 0.9701688484951179\n",
      "step 113 gen_loss : 2.671222 disc_loss : 1.158968 Bright coef : 1.1\n",
      "step 114 gen_loss : 2.1790655 disc_loss : 1.4492416 Bright coef : 0.9718171091560759\n",
      "step 115 gen_loss : 4.090476 disc_loss : 1.2964315 Bright coef : 0.8814758441673977\n",
      "step 116 gen_loss : 4.449248 disc_loss : 1.4945002 Bright coef : 1.1\n",
      "step 117 gen_loss : 2.3910494 disc_loss : 1.5629102 Bright coef : 0.9257950576430789\n",
      "step 118 gen_loss : 2.6738987 disc_loss : 1.6614541 Bright coef : 1.1\n",
      "step 119 gen_loss : 2.6270068 disc_loss : 1.5405724 Bright coef : 1.0164300840030043\n",
      "step 120 gen_loss : 3.8325984 disc_loss : 1.6267319 Bright coef : 0.8\n",
      "step 121 gen_loss : 4.3925614 disc_loss : 1.590636 Bright coef : 0.8514953386655592\n",
      "step 122 gen_loss : 3.0781069 disc_loss : 1.2962344 Bright coef : 0.8\n",
      "step 123 gen_loss : 2.807015 disc_loss : 1.2246963 Bright coef : 0.9648202676455028\n",
      "step 124 gen_loss : 3.0104172 disc_loss : 1.3117546 Bright coef : 1.1\n",
      "step 125 gen_loss : 3.508967 disc_loss : 1.3798308 Bright coef : 1.1\n",
      "step 126 gen_loss : 3.5574791 disc_loss : 1.372626 Bright coef : 1.0732703324534016\n",
      "step 127 gen_loss : 3.8555143 disc_loss : 1.5669566 Bright coef : 0.9599161888496506\n",
      "step 128 gen_loss : 5.8729234 disc_loss : 1.4479549 Bright coef : 0.8\n",
      "step 129 gen_loss : 3.091163 disc_loss : 1.8057921 Bright coef : 0.8\n",
      "step 130 gen_loss : 2.884878 disc_loss : 1.5794387 Bright coef : 0.9728630539597607\n",
      "step 131 gen_loss : 2.0344014 disc_loss : 1.5444427 Bright coef : 0.8\n",
      "step 132 gen_loss : 2.6661038 disc_loss : 1.3623 Bright coef : 0.8904548326638774\n",
      "step 133 gen_loss : 2.0829477 disc_loss : 1.5449572 Bright coef : 0.8\n",
      "step 134 gen_loss : 2.054824 disc_loss : 1.3580815 Bright coef : 0.8\n",
      "step 135 gen_loss : 4.432145 disc_loss : 1.5154461 Bright coef : 0.9706496087202795\n",
      "step 136 gen_loss : 2.9391017 disc_loss : 1.256213 Bright coef : 1.1\n",
      "step 137 gen_loss : 2.5391245 disc_loss : 1.3991097 Bright coef : 1.0494733474553133\n",
      "step 138 gen_loss : 2.990848 disc_loss : 1.5341835 Bright coef : 0.891563629796328\n",
      "step 139 gen_loss : 5.955045 disc_loss : 1.6591885 Bright coef : 1.1\n",
      "step 140 gen_loss : 3.978497 disc_loss : 1.7256019 Bright coef : 1.074491381008944\n",
      "step 141 gen_loss : 3.145843 disc_loss : 1.5801865 Bright coef : 0.9622023989142914\n",
      "step 142 gen_loss : 3.0136309 disc_loss : 1.3498856 Bright coef : 0.8\n",
      "step 143 gen_loss : 2.987865 disc_loss : 1.372947 Bright coef : 0.8\n",
      "step 144 gen_loss : 3.6270978 disc_loss : 1.2655818 Bright coef : 0.9091953866393019\n",
      "step 145 gen_loss : 2.7764065 disc_loss : 1.6710554 Bright coef : 1.0696197175402966\n",
      "step 146 gen_loss : 2.1662583 disc_loss : 1.2107401 Bright coef : 0.8\n",
      "step 147 gen_loss : 2.339045 disc_loss : 1.2675393 Bright coef : 0.8\n",
      "step 148 gen_loss : 5.0886335 disc_loss : 1.646589 Bright coef : 1.075348161073004\n",
      "step 149 gen_loss : 2.5573962 disc_loss : 1.3898696 Bright coef : 0.8\n",
      "step 150 gen_loss : 3.0302432 disc_loss : 1.5778022 Bright coef : 0.8966136856416612\n",
      "step 151 gen_loss : 2.0243423 disc_loss : 1.3927422 Bright coef : 0.8049649177458249\n",
      "step 152 gen_loss : 2.892637 disc_loss : 1.5387797 Bright coef : 0.9695314775061654\n",
      "step 153 gen_loss : 3.3836408 disc_loss : 1.4857326 Bright coef : 0.9719047342173466\n",
      "step 154 gen_loss : 3.6428185 disc_loss : 1.4600666 Bright coef : 0.8\n",
      "step 155 gen_loss : 4.735973 disc_loss : 1.3496879 Bright coef : 1.044242004088992\n",
      "step 156 gen_loss : 3.328936 disc_loss : 1.5274956 Bright coef : 1.0433431241031113\n",
      "step 157 gen_loss : 3.2367237 disc_loss : 1.7071571 Bright coef : 0.8\n",
      "step 158 gen_loss : 3.2444339 disc_loss : 1.575611 Bright coef : 0.9064581905130676\n",
      "step 159 gen_loss : 2.5830014 disc_loss : 1.600344 Bright coef : 0.8\n",
      "step 160 gen_loss : 2.4572797 disc_loss : 1.2851095 Bright coef : 0.8826846111393822\n",
      "step 161 gen_loss : 4.1821585 disc_loss : 1.2967747 Bright coef : 1.1\n",
      "step 162 gen_loss : 3.4221673 disc_loss : 1.7368834 Bright coef : 0.8080364976546518\n",
      "step 163 gen_loss : 3.7500687 disc_loss : 1.6582394 Bright coef : 0.8\n",
      "step 164 gen_loss : 4.5946164 disc_loss : 1.2745699 Bright coef : 1.04191378246207\n",
      "step 165 gen_loss : 3.5866883 disc_loss : 1.7192633 Bright coef : 0.8693543092096905\n",
      "step 166 gen_loss : 3.199933 disc_loss : 1.5035253 Bright coef : 1.1\n",
      "step 167 gen_loss : 4.4371862 disc_loss : 1.5606551 Bright coef : 0.8\n",
      "step 168 gen_loss : 4.033758 disc_loss : 1.5384989 Bright coef : 0.8\n",
      "step 169 gen_loss : 2.6374035 disc_loss : 1.3953294 Bright coef : 0.8\n",
      "step 170 gen_loss : 4.140759 disc_loss : 1.6159673 Bright coef : 1.1\n",
      "step 171 gen_loss : 3.7455492 disc_loss : 1.769501 Bright coef : 1.0852804529877713\n",
      "step 172 gen_loss : 2.683918 disc_loss : 1.314111 Bright coef : 0.8\n",
      "step 173 gen_loss : 2.416062 disc_loss : 1.5505929 Bright coef : 0.8\n",
      "step 174 gen_loss : 2.494647 disc_loss : 1.512419 Bright coef : 0.9772484092634484\n",
      "step 175 gen_loss : 3.8640893 disc_loss : 2.0120094 Bright coef : 1.0643056507695308\n",
      "step 176 gen_loss : 3.3372164 disc_loss : 1.7562422 Bright coef : 0.8917700442066632\n",
      "step 177 gen_loss : 2.8874497 disc_loss : 1.6890125 Bright coef : 1.0936171884173511\n",
      "step 178 gen_loss : 2.4350412 disc_loss : 1.76366 Bright coef : 1.0480935242313905\n",
      "step 179 gen_loss : 2.0883243 disc_loss : 1.4392877 Bright coef : 1.1\n",
      "step 180 gen_loss : 2.7787836 disc_loss : 1.7021403 Bright coef : 0.8554685342645123\n",
      "step 181 gen_loss : 2.8242202 disc_loss : 1.4720318 Bright coef : 0.9295106454674273\n",
      "step 182 gen_loss : 3.5386424 disc_loss : 1.5863328 Bright coef : 1.1\n",
      "step 183 gen_loss : 2.6924825 disc_loss : 1.5133181 Bright coef : 1.1\n",
      "step 184 gen_loss : 2.823585 disc_loss : 1.697828 Bright coef : 1.0386933864320151\n",
      "step 185 gen_loss : 3.6663036 disc_loss : 1.6022217 Bright coef : 1.1\n",
      "step 186 gen_loss : 3.3577385 disc_loss : 1.3222489 Bright coef : 0.8978447638282195\n",
      "step 187 gen_loss : 2.8599064 disc_loss : 1.5184672 Bright coef : 1.1\n",
      "step 188 gen_loss : 3.2361033 disc_loss : 1.516066 Bright coef : 0.857334389961426\n",
      "step 189 gen_loss : 3.1492383 disc_loss : 1.3291684 Bright coef : 0.9153937164867166\n",
      "step 190 gen_loss : 3.093739 disc_loss : 1.2155385 Bright coef : 0.8084114005330606\n",
      "step 191 gen_loss : 2.420129 disc_loss : 1.4192836 Bright coef : 0.8825560316147357\n",
      "step 192 gen_loss : 3.2769797 disc_loss : 1.3542356 Bright coef : 0.8\n",
      "step 193 gen_loss : 2.358446 disc_loss : 1.5733383 Bright coef : 0.8\n",
      "step 194 gen_loss : 3.1146657 disc_loss : 1.3946352 Bright coef : 0.9383312290121798\n",
      "step 195 gen_loss : 3.4739094 disc_loss : 1.2055898 Bright coef : 0.8\n",
      "step 196 gen_loss : 3.1492896 disc_loss : 1.400137 Bright coef : 0.8\n",
      "step 197 gen_loss : 3.6447356 disc_loss : 1.5938216 Bright coef : 1.1\n",
      "step 198 gen_loss : 3.6597383 disc_loss : 1.5800979 Bright coef : 0.900087427019453\n",
      "step 199 gen_loss : 2.8260138 disc_loss : 1.4374969 Bright coef : 0.8\n",
      "step 200 gen_loss : 2.6701584 disc_loss : 1.6448934 Bright coef : 0.9840031002890797\n",
      "step 201 gen_loss : 2.3637202 disc_loss : 1.53897 Bright coef : 0.8428580766941358\n",
      "step 202 gen_loss : 3.041889 disc_loss : 1.3536783 Bright coef : 1.1\n",
      "step 203 gen_loss : 4.2069206 disc_loss : 1.6583333 Bright coef : 0.8822466420010646\n",
      "step 204 gen_loss : 2.4757318 disc_loss : 1.3441403 Bright coef : 0.8743397452843319\n",
      "step 205 gen_loss : 4.4878206 disc_loss : 1.637805 Bright coef : 0.9476563228127145\n",
      "step 206 gen_loss : 2.7990766 disc_loss : 1.5689719 Bright coef : 0.8228523098431342\n",
      "step 207 gen_loss : 3.4263725 disc_loss : 1.7377506 Bright coef : 0.8\n",
      "step 208 gen_loss : 2.3214004 disc_loss : 1.6084995 Bright coef : 0.8\n",
      "step 209 gen_loss : 2.3986971 disc_loss : 1.7554159 Bright coef : 0.8\n",
      "step 210 gen_loss : 4.6370616 disc_loss : 1.3861136 Bright coef : 1.1\n",
      "step 211 gen_loss : 3.9459321 disc_loss : 1.8912506 Bright coef : 1.0934963531210449\n",
      "step 212 gen_loss : 4.1522655 disc_loss : 1.4080385 Bright coef : 1.0857806073440246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 213 gen_loss : 3.4811795 disc_loss : 1.6228018 Bright coef : 0.93188247253411\n",
      "step 214 gen_loss : 3.7418454 disc_loss : 1.741698 Bright coef : 0.839047294286078\n",
      "step 215 gen_loss : 2.8894827 disc_loss : 1.494499 Bright coef : 0.9023966118214836\n",
      "step 216 gen_loss : 2.9276536 disc_loss : 1.5427617 Bright coef : 0.9784845629192311\n",
      "step 217 gen_loss : 3.0332177 disc_loss : 1.4683995 Bright coef : 1.0431537506064439\n",
      "step 218 gen_loss : 2.9751108 disc_loss : 1.2064161 Bright coef : 0.9151179055812522\n",
      "step 219 gen_loss : 2.1796877 disc_loss : 1.44349 Bright coef : 0.8324993694035441\n",
      "step 220 gen_loss : 3.131941 disc_loss : 1.3805722 Bright coef : 1.1\n",
      "step 221 gen_loss : 3.431117 disc_loss : 1.4636732 Bright coef : 0.8\n",
      "step 222 gen_loss : 2.2607725 disc_loss : 1.7192321 Bright coef : 1.1\n",
      "step 223 gen_loss : 2.3127184 disc_loss : 1.0611748 Bright coef : 1.1\n",
      "step 224 gen_loss : 2.7204294 disc_loss : 1.2843673 Bright coef : 1.1\n",
      "step 225 gen_loss : 2.3738658 disc_loss : 1.6562624 Bright coef : 1.0510118637974066\n",
      "step 226 gen_loss : 2.645842 disc_loss : 1.6635847 Bright coef : 0.8\n",
      "step 227 gen_loss : 2.068777 disc_loss : 1.6245239 Bright coef : 0.8\n",
      "step 228 gen_loss : 4.2374134 disc_loss : 1.5513153 Bright coef : 0.8475431538351008\n",
      "step 229 gen_loss : 2.171229 disc_loss : 1.4356543 Bright coef : 0.8\n",
      "step 230 gen_loss : 2.826078 disc_loss : 1.5907401 Bright coef : 1.1\n",
      "step 231 gen_loss : 3.9012675 disc_loss : 1.5835153 Bright coef : 0.9474484093686384\n",
      "step 232 gen_loss : 2.1462848 disc_loss : 1.3165153 Bright coef : 0.8018725291735115\n",
      "step 233 gen_loss : 2.8063834 disc_loss : 1.4860963 Bright coef : 0.8\n",
      "step 234 gen_loss : 2.5951996 disc_loss : 1.413998 Bright coef : 0.9720585962799935\n",
      "step 235 gen_loss : 2.1272595 disc_loss : 1.7604799 Bright coef : 1.0031490671308287\n",
      "step 236 gen_loss : 2.2923856 disc_loss : 1.3844604 Bright coef : 0.8\n",
      "step 237 gen_loss : 2.4823813 disc_loss : 1.6508248 Bright coef : 0.8\n",
      "step 238 gen_loss : 5.572662 disc_loss : 1.3856827 Bright coef : 1.1\n",
      "step 239 gen_loss : 3.0922492 disc_loss : 1.5193992 Bright coef : 0.8\n",
      "step 240 gen_loss : 1.8941522 disc_loss : 1.7573814 Bright coef : 0.8\n",
      "step 241 gen_loss : 2.1633904 disc_loss : 1.620904 Bright coef : 0.8935122589883859\n",
      "step 242 gen_loss : 2.043872 disc_loss : 1.230439 Bright coef : 0.8\n",
      "step 243 gen_loss : 4.096118 disc_loss : 1.5603014 Bright coef : 1.1\n",
      "step 244 gen_loss : 3.706718 disc_loss : 1.4777952 Bright coef : 0.8\n",
      "step 245 gen_loss : 2.8322055 disc_loss : 1.366781 Bright coef : 1.050686802198338\n",
      "step 246 gen_loss : 2.9548914 disc_loss : 1.6731834 Bright coef : 0.8060732411211496\n",
      "step 247 gen_loss : 2.681957 disc_loss : 1.422499 Bright coef : 0.8930958905760962\n",
      "step 248 gen_loss : 4.0150695 disc_loss : 1.4217637 Bright coef : 1.071481823016887\n",
      "step 249 gen_loss : 2.849806 disc_loss : 1.6787612 Bright coef : 0.8\n",
      "step 250 gen_loss : 3.1815505 disc_loss : 1.5194849 Bright coef : 1.1\n",
      "step 251 gen_loss : 3.1710737 disc_loss : 1.7491586 Bright coef : 0.8313513589658967\n",
      "step 252 gen_loss : 1.7474182 disc_loss : 1.4347407 Bright coef : 0.8935905503495377\n",
      "step 253 gen_loss : 3.5798454 disc_loss : 1.2704096 Bright coef : 0.8\n",
      "step 254 gen_loss : 3.132582 disc_loss : 1.5049498 Bright coef : 1.045520831614812\n",
      "step 255 gen_loss : 4.2039027 disc_loss : 1.5197215 Bright coef : 1.1\n",
      "step 256 gen_loss : 2.556251 disc_loss : 1.2133801 Bright coef : 1.1\n",
      "step 257 gen_loss : 2.0118678 disc_loss : 1.507286 Bright coef : 0.8\n",
      "step 258 gen_loss : 2.8254616 disc_loss : 1.5760922 Bright coef : 1.1\n",
      "step 259 gen_loss : 2.20391 disc_loss : 1.3286537 Bright coef : 0.8\n",
      "step 260 gen_loss : 3.0077581 disc_loss : 1.316787 Bright coef : 1.1\n",
      "step 261 gen_loss : 2.4138591 disc_loss : 1.6824712 Bright coef : 0.8188113509385273\n",
      "step 262 gen_loss : 2.58673 disc_loss : 1.5169024 Bright coef : 1.0435736654721806\n",
      "step 263 gen_loss : 3.060638 disc_loss : 1.7611237 Bright coef : 0.8\n",
      "step 264 gen_loss : 3.4579513 disc_loss : 1.5361019 Bright coef : 1.1\n",
      "step 265 gen_loss : 2.6788633 disc_loss : 1.3302697 Bright coef : 0.9727667405742609\n",
      "step 266 gen_loss : 2.4656966 disc_loss : 1.1490321 Bright coef : 0.9544853302899337\n",
      "step 267 gen_loss : 4.8809004 disc_loss : 1.5458829 Bright coef : 0.8\n",
      "step 268 gen_loss : 3.799941 disc_loss : 1.4663111 Bright coef : 1.0020807840363077\n",
      "step 269 gen_loss : 3.1960554 disc_loss : 1.3494031 Bright coef : 1.0766475735736554\n",
      "step 270 gen_loss : 3.2323582 disc_loss : 1.5637002 Bright coef : 0.8191459875123627\n",
      "step 271 gen_loss : 3.407368 disc_loss : 1.375054 Bright coef : 1.1\n",
      "step 272 gen_loss : 2.2855833 disc_loss : 1.3487562 Bright coef : 0.8494897718430264\n",
      "step 273 gen_loss : 2.3036003 disc_loss : 1.6395719 Bright coef : 0.9894135845163655\n",
      "step 274 gen_loss : 4.6949444 disc_loss : 1.2456636 Bright coef : 1.1\n",
      "step 275 gen_loss : 3.2151453 disc_loss : 1.6509957 Bright coef : 0.9649270153560502\n",
      "step 276 gen_loss : 2.2742743 disc_loss : 1.6346416 Bright coef : 0.8\n",
      "step 277 gen_loss : 2.3653212 disc_loss : 1.7671397 Bright coef : 0.8621341430296579\n",
      "step 278 gen_loss : 2.3718965 disc_loss : 1.642765 Bright coef : 0.8997122154359839\n",
      "step 279 gen_loss : 2.858554 disc_loss : 1.4952891 Bright coef : 1.1\n",
      "step 280 gen_loss : 2.6042855 disc_loss : 1.6657654 Bright coef : 0.8\n",
      "step 281 gen_loss : 3.8308268 disc_loss : 1.4164282 Bright coef : 0.8660886142307396\n",
      "step 282 gen_loss : 2.441702 disc_loss : 1.6825784 Bright coef : 0.9571937047293785\n",
      "step 283 gen_loss : 3.4056668 disc_loss : 1.5400519 Bright coef : 0.8\n",
      "step 284 gen_loss : 2.1279435 disc_loss : 1.688832 Bright coef : 0.9254520265352842\n",
      "step 285 gen_loss : 3.265624 disc_loss : 1.2764249 Bright coef : 1.0187852716579362\n",
      "step 286 gen_loss : 4.4875574 disc_loss : 1.5117414 Bright coef : 0.8139821041783293\n",
      "step 287 gen_loss : 2.2406108 disc_loss : 1.8447706 Bright coef : 1.0954461966500282\n",
      "step 288 gen_loss : 2.4559276 disc_loss : 1.6918365 Bright coef : 1.1\n",
      "step 289 gen_loss : 2.5145166 disc_loss : 1.6227422 Bright coef : 0.8\n",
      "step 290 gen_loss : 2.1450305 disc_loss : 1.6357758 Bright coef : 1.1\n",
      "step 291 gen_loss : 4.4534445 disc_loss : 1.4499139 Bright coef : 0.9391710081813065\n",
      "step 292 gen_loss : 3.3887 disc_loss : 1.641152 Bright coef : 0.9414956501692431\n",
      "step 293 gen_loss : 3.093911 disc_loss : 1.210686 Bright coef : 0.8\n",
      "step 294 gen_loss : 2.153171 disc_loss : 1.385603 Bright coef : 0.8\n",
      "step 295 gen_loss : 3.567257 disc_loss : 1.5834908 Bright coef : 0.8807238652267781\n",
      "step 296 gen_loss : 2.331694 disc_loss : 1.3884809 Bright coef : 0.8\n",
      "step 297 gen_loss : 4.0760326 disc_loss : 1.3909676 Bright coef : 0.999251690544193\n",
      "step 298 gen_loss : 2.1269238 disc_loss : 1.3734753 Bright coef : 0.8\n",
      "step 299 gen_loss : 2.711916 disc_loss : 1.6131889 Bright coef : 0.8\n",
      "step 300 gen_loss : 3.4590373 disc_loss : 1.3976157 Bright coef : 1.1\n",
      "step 301 gen_loss : 2.909598 disc_loss : 1.2181551 Bright coef : 0.9867786694826026\n",
      "step 302 gen_loss : 2.7483113 disc_loss : 1.5814271 Bright coef : 0.8\n",
      "step 303 gen_loss : 2.7490027 disc_loss : 1.2537436 Bright coef : 0.8563085771697614\n",
      "step 304 gen_loss : 2.5354614 disc_loss : 1.4194788 Bright coef : 0.8\n",
      "step 305 gen_loss : 3.200648 disc_loss : 1.2240633 Bright coef : 1.047254757164224\n",
      "step 306 gen_loss : 2.7504468 disc_loss : 1.6819439 Bright coef : 0.9753658531389504\n",
      "step 307 gen_loss : 3.1592305 disc_loss : 1.544456 Bright coef : 1.009702470279056\n",
      "step 308 gen_loss : 2.0126278 disc_loss : 1.3826364 Bright coef : 0.9966527017951239\n",
      "step 309 gen_loss : 2.45568 disc_loss : 1.3503776 Bright coef : 0.8926951670771751\n",
      "step 310 gen_loss : 1.823551 disc_loss : 1.3722247 Bright coef : 0.8782991289392685\n",
      "step 311 gen_loss : 1.7775277 disc_loss : 1.3328267 Bright coef : 0.8350156023854546\n",
      "step 312 gen_loss : 1.923142 disc_loss : 1.4581046 Bright coef : 0.8382344374882909\n",
      "step 313 gen_loss : 1.9331003 disc_loss : 1.7532163 Bright coef : 0.8420907273235165\n",
      "step 314 gen_loss : 1.8859826 disc_loss : 1.7596009 Bright coef : 0.8\n",
      "step 315 gen_loss : 2.4805925 disc_loss : 1.3594803 Bright coef : 0.8\n",
      "step 316 gen_loss : 2.228614 disc_loss : 1.656459 Bright coef : 0.8985515591602874\n",
      "step 317 gen_loss : 2.947062 disc_loss : 1.7339224 Bright coef : 1.068162355387499\n",
      "step 318 gen_loss : 2.6874707 disc_loss : 1.3399701 Bright coef : 0.959656168446384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 319 gen_loss : 2.4470851 disc_loss : 1.3738495 Bright coef : 1.0291958406341444\n",
      "step 320 gen_loss : 2.9629843 disc_loss : 1.5365126 Bright coef : 0.8\n",
      "step 321 gen_loss : 2.454807 disc_loss : 1.5812702 Bright coef : 0.9179153993614526\n",
      "step 322 gen_loss : 2.3984349 disc_loss : 1.6552308 Bright coef : 0.8553578138908272\n",
      "step 323 gen_loss : 2.7824106 disc_loss : 1.6350055 Bright coef : 1.1\n",
      "step 324 gen_loss : 2.5022657 disc_loss : 1.8923466 Bright coef : 1.1\n",
      "step 325 gen_loss : 2.069081 disc_loss : 1.242878 Bright coef : 0.968294514577636\n",
      "step 326 gen_loss : 2.1392658 disc_loss : 1.6651863 Bright coef : 0.8\n",
      "step 327 gen_loss : 2.4389758 disc_loss : 1.6739607 Bright coef : 0.8\n",
      "step 328 gen_loss : 2.1895237 disc_loss : 1.3445417 Bright coef : 0.8\n",
      "step 329 gen_loss : 3.4101682 disc_loss : 1.4649711 Bright coef : 1.1\n",
      "step 330 gen_loss : 2.8823528 disc_loss : 1.6980687 Bright coef : 1.0773350247783053\n",
      "step 331 gen_loss : 3.153538 disc_loss : 1.6524962 Bright coef : 1.06609258556247\n",
      "step 332 gen_loss : 2.9318638 disc_loss : 1.2792292 Bright coef : 1.1\n",
      "step 333 gen_loss : 2.6151624 disc_loss : 1.6427339 Bright coef : 0.9405368385486395\n",
      "step 334 gen_loss : 3.757163 disc_loss : 1.6198536 Bright coef : 0.8553278746618547\n",
      "step 335 gen_loss : 3.5351248 disc_loss : 1.7829025 Bright coef : 1.1\n",
      "step 336 gen_loss : 4.0116687 disc_loss : 1.6054745 Bright coef : 0.8\n",
      "step 337 gen_loss : 3.507056 disc_loss : 1.2442484 Bright coef : 0.9904475810402799\n",
      "step 338 gen_loss : 3.399063 disc_loss : 1.7059976 Bright coef : 1.1\n",
      "step 339 gen_loss : 2.675767 disc_loss : 1.4963753 Bright coef : 0.8\n",
      "step 340 gen_loss : 2.3306332 disc_loss : 1.1943452 Bright coef : 0.8\n",
      "step 341 gen_loss : 3.3230033 disc_loss : 1.3049073 Bright coef : 0.9507630253723814\n",
      "step 342 gen_loss : 2.3800914 disc_loss : 1.7246804 Bright coef : 0.8\n",
      "step 343 gen_loss : 2.1037269 disc_loss : 1.2037305 Bright coef : 0.8\n",
      "step 344 gen_loss : 3.3439844 disc_loss : 1.5909748 Bright coef : 0.8\n",
      "step 345 gen_loss : 2.0885608 disc_loss : 1.8102138 Bright coef : 1.0746505438529086\n",
      "step 346 gen_loss : 2.7840865 disc_loss : 1.6298156 Bright coef : 1.0943311393275854\n",
      "step 347 gen_loss : 1.807127 disc_loss : 1.585132 Bright coef : 0.8735969420461042\n",
      "step 348 gen_loss : 1.8954519 disc_loss : 1.6782596 Bright coef : 0.8958290086381538\n",
      "step 349 gen_loss : 5.4697256 disc_loss : 1.524779 Bright coef : 1.1\n",
      "step 350 gen_loss : 2.6195586 disc_loss : 1.8177813 Bright coef : 1.0652249916394354\n",
      "step 351 gen_loss : 3.0262938 disc_loss : 1.3077049 Bright coef : 0.8558199818577518\n",
      "step 352 gen_loss : 2.5072486 disc_loss : 1.5199893 Bright coef : 0.9048266445386408\n",
      "step 353 gen_loss : 3.338983 disc_loss : 1.1298618 Bright coef : 0.8\n",
      "step 354 gen_loss : 5.5959826 disc_loss : 1.3148179 Bright coef : 1.1\n",
      "step 355 gen_loss : 3.4847772 disc_loss : 1.026778 Bright coef : 0.8\n",
      "step 356 gen_loss : 3.0063872 disc_loss : 1.4472189 Bright coef : 0.8\n",
      "step 357 gen_loss : 2.1595201 disc_loss : 1.6382868 Bright coef : 0.8839462487123251\n",
      "step 358 gen_loss : 1.8904786 disc_loss : 1.5106933 Bright coef : 0.8\n",
      "step 359 gen_loss : 2.0677085 disc_loss : 1.4884129 Bright coef : 0.87725805371348\n",
      "step 360 gen_loss : 1.740092 disc_loss : 1.4054179 Bright coef : 0.8\n",
      "step 361 gen_loss : 1.9849497 disc_loss : 1.4286824 Bright coef : 0.8\n",
      "step 362 gen_loss : 3.7840424 disc_loss : 1.489625 Bright coef : 0.9765953804080072\n",
      "step 363 gen_loss : 2.0730681 disc_loss : 1.3136332 Bright coef : 0.9023579470278206\n",
      "step 364 gen_loss : 2.2249582 disc_loss : 1.2697129 Bright coef : 0.9517278317414727\n",
      "step 365 gen_loss : 2.4754968 disc_loss : 1.6294817 Bright coef : 0.892120826455823\n",
      "step 366 gen_loss : 3.9192028 disc_loss : 1.8100947 Bright coef : 1.0718266465026867\n",
      "step 367 gen_loss : 2.585081 disc_loss : 1.302005 Bright coef : 1.0710366454365619\n",
      "step 368 gen_loss : 2.353698 disc_loss : 1.8581353 Bright coef : 1.0749032333211916\n",
      "step 369 gen_loss : 2.3900738 disc_loss : 1.417666 Bright coef : 0.9103971746039555\n",
      "step 370 gen_loss : 1.9992723 disc_loss : 1.7172598 Bright coef : 0.8\n",
      "step 371 gen_loss : 3.6879332 disc_loss : 1.5432231 Bright coef : 1.015548009740542\n",
      "step 372 gen_loss : 2.5808938 disc_loss : 1.9674107 Bright coef : 1.0835962956741008\n",
      "step 373 gen_loss : 3.0328119 disc_loss : 1.5883073 Bright coef : 0.8\n",
      "step 374 gen_loss : 3.0773778 disc_loss : 1.448075 Bright coef : 1.1\n",
      "step 375 gen_loss : 3.1037323 disc_loss : 1.4128053 Bright coef : 1.082489995081517\n",
      "step 376 gen_loss : 3.6495566 disc_loss : 1.41608 Bright coef : 0.9586575703951089\n",
      "step 377 gen_loss : 2.3275008 disc_loss : 1.475081 Bright coef : 0.8\n",
      "step 378 gen_loss : 4.718029 disc_loss : 1.6287568 Bright coef : 1.0702317088434086\n",
      "step 379 gen_loss : 1.9763037 disc_loss : 1.658381 Bright coef : 0.9137425598149466\n",
      "step 380 gen_loss : 2.532544 disc_loss : 1.2670734 Bright coef : 0.9345071581703843\n",
      "step 381 gen_loss : 3.227372 disc_loss : 1.5392201 Bright coef : 0.817235463320171\n",
      "step 382 gen_loss : 5.070599 disc_loss : 1.4091089 Bright coef : 1.0421078562953414\n",
      "step 383 gen_loss : 2.6768088 disc_loss : 1.454778 Bright coef : 0.9413032117312711\n",
      "step 384 gen_loss : 1.9616108 disc_loss : 1.2908578 Bright coef : 0.9347941986813112\n",
      "step 385 gen_loss : 4.0883718 disc_loss : 1.4183573 Bright coef : 0.8110638714742517\n",
      "step 386 gen_loss : 2.0923278 disc_loss : 1.7031604 Bright coef : 0.8\n",
      "step 387 gen_loss : 2.1316466 disc_loss : 1.4519517 Bright coef : 0.9577186516523695\n",
      "step 388 gen_loss : 3.226412 disc_loss : 1.5634575 Bright coef : 0.8\n",
      "step 389 gen_loss : 2.1148303 disc_loss : 1.4493188 Bright coef : 0.8\n",
      "step 390 gen_loss : 1.7080783 disc_loss : 1.5960742 Bright coef : 0.8\n",
      "step 391 gen_loss : 2.1835642 disc_loss : 1.4924961 Bright coef : 0.8\n",
      "step 392 gen_loss : 1.9308076 disc_loss : 1.2236532 Bright coef : 0.9742869112688196\n",
      "step 393 gen_loss : 2.3116498 disc_loss : 1.6569645 Bright coef : 0.8577050378935595\n",
      "step 394 gen_loss : 4.472171 disc_loss : 1.5518286 Bright coef : 1.0508290244541156\n",
      "step 395 gen_loss : 2.4660566 disc_loss : 1.130561 Bright coef : 1.053232883306854\n",
      "step 396 gen_loss : 1.8390653 disc_loss : 1.344938 Bright coef : 0.8702376338377432\n",
      "step 397 gen_loss : 2.202216 disc_loss : 1.2854805 Bright coef : 0.8736858057266124\n",
      "step 398 gen_loss : 3.7521443 disc_loss : 1.4316382 Bright coef : 0.9413258520601399\n",
      "step 399 gen_loss : 2.1074436 disc_loss : 1.1415274 Bright coef : 0.9082665653474914\n",
      "step 400 gen_loss : 2.8449404 disc_loss : 1.447078 Bright coef : 0.8\n",
      "step 401 gen_loss : 2.6535153 disc_loss : 1.6457052 Bright coef : 0.8060511919992407\n",
      "step 402 gen_loss : 2.2914248 disc_loss : 1.5326807 Bright coef : 0.894636123169382\n",
      "step 403 gen_loss : 3.3291228 disc_loss : 1.6130571 Bright coef : 0.8095572333197204\n",
      "step 404 gen_loss : 3.7491107 disc_loss : 1.2056807 Bright coef : 1.047311581493697\n",
      "step 405 gen_loss : 3.6795907 disc_loss : 1.5666244 Bright coef : 0.9051640548122486\n",
      "step 406 gen_loss : 4.522073 disc_loss : 1.3321192 Bright coef : 0.8261121648145844\n",
      "step 407 gen_loss : 3.1911814 disc_loss : 1.9013417 Bright coef : 0.9982976695702821\n",
      "step 408 gen_loss : 1.8015547 disc_loss : 1.6684182 Bright coef : 0.8\n",
      "step 409 gen_loss : 2.222609 disc_loss : 1.903083 Bright coef : 0.8\n",
      "step 410 gen_loss : 2.6342213 disc_loss : 1.4397798 Bright coef : 0.8\n",
      "step 411 gen_loss : 2.7083333 disc_loss : 1.122741 Bright coef : 0.9023669031989716\n",
      "step 412 gen_loss : 3.4939365 disc_loss : 1.9194884 Bright coef : 1.0927112035055127\n",
      "step 413 gen_loss : 2.3237064 disc_loss : 1.1566445 Bright coef : 0.8\n",
      "step 414 gen_loss : 3.828602 disc_loss : 1.3982499 Bright coef : 0.8816377874703554\n",
      "step 415 gen_loss : 2.2194438 disc_loss : 1.2909179 Bright coef : 1.0529075475103495\n",
      "step 416 gen_loss : 2.453073 disc_loss : 1.6163268 Bright coef : 0.8449153058190982\n",
      "step 417 gen_loss : 2.5229647 disc_loss : 2.1510005 Bright coef : 0.8\n",
      "step 418 gen_loss : 3.717365 disc_loss : 1.5537146 Bright coef : 0.8\n",
      "step 419 gen_loss : 2.3973153 disc_loss : 1.7495456 Bright coef : 0.8\n",
      "step 420 gen_loss : 2.5086174 disc_loss : 1.4064338 Bright coef : 0.8\n",
      "step 421 gen_loss : 2.1408467 disc_loss : 1.3740027 Bright coef : 0.9355706255771652\n",
      "step 422 gen_loss : 4.1143374 disc_loss : 1.316725 Bright coef : 1.1\n",
      "step 423 gen_loss : 2.1600964 disc_loss : 1.505169 Bright coef : 0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 424 gen_loss : 2.329239 disc_loss : 1.5348519 Bright coef : 0.8\n",
      "step 425 gen_loss : 3.2961576 disc_loss : 1.3846605 Bright coef : 1.1\n",
      "step 426 gen_loss : 3.3715668 disc_loss : 1.5187633 Bright coef : 1.1\n",
      "step 427 gen_loss : 2.963901 disc_loss : 1.3846856 Bright coef : 0.8399413041320759\n",
      "step 428 gen_loss : 2.5489798 disc_loss : 1.430404 Bright coef : 0.8\n",
      "step 429 gen_loss : 2.1642387 disc_loss : 1.7608416 Bright coef : 0.8\n",
      "step 430 gen_loss : 2.352018 disc_loss : 1.8088486 Bright coef : 0.8649399701226085\n",
      "step 431 gen_loss : 3.2896643 disc_loss : 2.0972762 Bright coef : 1.1\n",
      "step 432 gen_loss : 2.935738 disc_loss : 1.6077318 Bright coef : 1.1\n",
      "step 433 gen_loss : 2.3114133 disc_loss : 1.4543335 Bright coef : 0.9075059029888585\n",
      "step 434 gen_loss : 1.814586 disc_loss : 1.4874501 Bright coef : 0.8742206873517804\n",
      "step 435 gen_loss : 2.5432487 disc_loss : 1.4183813 Bright coef : 0.9055620673522242\n",
      "step 436 gen_loss : 2.9288654 disc_loss : 1.6495602 Bright coef : 1.0664578093464596\n",
      "step 437 gen_loss : 2.545817 disc_loss : 1.6517652 Bright coef : 1.0460837130306375\n",
      "step 438 gen_loss : 3.2860808 disc_loss : 1.5277977 Bright coef : 1.0969336021299028\n",
      "step 439 gen_loss : 2.5697522 disc_loss : 1.4409474 Bright coef : 1.0174827331601444\n",
      "step 440 gen_loss : 2.5372744 disc_loss : 1.8037841 Bright coef : 0.8\n",
      "step 441 gen_loss : 2.3030176 disc_loss : 1.5649598 Bright coef : 0.8259959243598463\n",
      "step 442 gen_loss : 2.4465532 disc_loss : 1.3298461 Bright coef : 1.077704571873025\n",
      "step 443 gen_loss : 1.5889423 disc_loss : 1.8256755 Bright coef : 0.9256198052623753\n",
      "step 444 gen_loss : 2.7400177 disc_loss : 1.5784662 Bright coef : 1.036067231847774\n",
      "step 445 gen_loss : 3.6647143 disc_loss : 1.5051093 Bright coef : 1.1\n",
      "step 446 gen_loss : 3.4635143 disc_loss : 1.3461843 Bright coef : 0.9937312414495998\n",
      "step 447 gen_loss : 4.443357 disc_loss : 1.7915958 Bright coef : 0.8\n",
      "step 448 gen_loss : 2.9056687 disc_loss : 1.867125 Bright coef : 0.8269607875131444\n",
      "step 449 gen_loss : 4.1074123 disc_loss : 1.8883011 Bright coef : 1.0144413376307952\n",
      "step 450 gen_loss : 2.7583191 disc_loss : 1.3224498 Bright coef : 0.8\n",
      "step 451 gen_loss : 2.0258682 disc_loss : 1.6277907 Bright coef : 1.1\n",
      "step 452 gen_loss : 2.1086776 disc_loss : 1.4460133 Bright coef : 0.9339210288771636\n",
      "step 453 gen_loss : 2.7757561 disc_loss : 1.4229763 Bright coef : 1.053243115879535\n",
      "step 454 gen_loss : 2.2545724 disc_loss : 1.5675888 Bright coef : 0.8\n",
      "step 455 gen_loss : 4.2141027 disc_loss : 1.5886376 Bright coef : 1.1\n",
      "step 456 gen_loss : 4.489075 disc_loss : 1.4633054 Bright coef : 0.9565007489657744\n",
      "step 457 gen_loss : 4.0291967 disc_loss : 1.7742202 Bright coef : 0.8\n",
      "step 458 gen_loss : 3.3208725 disc_loss : 1.4459397 Bright coef : 0.8453923113944605\n",
      "step 459 gen_loss : 2.65652 disc_loss : 1.5056243 Bright coef : 0.8\n",
      "step 460 gen_loss : 2.9653478 disc_loss : 1.516375 Bright coef : 0.9448325942607902\n",
      "step 461 gen_loss : 2.540789 disc_loss : 1.6701936 Bright coef : 0.8222087538796428\n",
      "step 462 gen_loss : 2.9698527 disc_loss : 1.2971098 Bright coef : 0.8\n",
      "step 463 gen_loss : 2.4781861 disc_loss : 1.5541996 Bright coef : 0.882154467907174\n",
      "step 464 gen_loss : 2.3042755 disc_loss : 1.763036 Bright coef : 1.1\n",
      "step 465 gen_loss : 2.0761864 disc_loss : 1.5635073 Bright coef : 0.8101812738732088\n",
      "step 466 gen_loss : 2.0944078 disc_loss : 1.3402877 Bright coef : 0.8\n",
      "step 467 gen_loss : 1.8474153 disc_loss : 1.7307274 Bright coef : 0.8557762019677229\n",
      "step 468 gen_loss : 2.1955624 disc_loss : 1.7009585 Bright coef : 0.9992321107367099\n",
      "step 469 gen_loss : 2.246509 disc_loss : 1.4536791 Bright coef : 0.8179977714433202\n",
      "step 470 gen_loss : 2.1192782 disc_loss : 1.6516156 Bright coef : 0.8\n",
      "step 471 gen_loss : 2.7616591 disc_loss : 1.3717695 Bright coef : 0.9868235000405006\n",
      "step 472 gen_loss : 2.121781 disc_loss : 1.7300441 Bright coef : 0.836227402665623\n",
      "step 473 gen_loss : 3.13244 disc_loss : 1.7055618 Bright coef : 1.1\n",
      "step 474 gen_loss : 1.8338196 disc_loss : 1.604764 Bright coef : 0.8\n",
      "step 475 gen_loss : 3.9862142 disc_loss : 1.4680297 Bright coef : 1.1\n",
      "step 476 gen_loss : 2.9164093 disc_loss : 1.3202721 Bright coef : 1.0691421313388563\n",
      "step 477 gen_loss : 2.6917386 disc_loss : 1.8508054 Bright coef : 0.8\n",
      "step 478 gen_loss : 5.6555805 disc_loss : 1.3692298 Bright coef : 1.1\n",
      "step 479 gen_loss : 2.9066012 disc_loss : 1.5655237 Bright coef : 1.0517005832705102\n",
      "step 480 gen_loss : 3.1281972 disc_loss : 1.5999823 Bright coef : 0.8283140421724844\n",
      "step 481 gen_loss : 1.7336186 disc_loss : 1.6055112 Bright coef : 0.93043126034972\n",
      "step 482 gen_loss : 1.9169829 disc_loss : 1.554534 Bright coef : 0.8343848311297943\n",
      "step 483 gen_loss : 2.290097 disc_loss : 1.2994174 Bright coef : 0.9310083747090766\n",
      "step 484 gen_loss : 2.801692 disc_loss : 1.4580306 Bright coef : 1.1\n",
      "step 485 gen_loss : 2.360495 disc_loss : 1.4164715 Bright coef : 0.8356639378454986\n",
      "step 486 gen_loss : 2.515293 disc_loss : 1.5339463 Bright coef : 0.8\n",
      "step 487 gen_loss : 2.9960575 disc_loss : 1.3974752 Bright coef : 0.8\n",
      "step 488 gen_loss : 3.243355 disc_loss : 1.5619689 Bright coef : 1.1\n",
      "step 489 gen_loss : 2.4580784 disc_loss : 1.3047898 Bright coef : 0.9095280987531585\n",
      "step 490 gen_loss : 4.135363 disc_loss : 1.3648046 Bright coef : 0.8\n",
      "step 491 gen_loss : 2.4558797 disc_loss : 1.5785195 Bright coef : 1.1\n",
      "step 492 gen_loss : 1.9957355 disc_loss : 1.8333664 Bright coef : 0.8143949456680304\n",
      "step 493 gen_loss : 2.157379 disc_loss : 1.4664756 Bright coef : 0.8\n",
      "step 494 gen_loss : 1.9811504 disc_loss : 1.3571548 Bright coef : 1.0801928592452263\n",
      "step 495 gen_loss : 1.9267459 disc_loss : 1.7001 Bright coef : 0.8\n",
      "step 496 gen_loss : 2.041607 disc_loss : 1.6317505 Bright coef : 0.9980428777210032\n",
      "step 497 gen_loss : 4.159817 disc_loss : 1.7136257 Bright coef : 1.1\n",
      "step 498 gen_loss : 2.883817 disc_loss : 1.3983955 Bright coef : 0.8\n",
      "step 499 gen_loss : 2.7304792 disc_loss : 1.4053164 Bright coef : 0.814554961225194\n",
      "step 500 gen_loss : 3.1375413 disc_loss : 1.3091447 Bright coef : 1.074619563759073\n",
      "step 501 gen_loss : 2.4235783 disc_loss : 1.5460607 Bright coef : 0.9352515566492843\n",
      "step 502 gen_loss : 4.874089 disc_loss : 1.6268914 Bright coef : 1.1\n",
      "step 503 gen_loss : 2.2814186 disc_loss : 1.9098043 Bright coef : 0.8\n",
      "step 504 gen_loss : 2.8671892 disc_loss : 1.9847931 Bright coef : 0.8\n",
      "step 505 gen_loss : 2.711008 disc_loss : 1.3895795 Bright coef : 0.9618262835774734\n",
      "step 506 gen_loss : 2.3075702 disc_loss : 1.7223285 Bright coef : 0.983889952735529\n",
      "step 507 gen_loss : 2.8262148 disc_loss : 1.4836982 Bright coef : 0.8\n",
      "step 508 gen_loss : 2.0413394 disc_loss : 1.875804 Bright coef : 0.8\n",
      "step 509 gen_loss : 2.874461 disc_loss : 1.4995339 Bright coef : 0.8\n",
      "step 510 gen_loss : 1.7824099 disc_loss : 1.5405117 Bright coef : 0.8\n",
      "step 511 gen_loss : 2.9283056 disc_loss : 1.5572233 Bright coef : 0.9833613652312598\n",
      "step 512 gen_loss : 3.4989722 disc_loss : 1.1112657 Bright coef : 1.060594852757132\n",
      "step 513 gen_loss : 3.0593953 disc_loss : 1.378432 Bright coef : 0.8\n",
      "step 514 gen_loss : 1.7248319 disc_loss : 1.8558569 Bright coef : 1.0300697508698209\n",
      "step 515 gen_loss : 2.2317736 disc_loss : 1.6770029 Bright coef : 0.8\n",
      "step 516 gen_loss : 2.0358753 disc_loss : 1.3179947 Bright coef : 0.9937487545004638\n",
      "step 517 gen_loss : 2.1255765 disc_loss : 1.6322705 Bright coef : 0.8\n",
      "step 518 gen_loss : 2.3347225 disc_loss : 1.4238102 Bright coef : 1.003629609377764\n",
      "step 519 gen_loss : 1.8639244 disc_loss : 1.140442 Bright coef : 0.8776021787923041\n",
      "step 520 gen_loss : 2.1232557 disc_loss : 1.2496599 Bright coef : 0.8056956242907594\n",
      "step 521 gen_loss : 3.0657022 disc_loss : 1.5467584 Bright coef : 1.0603302584391483\n",
      "step 522 gen_loss : 2.2139645 disc_loss : 1.6895366 Bright coef : 0.8\n",
      "step 523 gen_loss : 2.7785912 disc_loss : 1.7061691 Bright coef : 0.8\n",
      "step 524 gen_loss : 1.5753702 disc_loss : 1.380386 Bright coef : 0.8\n",
      "step 525 gen_loss : 2.3966365 disc_loss : 1.131577 Bright coef : 0.8\n",
      "step 526 gen_loss : 2.385316 disc_loss : 1.0887272 Bright coef : 0.8983895365298786\n",
      "step 527 gen_loss : 3.7591355 disc_loss : 1.3611388 Bright coef : 0.8994956499681009\n",
      "image_00000087_2.png\n",
      "image_00000088-3.png\n",
      "image_00000092-3.png\n",
      "image_00000092_1.png\n",
      "image_00000095_2.png\n",
      "image_00000096-3.png\n",
      "image_00000099-3.png\n",
      "image_00000099_1.png\n",
      "image_00000107-3.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_00000108-3.png\n",
      "image_00000109-3.png\n",
      "image_00000110-3.png\n",
      "image_00000117-3.png\n",
      "image_00000118-3.png\n",
      "image_00000122-3.png\n",
      "image_00000123-3.png\n",
      "image_00000128-3.png\n",
      "image_00000129-3.png\n",
      "image_00000134-3.png\n",
      "image_00000135-3.png\n",
      "image_00000138-3.png\n",
      "image_00000139-3.png\n",
      "image_00000147-3.png\n",
      "image_00000148-3.png\n",
      "X :  [1]\n",
      "G :  [3.0222337]\n",
      "D :  [1.5251389]\n",
      "T_avg :  [0.7182927]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed :  5848\n",
      "\n",
      "Start of epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:1: Invalid control characters encountered in text.\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:3: Expected identifier, got: 4040241663323905875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 gen_loss : 2.9491725 disc_loss : 1.3831406 Bright coef : 1.0587693022666198\n",
      "step 1 gen_loss : 2.2093098 disc_loss : 1.535143 Bright coef : 0.8689617695098424\n",
      "step 2 gen_loss : 4.1197677 disc_loss : 1.7146344 Bright coef : 0.8\n",
      "step 3 gen_loss : 3.993009 disc_loss : 1.4055902 Bright coef : 0.8\n",
      "step 4 gen_loss : 2.3628333 disc_loss : 1.680497 Bright coef : 0.8\n",
      "step 5 gen_loss : 3.0850148 disc_loss : 1.3897853 Bright coef : 0.9627067462785022\n",
      "step 6 gen_loss : 2.1749895 disc_loss : 1.3520195 Bright coef : 0.8\n",
      "step 7 gen_loss : 2.373806 disc_loss : 1.5326481 Bright coef : 0.8396101120999135\n",
      "step 8 gen_loss : 3.4394953 disc_loss : 1.4737265 Bright coef : 0.9483525271688995\n",
      "step 9 gen_loss : 3.602229 disc_loss : 1.7736237 Bright coef : 0.9632299446206054\n",
      "step 10 gen_loss : 1.777467 disc_loss : 1.8351221 Bright coef : 0.8\n",
      "step 11 gen_loss : 3.4508572 disc_loss : 1.2234412 Bright coef : 0.8075338977422939\n",
      "step 12 gen_loss : 5.009096 disc_loss : 1.2732877 Bright coef : 1.1\n",
      "step 13 gen_loss : 2.23959 disc_loss : 1.3700902 Bright coef : 0.8653041787704135\n",
      "step 14 gen_loss : 2.1052794 disc_loss : 1.3327274 Bright coef : 0.8017011522129449\n",
      "step 15 gen_loss : 1.8062762 disc_loss : 1.475031 Bright coef : 0.8524379956900897\n",
      "step 16 gen_loss : 2.6468768 disc_loss : 1.6663654 Bright coef : 0.8639928607377364\n",
      "step 17 gen_loss : 2.1434593 disc_loss : 1.3111031 Bright coef : 1.1\n",
      "step 18 gen_loss : 2.5976157 disc_loss : 1.4410295 Bright coef : 1.1\n",
      "step 19 gen_loss : 2.1392145 disc_loss : 1.790989 Bright coef : 0.8\n",
      "step 20 gen_loss : 3.2761285 disc_loss : 1.5440217 Bright coef : 1.1\n",
      "step 21 gen_loss : 2.2815018 disc_loss : 1.3624377 Bright coef : 0.8\n",
      "step 22 gen_loss : 3.5427954 disc_loss : 1.4845772 Bright coef : 0.8398972465229072\n",
      "step 23 gen_loss : 1.9427234 disc_loss : 1.2824767 Bright coef : 0.9108327931173511\n",
      "step 24 gen_loss : 2.348552 disc_loss : 1.452771 Bright coef : 0.8\n",
      "step 25 gen_loss : 2.009507 disc_loss : 1.5546186 Bright coef : 0.8\n",
      "step 26 gen_loss : 3.17027 disc_loss : 1.4699261 Bright coef : 1.1\n",
      "step 27 gen_loss : 2.5504842 disc_loss : 1.2427092 Bright coef : 0.8536391067063083\n",
      "step 28 gen_loss : 2.9010808 disc_loss : 1.5859606 Bright coef : 0.8\n",
      "step 29 gen_loss : 1.7990705 disc_loss : 1.306105 Bright coef : 0.8\n",
      "step 30 gen_loss : 3.19828 disc_loss : 1.5684006 Bright coef : 0.8\n",
      "step 31 gen_loss : 2.1705804 disc_loss : 1.4064248 Bright coef : 0.8\n",
      "step 32 gen_loss : 1.6985706 disc_loss : 1.3103633 Bright coef : 0.8\n",
      "step 33 gen_loss : 1.9276121 disc_loss : 1.7629675 Bright coef : 0.829114946900296\n",
      "step 34 gen_loss : 3.4635963 disc_loss : 1.4464872 Bright coef : 1.1\n",
      "step 35 gen_loss : 3.3155408 disc_loss : 1.2261539 Bright coef : 0.8\n",
      "step 36 gen_loss : 2.046702 disc_loss : 1.373846 Bright coef : 0.8\n",
      "step 37 gen_loss : 2.899293 disc_loss : 1.5068487 Bright coef : 0.9767995591697439\n",
      "step 38 gen_loss : 2.979334 disc_loss : 1.935422 Bright coef : 1.1\n",
      "step 39 gen_loss : 2.9538546 disc_loss : 1.170093 Bright coef : 0.9310112143904736\n",
      "step 40 gen_loss : 3.2491508 disc_loss : 1.3300364 Bright coef : 0.8\n",
      "step 41 gen_loss : 2.8310132 disc_loss : 1.712738 Bright coef : 0.8\n",
      "step 42 gen_loss : 2.5248966 disc_loss : 1.4435406 Bright coef : 0.8\n",
      "step 43 gen_loss : 1.9336534 disc_loss : 1.5970737 Bright coef : 1.0609499396321593\n",
      "step 44 gen_loss : 2.215906 disc_loss : 1.7986414 Bright coef : 0.997017632542088\n",
      "step 45 gen_loss : 1.5382082 disc_loss : 1.7740529 Bright coef : 0.8659710823862625\n",
      "step 46 gen_loss : 3.1501954 disc_loss : 1.2523084 Bright coef : 1.0074716037424432\n",
      "step 47 gen_loss : 2.3276749 disc_loss : 1.1607282 Bright coef : 0.8515023039067763\n",
      "step 48 gen_loss : 1.8097173 disc_loss : 1.571811 Bright coef : 0.8\n",
      "step 49 gen_loss : 2.9595854 disc_loss : 1.4432921 Bright coef : 0.9459179372048919\n",
      "step 50 gen_loss : 2.1946254 disc_loss : 1.3428668 Bright coef : 0.8\n",
      "step 51 gen_loss : 1.551281 disc_loss : 1.6650796 Bright coef : 0.8831362498671207\n",
      "step 52 gen_loss : 3.2896683 disc_loss : 1.3665594 Bright coef : 1.1\n",
      "step 53 gen_loss : 3.5938697 disc_loss : 1.9962217 Bright coef : 1.0461737968028249\n",
      "step 54 gen_loss : 3.769536 disc_loss : 1.1186955 Bright coef : 1.0532043698643552\n",
      "step 55 gen_loss : 2.863808 disc_loss : 1.508513 Bright coef : 0.8\n",
      "step 56 gen_loss : 5.4126835 disc_loss : 1.2735167 Bright coef : 1.1\n",
      "step 57 gen_loss : 4.617715 disc_loss : 1.5449071 Bright coef : 1.1\n",
      "step 58 gen_loss : 2.8458605 disc_loss : 1.4834886 Bright coef : 0.9339325818727613\n",
      "step 59 gen_loss : 1.8797098 disc_loss : 1.4582887 Bright coef : 0.8351229533217952\n",
      "step 60 gen_loss : 3.2105618 disc_loss : 1.4654698 Bright coef : 1.0828581014428442\n",
      "step 61 gen_loss : 3.6780627 disc_loss : 1.3464843 Bright coef : 1.0197936393660016\n",
      "step 62 gen_loss : 3.7469368 disc_loss : 1.3338628 Bright coef : 0.8\n",
      "step 63 gen_loss : 2.3948653 disc_loss : 1.8362689 Bright coef : 0.9818840118317402\n",
      "step 64 gen_loss : 4.790681 disc_loss : 1.260571 Bright coef : 1.0403487542065672\n",
      "step 65 gen_loss : 2.5353942 disc_loss : 1.6990553 Bright coef : 1.0514657292962286\n",
      "step 66 gen_loss : 2.8107827 disc_loss : 1.3240788 Bright coef : 1.0804048626767409\n",
      "step 67 gen_loss : 1.9141637 disc_loss : 1.6386951 Bright coef : 0.8106146016024701\n",
      "step 68 gen_loss : 4.7718763 disc_loss : 1.3929894 Bright coef : 1.1\n",
      "step 69 gen_loss : 2.3655863 disc_loss : 1.2486274 Bright coef : 0.8\n",
      "step 70 gen_loss : 1.7185134 disc_loss : 1.8955933 Bright coef : 0.9013544391611099\n",
      "step 71 gen_loss : 2.348093 disc_loss : 1.6622261 Bright coef : 0.8731549054042802\n",
      "step 72 gen_loss : 2.0235236 disc_loss : 1.5890567 Bright coef : 0.8\n",
      "step 73 gen_loss : 2.2479255 disc_loss : 1.6593277 Bright coef : 0.9556253140929544\n",
      "step 74 gen_loss : 2.9064577 disc_loss : 1.2038532 Bright coef : 0.8\n",
      "step 75 gen_loss : 2.9538605 disc_loss : 1.4816447 Bright coef : 0.9983519043441444\n",
      "step 76 gen_loss : 5.2164764 disc_loss : 1.4556673 Bright coef : 0.9089992779564299\n",
      "step 77 gen_loss : 2.9269676 disc_loss : 1.541252 Bright coef : 0.8595299957844652\n",
      "step 78 gen_loss : 3.2493114 disc_loss : 1.6675172 Bright coef : 0.8\n",
      "step 79 gen_loss : 1.9593042 disc_loss : 1.505491 Bright coef : 0.8353611166743871\n",
      "step 80 gen_loss : 1.9914509 disc_loss : 1.7363424 Bright coef : 0.8\n",
      "step 81 gen_loss : 1.6910232 disc_loss : 1.8504252 Bright coef : 0.8\n",
      "step 82 gen_loss : 2.72679 disc_loss : 1.1730136 Bright coef : 1.0198969792336283\n",
      "step 83 gen_loss : 1.8864112 disc_loss : 1.2233152 Bright coef : 0.8343140113057403\n",
      "step 84 gen_loss : 1.8302419 disc_loss : 1.3423605 Bright coef : 0.8\n",
      "step 85 gen_loss : 2.0914207 disc_loss : 1.8759614 Bright coef : 0.935046218320266\n",
      "step 86 gen_loss : 3.588698 disc_loss : 1.3219357 Bright coef : 1.1\n",
      "step 87 gen_loss : 2.2499037 disc_loss : 1.5531774 Bright coef : 0.8\n",
      "step 88 gen_loss : 1.5115951 disc_loss : 1.6995097 Bright coef : 0.9535658133419034\n",
      "step 89 gen_loss : 1.6184438 disc_loss : 1.8102082 Bright coef : 0.9263125638986164\n",
      "step 90 gen_loss : 2.27169 disc_loss : 1.4355962 Bright coef : 0.8\n",
      "step 91 gen_loss : 2.4649076 disc_loss : 1.3552458 Bright coef : 0.8\n",
      "step 92 gen_loss : 2.4391344 disc_loss : 1.3867692 Bright coef : 0.8911303491329768\n",
      "step 93 gen_loss : 2.4524853 disc_loss : 1.106452 Bright coef : 0.8\n",
      "step 94 gen_loss : 2.7626863 disc_loss : 1.5284779 Bright coef : 1.0805438778542764\n",
      "step 95 gen_loss : 2.2471402 disc_loss : 1.8227019 Bright coef : 0.872555374064515\n",
      "step 96 gen_loss : 2.0863912 disc_loss : 1.3308489 Bright coef : 0.8656989517787307\n",
      "step 97 gen_loss : 1.7143798 disc_loss : 1.3915443 Bright coef : 0.8319006722450917\n",
      "step 98 gen_loss : 1.863028 disc_loss : 1.4067662 Bright coef : 0.8\n",
      "step 99 gen_loss : 2.167853 disc_loss : 1.7857114 Bright coef : 0.8\n",
      "step 100 gen_loss : 3.9293776 disc_loss : 1.4289117 Bright coef : 1.0209340398835198\n",
      "step 101 gen_loss : 3.4300237 disc_loss : 1.3227854 Bright coef : 0.8\n",
      "step 102 gen_loss : 2.5787783 disc_loss : 1.5210395 Bright coef : 0.8180882731194679\n",
      "step 103 gen_loss : 2.75649 disc_loss : 1.6466141 Bright coef : 0.8\n",
      "step 104 gen_loss : 3.3350663 disc_loss : 1.857762 Bright coef : 1.1\n",
      "step 105 gen_loss : 1.915626 disc_loss : 1.7324271 Bright coef : 1.1\n",
      "step 106 gen_loss : 2.9261818 disc_loss : 1.2425334 Bright coef : 0.977210307006345\n",
      "step 107 gen_loss : 2.4977527 disc_loss : 1.4145396 Bright coef : 1.0511775140677029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 108 gen_loss : 2.758548 disc_loss : 1.3434111 Bright coef : 0.8\n",
      "step 109 gen_loss : 1.8769475 disc_loss : 1.7717218 Bright coef : 0.9241841559492932\n",
      "step 110 gen_loss : 2.666643 disc_loss : 1.4202849 Bright coef : 0.9185179163697358\n",
      "step 111 gen_loss : 2.6797526 disc_loss : 1.6641606 Bright coef : 0.9915939288202404\n",
      "step 112 gen_loss : 3.2538428 disc_loss : 1.3533462 Bright coef : 0.8\n",
      "step 113 gen_loss : 2.4061189 disc_loss : 1.6967955 Bright coef : 0.9596292196178903\n",
      "step 114 gen_loss : 4.3034973 disc_loss : 1.315993 Bright coef : 1.1\n",
      "step 115 gen_loss : 1.6253263 disc_loss : 1.8085537 Bright coef : 0.9462299456552004\n",
      "step 116 gen_loss : 2.8872902 disc_loss : 1.3319111 Bright coef : 0.8\n",
      "step 117 gen_loss : 2.02601 disc_loss : 1.4069209 Bright coef : 0.8118914799324181\n",
      "step 118 gen_loss : 2.7609882 disc_loss : 1.8863955 Bright coef : 0.8\n",
      "step 119 gen_loss : 3.94405 disc_loss : 1.3446858 Bright coef : 1.068415685595328\n",
      "step 120 gen_loss : 2.0771797 disc_loss : 1.4594576 Bright coef : 0.8\n",
      "step 121 gen_loss : 2.6553547 disc_loss : 1.5981004 Bright coef : 0.8062580223767672\n",
      "step 122 gen_loss : 2.8926766 disc_loss : 1.8504822 Bright coef : 1.0726698633224372\n",
      "step 123 gen_loss : 2.8999012 disc_loss : 1.26581 Bright coef : 1.1\n",
      "step 124 gen_loss : 2.1261206 disc_loss : 1.3496422 Bright coef : 0.8870451395129358\n",
      "step 125 gen_loss : 2.423739 disc_loss : 1.732235 Bright coef : 0.8182011147640726\n",
      "step 126 gen_loss : 3.2237613 disc_loss : 1.7483547 Bright coef : 0.8609698789188607\n",
      "step 127 gen_loss : 3.9500244 disc_loss : 1.2681842 Bright coef : 1.1\n",
      "step 128 gen_loss : 1.670131 disc_loss : 1.5055969 Bright coef : 0.8689247324894607\n",
      "step 129 gen_loss : 2.1487927 disc_loss : 1.6751256 Bright coef : 0.8\n",
      "step 130 gen_loss : 1.786394 disc_loss : 1.5668988 Bright coef : 0.8211405514678862\n",
      "step 131 gen_loss : 1.7787726 disc_loss : 1.5799015 Bright coef : 0.8\n",
      "step 132 gen_loss : 6.4225154 disc_loss : 1.4452081 Bright coef : 1.0925816851821797\n",
      "step 133 gen_loss : 3.3221662 disc_loss : 1.305207 Bright coef : 1.1\n",
      "step 134 gen_loss : 3.0868607 disc_loss : 1.4773734 Bright coef : 0.8\n",
      "step 135 gen_loss : 2.976042 disc_loss : 1.1236317 Bright coef : 0.9362090713426232\n",
      "step 136 gen_loss : 3.782015 disc_loss : 1.3798485 Bright coef : 1.1\n",
      "step 137 gen_loss : 3.028505 disc_loss : 1.234815 Bright coef : 1.090394292725815\n",
      "step 138 gen_loss : 2.762134 disc_loss : 1.18442 Bright coef : 1.1\n",
      "step 139 gen_loss : 2.999279 disc_loss : 1.3850957 Bright coef : 0.8273544053417365\n",
      "step 140 gen_loss : 3.1050434 disc_loss : 1.782582 Bright coef : 0.8\n",
      "step 141 gen_loss : 5.01197 disc_loss : 1.8935899 Bright coef : 1.1\n",
      "step 142 gen_loss : 2.4669352 disc_loss : 2.113054 Bright coef : 1.0105201347785384\n",
      "step 143 gen_loss : 2.7520645 disc_loss : 1.2578014 Bright coef : 0.8\n",
      "step 144 gen_loss : 2.3535779 disc_loss : 2.0041618 Bright coef : 0.8384912460411951\n",
      "step 145 gen_loss : 2.4017725 disc_loss : 1.5148115 Bright coef : 0.8909237701923214\n",
      "step 146 gen_loss : 2.8896823 disc_loss : 1.3759357 Bright coef : 0.8\n",
      "step 147 gen_loss : 1.7127554 disc_loss : 1.2960619 Bright coef : 0.8217549539628048\n",
      "step 148 gen_loss : 2.1528351 disc_loss : 1.5925668 Bright coef : 0.8\n",
      "step 149 gen_loss : 2.7735894 disc_loss : 1.6764624 Bright coef : 1.0989423088848311\n",
      "step 150 gen_loss : 1.9096068 disc_loss : 1.5306768 Bright coef : 0.853605258584471\n",
      "step 151 gen_loss : 1.935285 disc_loss : 1.2834123 Bright coef : 0.8\n",
      "step 152 gen_loss : 3.2204623 disc_loss : 1.3850906 Bright coef : 0.9786632512477245\n",
      "step 153 gen_loss : 3.358481 disc_loss : 1.8395948 Bright coef : 0.8272445772041126\n",
      "step 154 gen_loss : 2.0399666 disc_loss : 1.154577 Bright coef : 0.8\n",
      "step 155 gen_loss : 2.264018 disc_loss : 1.1575689 Bright coef : 0.8\n",
      "step 156 gen_loss : 2.0386617 disc_loss : 1.9575274 Bright coef : 0.8726007324063938\n",
      "step 157 gen_loss : 2.1195178 disc_loss : 1.3099095 Bright coef : 0.8\n",
      "step 158 gen_loss : 2.4102564 disc_loss : 1.2808694 Bright coef : 0.8\n",
      "step 159 gen_loss : 2.2981343 disc_loss : 1.9705857 Bright coef : 0.8745471268672123\n",
      "step 160 gen_loss : 5.3090363 disc_loss : 1.7825929 Bright coef : 1.1\n",
      "step 161 gen_loss : 2.1117992 disc_loss : 1.45854 Bright coef : 0.9355344100372378\n",
      "step 162 gen_loss : 2.229998 disc_loss : 1.8854117 Bright coef : 0.8\n",
      "step 163 gen_loss : 1.9946696 disc_loss : 1.5226798 Bright coef : 0.8\n",
      "step 164 gen_loss : 1.8189306 disc_loss : 1.6236231 Bright coef : 0.8\n",
      "step 165 gen_loss : 3.6191573 disc_loss : 1.390671 Bright coef : 1.016264286848989\n",
      "step 166 gen_loss : 2.6038935 disc_loss : 1.5545952 Bright coef : 0.9585081852934575\n",
      "step 167 gen_loss : 3.126586 disc_loss : 1.4870534 Bright coef : 1.1\n",
      "step 168 gen_loss : 2.4436917 disc_loss : 1.8185036 Bright coef : 1.0288768980990575\n",
      "step 169 gen_loss : 3.0908694 disc_loss : 1.287821 Bright coef : 0.8\n",
      "step 170 gen_loss : 2.8610275 disc_loss : 1.2757413 Bright coef : 1.1\n",
      "step 171 gen_loss : 3.1138775 disc_loss : 1.3161926 Bright coef : 0.8225340565390875\n",
      "step 172 gen_loss : 3.6300962 disc_loss : 1.5573846 Bright coef : 0.8\n",
      "step 173 gen_loss : 2.8397253 disc_loss : 1.288337 Bright coef : 1.1\n",
      "step 174 gen_loss : 2.7829783 disc_loss : 1.6589669 Bright coef : 0.8\n",
      "step 175 gen_loss : 2.1114542 disc_loss : 1.2561239 Bright coef : 0.9084048144356022\n",
      "step 176 gen_loss : 2.0051706 disc_loss : 1.2662184 Bright coef : 0.9908038624582342\n",
      "step 177 gen_loss : 2.369207 disc_loss : 1.2608852 Bright coef : 1.1\n",
      "step 178 gen_loss : 2.5645785 disc_loss : 1.4466585 Bright coef : 0.8\n",
      "step 179 gen_loss : 2.013878 disc_loss : 1.1929169 Bright coef : 1.0397685678408508\n",
      "step 180 gen_loss : 2.7910428 disc_loss : 1.8027825 Bright coef : 0.9138556842516334\n",
      "step 181 gen_loss : 1.7748576 disc_loss : 1.4411978 Bright coef : 0.8581669734914686\n",
      "step 182 gen_loss : 3.229142 disc_loss : 1.3983781 Bright coef : 0.9769120680679123\n",
      "step 183 gen_loss : 2.1920636 disc_loss : 1.5049233 Bright coef : 1.0979046457286006\n",
      "step 184 gen_loss : 2.4677947 disc_loss : 1.6047843 Bright coef : 1.1\n",
      "step 185 gen_loss : 2.9712608 disc_loss : 1.3233414 Bright coef : 0.9868225874362112\n",
      "step 186 gen_loss : 2.9821765 disc_loss : 1.6241071 Bright coef : 0.9374499460450952\n",
      "step 187 gen_loss : 2.9650528 disc_loss : 1.726702 Bright coef : 0.8753186247937317\n",
      "step 188 gen_loss : 2.3095384 disc_loss : 1.6235942 Bright coef : 0.9458854261464706\n",
      "step 189 gen_loss : 1.8210453 disc_loss : 1.3131535 Bright coef : 0.8\n",
      "step 190 gen_loss : 4.6275144 disc_loss : 1.4869845 Bright coef : 1.1\n",
      "step 191 gen_loss : 3.2624109 disc_loss : 1.1612229 Bright coef : 1.0951091240945092\n",
      "step 192 gen_loss : 2.8455293 disc_loss : 1.2592769 Bright coef : 1.1\n",
      "step 193 gen_loss : 3.7970083 disc_loss : 1.5439274 Bright coef : 0.8\n",
      "step 194 gen_loss : 3.125852 disc_loss : 1.5605621 Bright coef : 0.8\n",
      "step 195 gen_loss : 2.7320304 disc_loss : 1.4039855 Bright coef : 0.8\n",
      "step 196 gen_loss : 2.5675216 disc_loss : 1.5465772 Bright coef : 0.8\n",
      "step 197 gen_loss : 2.7424636 disc_loss : 1.5190789 Bright coef : 0.8\n",
      "step 198 gen_loss : 1.8365687 disc_loss : 1.3800175 Bright coef : 0.8\n",
      "step 199 gen_loss : 2.424161 disc_loss : 1.363744 Bright coef : 0.8321720918255954\n",
      "step 200 gen_loss : 2.966882 disc_loss : 1.2976923 Bright coef : 0.830197022904497\n",
      "step 201 gen_loss : 1.9964058 disc_loss : 1.8371253 Bright coef : 0.8\n",
      "step 202 gen_loss : 3.0269277 disc_loss : 1.4250093 Bright coef : 0.8910909558080498\n",
      "step 203 gen_loss : 1.8322289 disc_loss : 1.561345 Bright coef : 0.8942638748973785\n",
      "step 204 gen_loss : 2.545712 disc_loss : 1.6377531 Bright coef : 0.8980031362583685\n",
      "step 205 gen_loss : 3.1052296 disc_loss : 1.6871076 Bright coef : 1.1\n",
      "step 206 gen_loss : 2.016757 disc_loss : 1.3895757 Bright coef : 0.8\n",
      "step 207 gen_loss : 4.125381 disc_loss : 1.351562 Bright coef : 0.8\n",
      "step 208 gen_loss : 2.5155752 disc_loss : 1.6212962 Bright coef : 0.8515470590774371\n",
      "step 209 gen_loss : 1.866981 disc_loss : 1.4120563 Bright coef : 0.8\n",
      "step 210 gen_loss : 2.5770786 disc_loss : 1.8162141 Bright coef : 1.001097673828888\n",
      "step 211 gen_loss : 3.670594 disc_loss : 1.7484505 Bright coef : 1.1\n",
      "step 212 gen_loss : 2.9846234 disc_loss : 1.3478733 Bright coef : 0.823411500802025\n",
      "step 213 gen_loss : 1.9653683 disc_loss : 1.3554771 Bright coef : 0.8\n",
      "step 214 gen_loss : 2.1432445 disc_loss : 1.6237583 Bright coef : 0.8306010602432762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 215 gen_loss : 2.6966875 disc_loss : 1.5262513 Bright coef : 0.8\n",
      "step 216 gen_loss : 4.4350934 disc_loss : 1.3638651 Bright coef : 1.1\n",
      "step 217 gen_loss : 3.4334047 disc_loss : 1.5169002 Bright coef : 1.1\n",
      "step 218 gen_loss : 2.2381585 disc_loss : 1.5593272 Bright coef : 0.9728606195131189\n",
      "step 219 gen_loss : 3.1498036 disc_loss : 1.2882376 Bright coef : 0.8615795603553673\n",
      "step 220 gen_loss : 3.0472987 disc_loss : 1.1695355 Bright coef : 1.1\n",
      "step 221 gen_loss : 3.4376216 disc_loss : 1.4737071 Bright coef : 0.8\n",
      "step 222 gen_loss : 3.2764366 disc_loss : 1.7835598 Bright coef : 1.0932658734218774\n",
      "step 223 gen_loss : 3.357698 disc_loss : 1.6421887 Bright coef : 0.8\n",
      "step 224 gen_loss : 1.8211514 disc_loss : 1.2991786 Bright coef : 0.9438684231341818\n",
      "step 225 gen_loss : 2.4867985 disc_loss : 1.3821254 Bright coef : 1.1\n",
      "step 226 gen_loss : 3.9022539 disc_loss : 1.6041498 Bright coef : 1.1\n",
      "step 227 gen_loss : 2.5763419 disc_loss : 1.807657 Bright coef : 0.8\n",
      "step 228 gen_loss : 2.35437 disc_loss : 1.5284166 Bright coef : 1.005408673057184\n",
      "step 229 gen_loss : 2.4158535 disc_loss : 1.6229627 Bright coef : 0.8\n",
      "step 230 gen_loss : 2.6123884 disc_loss : 1.3253009 Bright coef : 1.1\n",
      "step 231 gen_loss : 2.6831446 disc_loss : 1.5307443 Bright coef : 0.8103534202464265\n",
      "step 232 gen_loss : 3.0069287 disc_loss : 1.1139028 Bright coef : 0.91679860816967\n",
      "step 233 gen_loss : 3.2977862 disc_loss : 1.3360751 Bright coef : 1.0904187259377398\n",
      "step 234 gen_loss : 3.0382671 disc_loss : 1.1296881 Bright coef : 1.022348229152196\n",
      "step 235 gen_loss : 3.1082919 disc_loss : 1.3554243 Bright coef : 0.8\n",
      "step 236 gen_loss : 2.7068346 disc_loss : 1.3378651 Bright coef : 1.1\n",
      "step 237 gen_loss : 4.1302986 disc_loss : 1.6950015 Bright coef : 0.8\n",
      "step 238 gen_loss : 2.350271 disc_loss : 1.2974157 Bright coef : 1.0182091605484387\n",
      "step 239 gen_loss : 1.8411158 disc_loss : 1.6730112 Bright coef : 1.0296308064627655\n",
      "step 240 gen_loss : 1.5164406 disc_loss : 1.8882545 Bright coef : 0.9160886860585544\n",
      "step 241 gen_loss : 3.7722168 disc_loss : 1.4901886 Bright coef : 1.1\n",
      "step 242 gen_loss : 2.6352127 disc_loss : 1.6922705 Bright coef : 0.9376857676102155\n",
      "step 243 gen_loss : 3.169482 disc_loss : 1.5435987 Bright coef : 0.8604603206484656\n",
      "step 244 gen_loss : 2.2407324 disc_loss : 1.3406532 Bright coef : 0.9484291191394509\n",
      "step 245 gen_loss : 2.275462 disc_loss : 1.3944614 Bright coef : 1.037234664462905\n",
      "step 246 gen_loss : 3.03559 disc_loss : 1.481478 Bright coef : 0.8\n",
      "step 247 gen_loss : 3.3060615 disc_loss : 1.5348086 Bright coef : 0.8293629753404692\n",
      "step 248 gen_loss : 2.8914285 disc_loss : 1.5866631 Bright coef : 0.8\n",
      "step 249 gen_loss : 3.3527734 disc_loss : 1.3974524 Bright coef : 0.9482457548666992\n",
      "step 250 gen_loss : 2.4681423 disc_loss : 1.4630015 Bright coef : 0.885586115038331\n",
      "step 251 gen_loss : 3.888055 disc_loss : 1.3919492 Bright coef : 0.8\n",
      "step 252 gen_loss : 3.626504 disc_loss : 1.5192431 Bright coef : 0.9988918766751276\n",
      "step 253 gen_loss : 2.058114 disc_loss : 1.367259 Bright coef : 0.8636885158952681\n",
      "step 254 gen_loss : 1.9301271 disc_loss : 1.6270705 Bright coef : 0.8\n",
      "step 255 gen_loss : 2.0658426 disc_loss : 1.6802394 Bright coef : 1.1\n",
      "step 256 gen_loss : 2.6195734 disc_loss : 1.6170171 Bright coef : 0.8\n",
      "step 257 gen_loss : 4.918632 disc_loss : 1.765824 Bright coef : 1.1\n",
      "step 258 gen_loss : 3.1432574 disc_loss : 1.2180717 Bright coef : 0.8\n",
      "step 259 gen_loss : 1.8732938 disc_loss : 1.4083668 Bright coef : 0.8261025883224674\n",
      "step 260 gen_loss : 2.6426919 disc_loss : 1.3124913 Bright coef : 0.8850049319027659\n",
      "step 261 gen_loss : 3.1917968 disc_loss : 1.5401613 Bright coef : 0.8\n",
      "step 262 gen_loss : 2.2859201 disc_loss : 1.453287 Bright coef : 0.9637176485827905\n",
      "step 263 gen_loss : 2.3239505 disc_loss : 1.370321 Bright coef : 1.0051000259457306\n",
      "step 264 gen_loss : 2.100931 disc_loss : 1.3106217 Bright coef : 0.8\n",
      "step 265 gen_loss : 2.630958 disc_loss : 1.4743891 Bright coef : 1.0170326625710047\n",
      "step 266 gen_loss : 2.0034542 disc_loss : 1.547677 Bright coef : 1.0086425582496616\n",
      "step 267 gen_loss : 2.136422 disc_loss : 1.6905699 Bright coef : 0.9371745301724856\n",
      "step 268 gen_loss : 1.6649165 disc_loss : 1.580863 Bright coef : 0.8276449521651406\n",
      "step 269 gen_loss : 3.648784 disc_loss : 1.505851 Bright coef : 0.8024840152599569\n",
      "step 270 gen_loss : 7.1764874 disc_loss : 1.5340714 Bright coef : 1.1\n",
      "step 271 gen_loss : 1.8530204 disc_loss : 1.7469112 Bright coef : 1.0525258943702696\n",
      "step 272 gen_loss : 1.8415742 disc_loss : 1.4188458 Bright coef : 1.0015999964829325\n",
      "step 273 gen_loss : 2.7029 disc_loss : 1.3402069 Bright coef : 0.8\n",
      "step 274 gen_loss : 2.1115916 disc_loss : 1.7052503 Bright coef : 0.8472299959481276\n",
      "step 275 gen_loss : 3.0751722 disc_loss : 1.2470385 Bright coef : 1.089806073329279\n",
      "step 276 gen_loss : 2.706412 disc_loss : 1.374794 Bright coef : 1.1\n",
      "step 277 gen_loss : 2.667068 disc_loss : 1.5427495 Bright coef : 1.0699558221590022\n",
      "step 278 gen_loss : 2.3664305 disc_loss : 1.1332967 Bright coef : 1.1\n",
      "step 279 gen_loss : 2.5233903 disc_loss : 1.4544549 Bright coef : 1.019348287989388\n",
      "step 280 gen_loss : 3.088118 disc_loss : 1.2696543 Bright coef : 1.055670597837722\n",
      "step 281 gen_loss : 3.9537008 disc_loss : 1.3021905 Bright coef : 1.0785430106095353\n",
      "step 282 gen_loss : 2.5628767 disc_loss : 1.4351566 Bright coef : 0.8\n",
      "step 283 gen_loss : 3.0515847 disc_loss : 1.553083 Bright coef : 0.8\n",
      "step 284 gen_loss : 2.3912263 disc_loss : 1.7232854 Bright coef : 0.8\n",
      "step 285 gen_loss : 2.6943548 disc_loss : 1.604929 Bright coef : 0.9868743968703002\n",
      "step 286 gen_loss : 2.3254697 disc_loss : 1.532367 Bright coef : 0.8\n",
      "step 287 gen_loss : 4.539317 disc_loss : 1.856519 Bright coef : 0.9970371854488402\n",
      "step 288 gen_loss : 2.718077 disc_loss : 1.6926382 Bright coef : 1.063160813191674\n",
      "step 289 gen_loss : 1.5939733 disc_loss : 1.5668535 Bright coef : 0.8325882080614875\n",
      "step 290 gen_loss : 2.6244526 disc_loss : 1.3326883 Bright coef : 0.804830366170501\n",
      "step 291 gen_loss : 2.3030581 disc_loss : 1.5019562 Bright coef : 0.8\n",
      "step 292 gen_loss : 2.1225863 disc_loss : 1.6040742 Bright coef : 0.8\n",
      "step 293 gen_loss : 5.146419 disc_loss : 1.6249434 Bright coef : 1.1\n",
      "step 294 gen_loss : 3.6574366 disc_loss : 1.2846702 Bright coef : 1.0588836676473121\n",
      "step 295 gen_loss : 2.4184706 disc_loss : 1.3571932 Bright coef : 0.9006984972280477\n",
      "step 296 gen_loss : 2.1641297 disc_loss : 1.2630272 Bright coef : 0.8\n",
      "step 297 gen_loss : 2.0378754 disc_loss : 1.4076685 Bright coef : 0.8\n",
      "step 298 gen_loss : 3.0537903 disc_loss : 1.7570343 Bright coef : 0.8\n",
      "step 299 gen_loss : 2.363798 disc_loss : 1.1480715 Bright coef : 0.8757293698348962\n",
      "step 300 gen_loss : 2.3969967 disc_loss : 1.3178031 Bright coef : 0.8\n",
      "step 301 gen_loss : 3.7658694 disc_loss : 1.0653989 Bright coef : 1.0581075099547164\n",
      "step 302 gen_loss : 2.342984 disc_loss : 1.2789037 Bright coef : 1.043560479421085\n",
      "step 303 gen_loss : 3.586726 disc_loss : 1.3148202 Bright coef : 0.8\n",
      "step 304 gen_loss : 2.1712983 disc_loss : 1.5878873 Bright coef : 1.1\n",
      "step 305 gen_loss : 3.3006275 disc_loss : 1.1999682 Bright coef : 1.1\n",
      "step 306 gen_loss : 3.1896198 disc_loss : 1.4391845 Bright coef : 0.8\n",
      "step 307 gen_loss : 4.59808 disc_loss : 1.403317 Bright coef : 1.1\n",
      "step 308 gen_loss : 4.2014475 disc_loss : 1.3511627 Bright coef : 0.8223621578256264\n",
      "step 309 gen_loss : 3.8501933 disc_loss : 1.1635857 Bright coef : 0.8634077277339097\n",
      "step 310 gen_loss : 2.6755943 disc_loss : 2.121687 Bright coef : 0.8039793991434983\n",
      "step 311 gen_loss : 2.0227766 disc_loss : 1.1869829 Bright coef : 0.8\n",
      "step 312 gen_loss : 3.3638706 disc_loss : 1.2694945 Bright coef : 0.8893430920179474\n",
      "step 313 gen_loss : 4.097116 disc_loss : 1.559838 Bright coef : 1.021788553477212\n",
      "step 314 gen_loss : 2.0620337 disc_loss : 1.2924595 Bright coef : 0.8\n",
      "step 315 gen_loss : 2.491168 disc_loss : 1.8119254 Bright coef : 1.023252615741107\n",
      "step 316 gen_loss : 2.1551878 disc_loss : 1.4082919 Bright coef : 0.9379639619027945\n",
      "step 317 gen_loss : 2.8351395 disc_loss : 1.4252752 Bright coef : 0.8\n",
      "step 318 gen_loss : 2.9624333 disc_loss : 1.3038894 Bright coef : 1.1\n",
      "step 319 gen_loss : 2.444864 disc_loss : 1.7640333 Bright coef : 0.8\n",
      "step 320 gen_loss : 2.72215 disc_loss : 1.5991732 Bright coef : 0.8\n",
      "step 321 gen_loss : 1.8042775 disc_loss : 1.52338 Bright coef : 0.8818219056863265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 322 gen_loss : 2.4306974 disc_loss : 1.5945268 Bright coef : 0.9937939706800487\n",
      "step 323 gen_loss : 2.8601208 disc_loss : 0.9363719 Bright coef : 1.1\n",
      "step 324 gen_loss : 2.7500231 disc_loss : 1.2261808 Bright coef : 0.9915304494063069\n",
      "step 325 gen_loss : 2.3237534 disc_loss : 1.527226 Bright coef : 0.8\n",
      "step 326 gen_loss : 2.3543108 disc_loss : 1.3239872 Bright coef : 1.1\n",
      "step 327 gen_loss : 2.2856803 disc_loss : 1.4581745 Bright coef : 1.041988210395898\n",
      "step 328 gen_loss : 2.9660494 disc_loss : 1.7128816 Bright coef : 1.1\n",
      "step 329 gen_loss : 2.1465485 disc_loss : 1.5395007 Bright coef : 1.1\n",
      "step 330 gen_loss : 2.4571924 disc_loss : 1.4575523 Bright coef : 0.879476675095967\n",
      "step 331 gen_loss : 2.5398555 disc_loss : 1.8145579 Bright coef : 0.80627471697812\n",
      "step 332 gen_loss : 3.3881993 disc_loss : 1.4526031 Bright coef : 1.0343841938195057\n",
      "step 333 gen_loss : 2.8737411 disc_loss : 1.193435 Bright coef : 0.8\n",
      "step 334 gen_loss : 2.3256195 disc_loss : 1.5581378 Bright coef : 0.8\n",
      "step 335 gen_loss : 1.8481108 disc_loss : 1.5009322 Bright coef : 1.0206302509700256\n",
      "step 336 gen_loss : 2.5969024 disc_loss : 1.5568671 Bright coef : 0.8\n",
      "step 337 gen_loss : 2.9110482 disc_loss : 1.225625 Bright coef : 0.8134382059590695\n",
      "step 338 gen_loss : 2.1773608 disc_loss : 1.3851938 Bright coef : 0.8\n",
      "step 339 gen_loss : 1.8469856 disc_loss : 1.1342046 Bright coef : 0.8\n",
      "step 340 gen_loss : 2.6661572 disc_loss : 1.3453081 Bright coef : 0.9456301095730666\n",
      "step 341 gen_loss : 4.007242 disc_loss : 1.5655352 Bright coef : 1.0793633290125024\n",
      "step 342 gen_loss : 2.4619415 disc_loss : 1.1363759 Bright coef : 0.9472033123932021\n",
      "step 343 gen_loss : 2.1027472 disc_loss : 2.0801015 Bright coef : 1.0988669379005878\n",
      "step 344 gen_loss : 2.4582875 disc_loss : 1.3459387 Bright coef : 1.1\n",
      "step 345 gen_loss : 2.9113438 disc_loss : 1.133748 Bright coef : 1.1\n",
      "step 346 gen_loss : 2.3332906 disc_loss : 1.1867903 Bright coef : 0.8\n",
      "step 347 gen_loss : 1.8293252 disc_loss : 1.4312503 Bright coef : 1.0601039219583006\n",
      "step 348 gen_loss : 4.1773043 disc_loss : 1.3684841 Bright coef : 1.0548210732168826\n",
      "step 349 gen_loss : 2.568502 disc_loss : 1.5249426 Bright coef : 1.1\n",
      "step 350 gen_loss : 2.2614336 disc_loss : 1.5371099 Bright coef : 0.9901263416178606\n",
      "step 351 gen_loss : 2.404082 disc_loss : 1.3016969 Bright coef : 0.8\n",
      "step 352 gen_loss : 1.7598345 disc_loss : 1.6377732 Bright coef : 0.9735384318742121\n",
      "step 353 gen_loss : 2.0996907 disc_loss : 1.3030932 Bright coef : 0.9514006720830558\n",
      "step 354 gen_loss : 1.7447739 disc_loss : 1.7789793 Bright coef : 0.8\n",
      "step 355 gen_loss : 3.6289568 disc_loss : 1.4214581 Bright coef : 1.1\n",
      "step 356 gen_loss : 3.1595979 disc_loss : 1.3699535 Bright coef : 0.9605181956054997\n",
      "step 357 gen_loss : 2.6039696 disc_loss : 1.4455848 Bright coef : 1.0591221868617327\n",
      "step 358 gen_loss : 3.0429113 disc_loss : 1.5788964 Bright coef : 1.0945190686941364\n",
      "step 359 gen_loss : 2.7424722 disc_loss : 1.236782 Bright coef : 1.1\n",
      "step 360 gen_loss : 2.6796532 disc_loss : 1.5853276 Bright coef : 0.976517990422819\n",
      "step 361 gen_loss : 3.271334 disc_loss : 1.8847735 Bright coef : 1.0682479606713884\n",
      "step 362 gen_loss : 2.2317536 disc_loss : 1.2240674 Bright coef : 1.1\n",
      "step 363 gen_loss : 2.7969112 disc_loss : 1.4117508 Bright coef : 0.8\n",
      "step 364 gen_loss : 2.8725913 disc_loss : 1.6052125 Bright coef : 0.8403613000171464\n",
      "step 365 gen_loss : 2.7211215 disc_loss : 1.3962765 Bright coef : 0.9516902741956195\n",
      "step 366 gen_loss : 2.1289825 disc_loss : 1.5897522 Bright coef : 0.8906330931849331\n",
      "step 367 gen_loss : 3.724574 disc_loss : 1.3018421 Bright coef : 0.8825831685770384\n",
      "step 368 gen_loss : 1.6362842 disc_loss : 1.7612591 Bright coef : 0.8899646838160314\n",
      "step 369 gen_loss : 1.9535556 disc_loss : 1.4918101 Bright coef : 0.8\n",
      "step 370 gen_loss : 1.8937458 disc_loss : 1.2069333 Bright coef : 0.8\n",
      "step 371 gen_loss : 3.3004541 disc_loss : 1.3378384 Bright coef : 1.1\n",
      "step 372 gen_loss : 2.640014 disc_loss : 1.6575804 Bright coef : 0.9798919299559902\n",
      "step 373 gen_loss : 2.3615544 disc_loss : 1.5386595 Bright coef : 0.8\n",
      "step 374 gen_loss : 2.3127995 disc_loss : 1.3053286 Bright coef : 0.8\n",
      "step 375 gen_loss : 4.002646 disc_loss : 1.267756 Bright coef : 1.0268891198573695\n",
      "step 376 gen_loss : 2.734283 disc_loss : 1.495779 Bright coef : 0.8738746548967812\n",
      "step 377 gen_loss : 2.7358487 disc_loss : 1.4940627 Bright coef : 0.8\n",
      "step 378 gen_loss : 1.6641581 disc_loss : 1.3208916 Bright coef : 0.8\n",
      "step 379 gen_loss : 4.8737555 disc_loss : 1.5275273 Bright coef : 1.1\n",
      "step 380 gen_loss : 2.684132 disc_loss : 1.4047287 Bright coef : 0.8725087466145531\n",
      "step 381 gen_loss : 2.4293466 disc_loss : 1.4068235 Bright coef : 0.8382736095843231\n",
      "step 382 gen_loss : 2.3194816 disc_loss : 1.4706373 Bright coef : 0.9481385953674998\n",
      "step 383 gen_loss : 1.3492618 disc_loss : 1.9377606 Bright coef : 0.9286181064069722\n",
      "step 384 gen_loss : 2.1703246 disc_loss : 1.4715046 Bright coef : 0.8957668792494697\n",
      "step 385 gen_loss : 2.5726702 disc_loss : 1.5477313 Bright coef : 0.8\n",
      "step 386 gen_loss : 3.076851 disc_loss : 1.4803319 Bright coef : 0.9513345309671186\n",
      "step 387 gen_loss : 3.2440915 disc_loss : 1.3824658 Bright coef : 1.0219328729976702\n",
      "step 388 gen_loss : 2.5813909 disc_loss : 1.8905131 Bright coef : 0.8\n",
      "step 389 gen_loss : 3.951675 disc_loss : 1.2594664 Bright coef : 1.073660647168676\n",
      "step 390 gen_loss : 2.3691041 disc_loss : 1.8128651 Bright coef : 1.1\n",
      "step 391 gen_loss : 3.0441122 disc_loss : 1.3581238 Bright coef : 0.9348263373987654\n",
      "step 392 gen_loss : 2.1082351 disc_loss : 1.260016 Bright coef : 0.8433611209054772\n",
      "step 393 gen_loss : 1.968156 disc_loss : 1.5163814 Bright coef : 0.8\n",
      "step 394 gen_loss : 2.5924904 disc_loss : 1.2949958 Bright coef : 0.8\n",
      "step 395 gen_loss : 2.0715177 disc_loss : 1.5624976 Bright coef : 0.8722531320225413\n",
      "step 396 gen_loss : 2.5057216 disc_loss : 1.3746583 Bright coef : 1.0614382049312174\n",
      "step 397 gen_loss : 2.949047 disc_loss : 1.6080012 Bright coef : 0.8\n",
      "step 398 gen_loss : 2.1582112 disc_loss : 1.6999886 Bright coef : 0.973692851287746\n",
      "step 399 gen_loss : 4.9379582 disc_loss : 1.296825 Bright coef : 1.1\n",
      "step 400 gen_loss : 2.100266 disc_loss : 1.2839608 Bright coef : 0.9599213154861732\n",
      "step 401 gen_loss : 2.8591506 disc_loss : 1.3306311 Bright coef : 0.9034457544863839\n",
      "step 402 gen_loss : 2.5099952 disc_loss : 1.3479176 Bright coef : 0.9682143085863275\n",
      "step 403 gen_loss : 2.9157882 disc_loss : 1.4144783 Bright coef : 0.8\n",
      "step 404 gen_loss : 2.6903772 disc_loss : 1.3931044 Bright coef : 0.9368610376621652\n",
      "step 405 gen_loss : 1.8617682 disc_loss : 1.1128515 Bright coef : 0.838292529777231\n",
      "step 406 gen_loss : 1.8065088 disc_loss : 1.3438077 Bright coef : 0.8\n",
      "step 407 gen_loss : 2.6997738 disc_loss : 1.361424 Bright coef : 0.9174426182048213\n",
      "step 408 gen_loss : 3.5015519 disc_loss : 1.0159156 Bright coef : 1.0766341945734457\n",
      "step 409 gen_loss : 5.915247 disc_loss : 1.2686222 Bright coef : 1.1\n",
      "step 410 gen_loss : 2.6357636 disc_loss : 1.1588649 Bright coef : 0.829850530717807\n",
      "step 411 gen_loss : 2.3969195 disc_loss : 1.8584299 Bright coef : 1.1\n",
      "step 412 gen_loss : 2.9081037 disc_loss : 1.7091889 Bright coef : 1.1\n",
      "step 413 gen_loss : 2.6031547 disc_loss : 1.7959869 Bright coef : 1.0524719976540307\n",
      "step 414 gen_loss : 2.5220306 disc_loss : 1.8349202 Bright coef : 0.8\n",
      "step 415 gen_loss : 2.8896294 disc_loss : 1.1248224 Bright coef : 0.8\n",
      "step 416 gen_loss : 2.2368846 disc_loss : 1.5120494 Bright coef : 0.8\n",
      "step 417 gen_loss : 5.2303925 disc_loss : 1.4469793 Bright coef : 1.0223913292080136\n",
      "step 418 gen_loss : 2.152637 disc_loss : 1.5098857 Bright coef : 0.9087343477865687\n",
      "step 419 gen_loss : 3.5476272 disc_loss : 1.4573426 Bright coef : 1.1\n",
      "step 420 gen_loss : 2.591881 disc_loss : 1.5262926 Bright coef : 1.0724100756115527\n",
      "step 421 gen_loss : 3.555854 disc_loss : 1.1899209 Bright coef : 0.8259679266886085\n",
      "step 422 gen_loss : 3.0841045 disc_loss : 1.5574915 Bright coef : 0.9890452353789947\n",
      "step 423 gen_loss : 1.6897932 disc_loss : 1.5479324 Bright coef : 1.1\n",
      "step 424 gen_loss : 1.8938605 disc_loss : 1.776607 Bright coef : 0.9384562860065164\n",
      "step 425 gen_loss : 2.1234758 disc_loss : 1.4408076 Bright coef : 0.8\n",
      "step 426 gen_loss : 3.402483 disc_loss : 1.4630426 Bright coef : 1.1\n",
      "step 427 gen_loss : 2.6047974 disc_loss : 1.8627183 Bright coef : 0.9412431208278954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 428 gen_loss : 2.2964268 disc_loss : 1.7216035 Bright coef : 0.9123044557254385\n",
      "step 429 gen_loss : 4.246685 disc_loss : 1.3544083 Bright coef : 0.961838930969243\n",
      "step 430 gen_loss : 2.3107448 disc_loss : 1.1750262 Bright coef : 1.1\n",
      "step 431 gen_loss : 1.8660212 disc_loss : 1.6423205 Bright coef : 0.8\n",
      "step 432 gen_loss : 2.241048 disc_loss : 1.562448 Bright coef : 0.8028476990179011\n",
      "step 433 gen_loss : 2.551671 disc_loss : 1.5326321 Bright coef : 0.8\n",
      "step 434 gen_loss : 2.9734676 disc_loss : 1.3548619 Bright coef : 0.8\n",
      "step 435 gen_loss : 2.276128 disc_loss : 1.8127162 Bright coef : 0.8\n",
      "step 436 gen_loss : 2.8739839 disc_loss : 1.1983607 Bright coef : 0.8\n",
      "step 437 gen_loss : 4.489882 disc_loss : 1.1917069 Bright coef : 1.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStart of epoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (i,))\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m (D,G, GEN_X, DISC_X, GEN_Y, DISC_Y, GanX, GanY, IdX, IdY, Cycle)\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGEN_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDISC_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGEN_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDISC_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataset_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m GLoss_Epochs\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(G))\n\u001b[1;32m     56\u001b[0m DLoss_Epochs\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(D))\n",
      "Cell \u001b[0;32mIn[7], line 101\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(GEN_X, DISC_X, GEN_Y, DISC_Y, dataset, batch)\u001b[0m\n\u001b[1;32m     99\u001b[0m bright \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0.9\u001b[39m,\u001b[38;5;241m0.2\u001b[39m),\u001b[38;5;241m0.8\u001b[39m,\u001b[38;5;241m1.1\u001b[39m)  \u001b[38;5;66;03m# (0.9,0.2),0.8,1.1)    (0.8,0.25),0.7,1.2) \u001b[39;00m\n\u001b[1;32m    100\u001b[0m img_X_B \u001b[38;5;241m=\u001b[39m change_Bright(img_X,bright)\n\u001b[0;32m--> 101\u001b[0m img_Y_B \u001b[38;5;241m=\u001b[39m \u001b[43mchange_Bright\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# train step, update loss, gradient\u001b[39;00m\n\u001b[1;32m    103\u001b[0m gen_loss_X, disc_loss_X, gen_loss_Y, disc_loss_Y, img_X_Y, img_Y_X, GANX, IDX, GANY, IDY, CYCLE \u001b[38;5;241m=\u001b[39m train_step(GEN_X, DISC_X, GEN_Y, DISC_Y, img_X_B, img_Y_B, batch)    \u001b[38;5;66;03m# tf.constant()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 70\u001b[0m, in \u001b[0;36mchange_Bright\u001b[0;34m(image, B)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchange_Bright\u001b[39m(image,B):        \u001b[38;5;66;03m#run_eagerly=True   ???\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mrgb2hsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m127.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m127.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     img[:,:,:,\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(img[:,:,:,\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m*\u001b[39mB,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m     72\u001b[0m     img \u001b[38;5;241m=\u001b[39m (hsv2rgb(img) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m127.5\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m127.5\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/skimage/_shared/utils.py:316\u001b[0m, in \u001b[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m channel_axis \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_axis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channel_axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m#       supporting a tuple of channel axes. Right now, only an\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m#       integer or a single-element tuple is supported, though.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(channel_axis):\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/skimage/color/colorconv.py:295\u001b[0m, in \u001b[0;36mrgb2hsv\u001b[0;34m(rgb, channel_axis)\u001b[0m\n\u001b[1;32m    291\u001b[0m out_s[delta \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# -- H channel\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# red is max\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m idx \u001b[38;5;241m=\u001b[39m (\u001b[43marr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mout_v\u001b[49m)\n\u001b[1;32m    296\u001b[0m out[idx, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (arr[idx, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m arr[idx, \u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m/\u001b[39m delta[idx]\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# green is max\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test Path\n",
    "path_test = 'test/test_set/test_set_Urban_Launcher_256'\n",
    "# Save path\n",
    "save_model = 'save_models/V_Urban_Launcher_Unet/V6_256_disc_downsize/'\n",
    "save_loss = 'loss/loss_V_Urban_Launcher_Unet/V6_256_disc_downsize/'\n",
    "#load\n",
    "#generator.load_weights(save_model+'gen_model_color_epoch_5.h5')\n",
    "#discriminator.load_weights(save_model+'dis_model_color_epoch_5.h5')\n",
    "\n",
    "EPOCHS_START=1\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE=1\n",
    "LR_G = 2e-4\n",
    "LR_D = 5e-5\n",
    "print(\"Learning Rate G : \"+str(LR_G)+\" Learning Rate D : \"+str(LR_D))\n",
    "GEN_X_optimizer = tf.keras.optimizers.Adam(LR_G, beta_1=0.5)    #2e-4\n",
    "DISC_X_optimizer = tf.keras.optimizers.Adam(LR_D, beta_1=0.5)  #2e-4\n",
    "\n",
    "GEN_Y_optimizer = tf.keras.optimizers.Adam(LR_G, beta_1=0.5)    #2e-4\n",
    "DISC_Y_optimizer = tf.keras.optimizers.Adam(LR_D, beta_1=0.5)  #2e-4\n",
    "#\n",
    "# Load dataset \n",
    "Dataset = tf.data.Dataset.load('./Dataset_wo_sky_256/V_Urban_Launcher/Dataset_VUL')\n",
    "#\n",
    "if EPOCHS_START!=1:\n",
    "    #load loss\n",
    "    GLoss_Epochs = list(np.load(save_loss+'Gloss_mean_5.npy')[0:EPOCHS_START])\n",
    "    DLoss_Epochs = list(np.load(save_loss+'Dloss_mean_5.npy')[0:EPOCHS_START])\n",
    "    TLoss_Epochs = list(np.load(save_loss+'Tloss_mean_5.npy')[0:EPOCHS_START])\n",
    "else:\n",
    "    GLoss_Epochs=[]\n",
    "    DLoss_Epochs=[]\n",
    "    TLoss_Epochs=[]\n",
    "    GANXLoss_Epochs=[]\n",
    "    GANYLoss_Epochs=[]\n",
    "    IDXLoss_Epochs=[]\n",
    "    IDYLoss_Epochs=[]\n",
    "    CYCLELoss_Epochs=[]\n",
    "#\n",
    "for i in range(EPOCHS_START,EPOCHS+1):\n",
    "    #\n",
    "    SEED = random.randint(0,10000)\n",
    "    print('Seed : ',SEED)\n",
    "    # SHUFFLE AND BATCH\n",
    "    Dataset_B = shuffle_and_batch(Dataset, 50, SEED)\n",
    "    #\n",
    "    #for j,L in enumerate(Dataset_B):\n",
    "    #    print('dataset_'+str(j)+' saved', tf.shape(L))\n",
    "    #print(j)\n",
    "    print(\"\\nStart of epoch %d\" % (i,))\n",
    "    #\n",
    "    # train model\n",
    "    (D,G, GEN_X, DISC_X, GEN_Y, DISC_Y, GanX, GanY, IdX, IdY, Cycle)=train(GEN_X, DISC_X, GEN_Y, DISC_Y, Dataset_B, batch=BATCH_SIZE)\n",
    "    \n",
    "    GLoss_Epochs.append(np.mean(G))\n",
    "    DLoss_Epochs.append(np.mean(D))\n",
    "    GANXLoss_Epochs.append(np.mean(GanX))\n",
    "    GANYLoss_Epochs.append(np.mean(GanY))\n",
    "    IDXLoss_Epochs.append(np.mean(IdX))\n",
    "    IDYLoss_Epochs.append(np.mean(IdY))\n",
    "    CYCLELoss_Epochs.append(np.mean(Cycle))\n",
    "    \n",
    "    DISC_X.save_weights(save_model+'disX_model_color_epoch_'+str(i)+'.h5')\n",
    "    GEN_X.save_weights(save_model+'genX_model_color_epoch_'+str(i)+'.h5')\n",
    "    DISC_Y.save_weights(save_model+'disY_model_color_epoch_'+str(i)+'.h5')\n",
    "    GEN_Y.save_weights(save_model+'genY_model_color_epoch_'+str(i)+'.h5')\n",
    "        \n",
    "    # test loss\n",
    "    AVG = test_loss_avg(GEN_X,DISC_Y,path_test)\n",
    "    TLoss_Epochs.append(AVG)\n",
    "    # Save loss\n",
    "    np.save(save_loss+'Dloss_mean_'+str(i)+'.npy',DLoss_Epochs)\n",
    "    np.save(save_loss+'Gloss_mean_'+str(i)+'.npy',GLoss_Epochs)\n",
    "    np.save(save_loss+'Tloss_mean_'+str(i)+'.npy',TLoss_Epochs)\n",
    "    \n",
    "    X_EPOCHS = np.arange(1,i+1, 1)\n",
    "    \n",
    "    print('X : ',X_EPOCHS)\n",
    "    print('G : ',GLoss_Epochs)\n",
    "    print('D : ',DLoss_Epochs)\n",
    "    print('T_avg : ',TLoss_Epochs)\n",
    "\n",
    "    plt.figure(figsize=(14,8))\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(X_EPOCHS, GLoss_Epochs, label='gan total loss : X -> Y')\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(X_EPOCHS, GANXLoss_Epochs, label='Gan loss : X -> Y')\n",
    "    plt.plot(X_EPOCHS, IDXLoss_Epochs, label='Identity loss : X -> Y')\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(X_EPOCHS, np.array(TLoss_Epochs), label='Test set loss : X -> Y')\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(X_EPOCHS, DLoss_Epochs,'orange', label='disc : X -> Y')\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(X_EPOCHS, GANYLoss_Epochs, label='Gan loss : Y -> X')\n",
    "    plt.plot(X_EPOCHS, IDYLoss_Epochs, label='Identity loss : Y -> X')    \n",
    "    plt.legend()\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.plot(X_EPOCHS,CYCLELoss_Epochs, label='Cycle loss')\n",
    "    plt.legend()\n",
    "    #\n",
    "    if (i%5 == 0):\n",
    "        plt.savefig(save_loss+'Loss_'+str(i)+'.png')\n",
    "        print('Saved')   \n",
    "    plt.show()             \n",
    "\n",
    "# save models\n",
    "print(\"gan_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 2, 2)\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)       [(None, 1024, 1024, 3)]      0         []                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.pad_36 (TFOpL  (None, 1026, 1026, 3)        0         ['input_12[0][0]']            \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " encoder_block_0 (Conv2D)    (None, 1024, 1024, 32)       896       ['tf.compat.v1.pad_36[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_98 (Ba  (None, 1024, 1024, 32)       128       ['encoder_block_0[0][0]']     \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_55 (LeakyReLU)  (None, 1024, 1024, 32)       0         ['batch_normalization_98[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " encoder_block_1 (Conv2D)    (None, 512, 512, 64)         18496     ['leaky_re_lu_55[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_99 (Ba  (None, 512, 512, 64)         256       ['encoder_block_1[0][0]']     \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_56 (LeakyReLU)  (None, 512, 512, 64)         0         ['batch_normalization_99[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " encoder_block_2 (Conv2D)    (None, 256, 256, 128)        73856     ['leaky_re_lu_56[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_100 (B  (None, 256, 256, 128)        512       ['encoder_block_2[0][0]']     \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_57 (LeakyReLU)  (None, 256, 256, 128)        0         ['batch_normalization_100[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " encoder_block_3 (Conv2D)    (None, 128, 128, 256)        295168    ['leaky_re_lu_57[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_101 (B  (None, 128, 128, 256)        1024      ['encoder_block_3[0][0]']     \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_58 (LeakyReLU)  (None, 128, 128, 256)        0         ['batch_normalization_101[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " encoder_block_4 (Conv2D)    (None, 64, 64, 512)          1180160   ['leaky_re_lu_58[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_102 (B  (None, 64, 64, 512)          2048      ['encoder_block_4[0][0]']     \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_59 (LeakyReLU)  (None, 64, 64, 512)          0         ['batch_normalization_102[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " bottleneck_conv2d (Conv2D)  (None, 32, 32, 512)          2359808   ['leaky_re_lu_59[0][0]']      \n",
      "                                                                                                  \n",
      " activation_65 (Activation)  (None, 32, 32, 512)          0         ['bottleneck_conv2d[0][0]']   \n",
      "                                                                                                  \n",
      " decoder_block_4 (Conv2DTra  (None, 64, 64, 512)          2359808   ['activation_65[0][0]']       \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_103 (B  (None, 64, 64, 512)          2048      ['decoder_block_4[0][0]']     \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " concatenate_31725 (Concate  (None, 64, 64, 1024)         0         ['batch_normalization_103[0][0\n",
      " nate)                                                              ]',                           \n",
      "                                                                     'leaky_re_lu_59[0][0]']      \n",
      "                                                                                                  \n",
      " activation_66 (Activation)  (None, 64, 64, 1024)         0         ['concatenate_31725[0][0]']   \n",
      "                                                                                                  \n",
      " decoder_block_3 (Conv2DTra  (None, 128, 128, 256)        2359552   ['activation_66[0][0]']       \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_104 (B  (None, 128, 128, 256)        1024      ['decoder_block_3[0][0]']     \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " concatenate_31726 (Concate  (None, 128, 128, 512)        0         ['batch_normalization_104[0][0\n",
      " nate)                                                              ]',                           \n",
      "                                                                     'leaky_re_lu_58[0][0]']      \n",
      "                                                                                                  \n",
      " activation_67 (Activation)  (None, 128, 128, 512)        0         ['concatenate_31726[0][0]']   \n",
      "                                                                                                  \n",
      " decoder_block_2 (Conv2DTra  (None, 256, 256, 128)        589952    ['activation_67[0][0]']       \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_105 (B  (None, 256, 256, 128)        512       ['decoder_block_2[0][0]']     \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " concatenate_31727 (Concate  (None, 256, 256, 256)        0         ['batch_normalization_105[0][0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nate)                                                              ]',                           \n",
      "                                                                     'leaky_re_lu_57[0][0]']      \n",
      "                                                                                                  \n",
      " activation_68 (Activation)  (None, 256, 256, 256)        0         ['concatenate_31727[0][0]']   \n",
      "                                                                                                  \n",
      " decoder_block_1 (Conv2DTra  (None, 512, 512, 64)         147520    ['activation_68[0][0]']       \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_106 (B  (None, 512, 512, 64)         256       ['decoder_block_1[0][0]']     \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " concatenate_31728 (Concate  (None, 512, 512, 128)        0         ['batch_normalization_106[0][0\n",
      " nate)                                                              ]',                           \n",
      "                                                                     'leaky_re_lu_56[0][0]']      \n",
      "                                                                                                  \n",
      " activation_69 (Activation)  (None, 512, 512, 128)        0         ['concatenate_31728[0][0]']   \n",
      "                                                                                                  \n",
      " decoder_block_0 (Conv2DTra  (None, 1024, 1024, 42)       48426     ['activation_69[0][0]']       \n",
      " nspose)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_107 (B  (None, 1024, 1024, 42)       168       ['decoder_block_0[0][0]']     \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " concatenate_31729 (Concate  (None, 1024, 1024, 74)       0         ['batch_normalization_107[0][0\n",
      " nate)                                                              ]',                           \n",
      "                                                                     'leaky_re_lu_55[0][0]']      \n",
      "                                                                                                  \n",
      " activation_70 (Activation)  (None, 1024, 1024, 74)       0         ['concatenate_31729[0][0]']   \n",
      "                                                                                                  \n",
      " tf.compat.v1.pad_37 (TFOpL  (None, 1026, 1026, 74)       0         ['activation_70[0][0]']       \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " 1st_last_layer (Conv2D)     (None, 1024, 1024, 42)       28014     ['tf.compat.v1.pad_37[0][0]'] \n",
      "                                                                                                  \n",
      " tf.compat.v1.pad_38 (TFOpL  (None, 1026, 1026, 42)       0         ['1st_last_layer[0][0]']      \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " 2nd_last_layer (Conv2D)     (None, 1024, 1024, 42)       15918     ['tf.compat.v1.pad_38[0][0]'] \n",
      "                                                                                                  \n",
      " tf.compat.v1.pad_39 (TFOpL  (None, 1026, 1026, 42)       0         ['2nd_last_layer[0][0]']      \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " 3rd_last_layer (Conv2D)     (None, 1024, 1024, 3)        1137      ['tf.compat.v1.pad_39[0][0]'] \n",
      "                                                                                                  \n",
      " activation_71 (Activation)  (None, 1024, 1024, 3)        0         ['3rd_last_layer[0][0]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9486687 (36.19 MB)\n",
      "Trainable params: 9482699 (36.17 MB)\n",
      "Non-trainable params: 3988 (15.58 KB)\n",
      "__________________________________________________________________________________________________\n",
      "[[[118 204 235]\n",
      "  [121 206 235]\n",
      "  [122 205 235]\n",
      "  ...\n",
      "  [116 203 233]\n",
      "  [120 205 235]\n",
      "  [120 209 236]]\n",
      "\n",
      " [[120 206 235]\n",
      "  [123 205 235]\n",
      "  [122 205 234]\n",
      "  ...\n",
      "  [111 199 231]\n",
      "  [112 201 234]\n",
      "  [115 204 234]]\n",
      "\n",
      " [[121 204 234]\n",
      "  [127 207 236]\n",
      "  [127 206 235]\n",
      "  ...\n",
      "  [119 203 232]\n",
      "  [119 202 234]\n",
      "  [119 206 234]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 28  38  39]\n",
      "  [ 28  37  39]\n",
      "  [ 29  35  39]\n",
      "  ...\n",
      "  [ 12  15  10]\n",
      "  [ 19  21  16]\n",
      "  [ 15  21  12]]\n",
      "\n",
      " [[ 23  31  33]\n",
      "  [ 25  35  35]\n",
      "  [ 25  29  32]\n",
      "  ...\n",
      "  [ 10  13   8]\n",
      "  [ 13  16  12]\n",
      "  [ 13  17  10]]\n",
      "\n",
      " [[ 27  39  37]\n",
      "  [ 28  38  37]\n",
      "  [ 29  37  38]\n",
      "  ...\n",
      "  [ 14  20  10]\n",
      "  [ 19  26  16]\n",
      "  [ 17  27  13]]]\n",
      "[[[191 240 251]\n",
      "  [197 242 251]\n",
      "  [191 239 250]\n",
      "  ...\n",
      "  [210 242 250]\n",
      "  [209 241 250]\n",
      "  [209 244 251]]\n",
      "\n",
      " [[193 242 251]\n",
      "  [202 244 251]\n",
      "  [192 240 250]\n",
      "  ...\n",
      "  [209 242 250]\n",
      "  [204 240 250]\n",
      "  [207 244 250]]\n",
      "\n",
      " [[194 239 250]\n",
      "  [204 243 251]\n",
      "  [197 239 250]\n",
      "  ...\n",
      "  [215 244 251]\n",
      "  [215 243 251]\n",
      "  [214 245 251]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 32  32  51]\n",
      "  [ 35  33  53]\n",
      "  [ 34  32  54]\n",
      "  ...\n",
      "  [ 33  33  52]\n",
      "  [ 36  46  73]\n",
      "  [ 34  42  59]]\n",
      "\n",
      " [[ 29  30  51]\n",
      "  [ 34  37  61]\n",
      "  [ 29  29  51]\n",
      "  ...\n",
      "  [ 24  30  50]\n",
      "  [ 27  37  62]\n",
      "  [ 27  37  57]]\n",
      "\n",
      " [[ 36  39  58]\n",
      "  [ 40  40  62]\n",
      "  [ 35  35  58]\n",
      "  ...\n",
      "  [ 31  39  53]\n",
      "  [ 35  53  76]\n",
      "  [ 35  50  64]]]\n",
      "[[[201 237 248]\n",
      "  [204 239 249]\n",
      "  [206 237 248]\n",
      "  ...\n",
      "  [104 174 212]\n",
      "  [120 182 216]\n",
      "  [111 178 212]]\n",
      "\n",
      " [[201 238 248]\n",
      "  [206 239 249]\n",
      "  [204 238 248]\n",
      "  ...\n",
      "  [ 92 157 202]\n",
      "  [107 169 213]\n",
      "  [ 99 167 207]]\n",
      "\n",
      " [[204 237 247]\n",
      "  [210 239 249]\n",
      "  [210 238 247]\n",
      "  ...\n",
      "  [119 178 212]\n",
      "  [126 180 216]\n",
      "  [112 174 208]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 74  85  96]\n",
      "  [ 77  82  95]\n",
      "  [ 82 101 112]\n",
      "  ...\n",
      "  [ 69  78  86]\n",
      "  [ 80  92  99]\n",
      "  [ 75  84  88]]\n",
      "\n",
      " [[ 71  75  90]\n",
      "  [ 77  84  95]\n",
      "  [ 76  91 101]\n",
      "  ...\n",
      "  [ 58  71  81]\n",
      "  [ 75  78  87]\n",
      "  [ 66  73  80]]\n",
      "\n",
      " [[ 72  85  94]\n",
      "  [ 76  82  92]\n",
      "  [ 76  98 104]\n",
      "  ...\n",
      "  [ 68  84  88]\n",
      "  [ 82  96 100]\n",
      "  [ 76  87  89]]]\n",
      "[[[189 232 246]\n",
      "  [193 235 247]\n",
      "  [196 234 246]\n",
      "  ...\n",
      "  [171 230 246]\n",
      "  [172 230 246]\n",
      "  [172 233 248]]\n",
      "\n",
      " [[189 234 247]\n",
      "  [197 236 247]\n",
      "  [196 235 246]\n",
      "  ...\n",
      "  [169 229 245]\n",
      "  [168 229 246]\n",
      "  [170 233 246]]\n",
      "\n",
      " [[193 233 246]\n",
      "  [200 236 248]\n",
      "  [203 235 247]\n",
      "  ...\n",
      "  [183 233 247]\n",
      "  [180 231 247]\n",
      "  [180 234 247]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 71  77  83]\n",
      "  [ 78  82  91]\n",
      "  [ 82  90  98]\n",
      "  ...\n",
      "  [175 168 170]\n",
      "  [182 175 181]\n",
      "  [176 166 164]]\n",
      "\n",
      " [[ 58  60  67]\n",
      "  [ 65  73  77]\n",
      "  [ 71  78  84]\n",
      "  ...\n",
      "  [173 169 169]\n",
      "  [190 177 181]\n",
      "  [176 170 165]]\n",
      "\n",
      " [[ 57  69  73]\n",
      "  [ 64  68  76]\n",
      "  [ 64  78  85]\n",
      "  ...\n",
      "  [176 171 163]\n",
      "  [188 180 178]\n",
      "  [179 173 164]]]\n",
      "[[[243 201 154]\n",
      "  [245 210 163]\n",
      "  [245 204 159]\n",
      "  ...\n",
      "  [196 238 249]\n",
      "  [197 238 249]\n",
      "  [197 240 250]]\n",
      "\n",
      " [[243 202 157]\n",
      "  [246 214 168]\n",
      "  [245 206 160]\n",
      "  ...\n",
      "  [195 238 248]\n",
      "  [194 237 248]\n",
      "  [196 240 249]]\n",
      "\n",
      " [[241 199 156]\n",
      "  [244 209 166]\n",
      "  [243 203 164]\n",
      "  ...\n",
      "  [206 240 249]\n",
      "  [205 239 249]\n",
      "  [205 241 250]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 27  22  40]\n",
      "  [ 27  23  40]\n",
      "  [ 26  18  38]\n",
      "  ...\n",
      "  [ 76  76  86]\n",
      "  [ 82  88  98]\n",
      "  [ 78  81  88]]\n",
      "\n",
      " [[ 22  19  32]\n",
      "  [ 25  24  37]\n",
      "  [ 22  18  31]\n",
      "  ...\n",
      "  [ 69  75  87]\n",
      "  [ 80  80  94]\n",
      "  [ 72  79  86]]\n",
      "\n",
      " [[ 26  24  39]\n",
      "  [ 28  24  39]\n",
      "  [ 25  20  37]\n",
      "  ...\n",
      "  [ 78  87  89]\n",
      "  [ 89  99 103]\n",
      "  [ 82  92  92]]]\n",
      "[[[188 242 252]\n",
      "  [191 243 252]\n",
      "  [195 242 251]\n",
      "  ...\n",
      "  [186 240 251]\n",
      "  [189 240 251]\n",
      "  [189 242 251]]\n",
      "\n",
      " [[189 243 251]\n",
      "  [194 244 251]\n",
      "  [192 241 251]\n",
      "  ...\n",
      "  [186 241 250]\n",
      "  [187 240 250]\n",
      "  [189 242 251]]\n",
      "\n",
      " [[191 242 251]\n",
      "  [197 243 251]\n",
      "  [201 242 251]\n",
      "  ...\n",
      "  [192 241 251]\n",
      "  [192 240 251]\n",
      "  [194 242 251]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 34  49  65]\n",
      "  [ 37  50  67]\n",
      "  [ 34  47  58]\n",
      "  ...\n",
      "  [ 64  87  94]\n",
      "  [ 68  98 101]\n",
      "  [ 67  92  94]]\n",
      "\n",
      " [[ 28  44  58]\n",
      "  [ 30  47  62]\n",
      "  [ 29  40  51]\n",
      "  ...\n",
      "  [ 43  69  78]\n",
      "  [ 51  76  86]\n",
      "  [ 49  75  82]]\n",
      "\n",
      " [[ 32  52  62]\n",
      "  [ 33  52  64]\n",
      "  [ 36  50  59]\n",
      "  ...\n",
      "  [ 50  79  84]\n",
      "  [ 57  92  95]\n",
      "  [ 56  86  90]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[230 247 252]\n",
      "  [232 248 252]\n",
      "  [234 248 252]\n",
      "  ...\n",
      "  [234 248 253]\n",
      "  [232 247 252]\n",
      "  [233 249 253]]\n",
      "\n",
      " [[231 248 252]\n",
      "  [235 249 252]\n",
      "  [234 248 252]\n",
      "  ...\n",
      "  [235 249 252]\n",
      "  [232 247 252]\n",
      "  [234 249 252]]\n",
      "\n",
      " [[231 247 252]\n",
      "  [234 249 252]\n",
      "  [236 248 252]\n",
      "  ...\n",
      "  [240 250 253]\n",
      "  [236 248 252]\n",
      "  [237 249 252]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[170 136 110]\n",
      "  [180 153 126]\n",
      "  [185 163 141]\n",
      "  ...\n",
      "  [213 184 164]\n",
      "  [215 187 171]\n",
      "  [210 182 161]]\n",
      "\n",
      " [[167 131 109]\n",
      "  [178 150 121]\n",
      "  [184 160 141]\n",
      "  ...\n",
      "  [208 182 157]\n",
      "  [217 187 167]\n",
      "  [209 184 156]]\n",
      "\n",
      " [[160 130  98]\n",
      "  [172 140 110]\n",
      "  [174 153 124]\n",
      "  ...\n",
      "  [213 189 158]\n",
      "  [220 195 171]\n",
      "  [214 190 159]]]\n",
      "[[[146 227 247]\n",
      "  [149 230 248]\n",
      "  [148 226 247]\n",
      "  ...\n",
      "  [238 237 240]\n",
      "  [237 233 237]\n",
      "  [236 233 235]]\n",
      "\n",
      " [[146 229 248]\n",
      "  [152 231 247]\n",
      "  [145 226 247]\n",
      "  ...\n",
      "  [239 238 240]\n",
      "  [237 233 237]\n",
      "  [237 236 236]]\n",
      "\n",
      " [[148 227 247]\n",
      "  [156 230 248]\n",
      "  [152 227 247]\n",
      "  ...\n",
      "  [241 241 244]\n",
      "  [238 237 241]\n",
      "  [237 236 237]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 39  34  40]\n",
      "  [ 38  31  39]\n",
      "  [ 38  30  37]\n",
      "  ...\n",
      "  [167 154 129]\n",
      "  [181 170 148]\n",
      "  [183 166 134]]\n",
      "\n",
      " [[ 26  24  29]\n",
      "  [ 29  26  31]\n",
      "  [ 30  25  29]\n",
      "  ...\n",
      "  [161 158 124]\n",
      "  [188 170 144]\n",
      "  [181 174 133]]\n",
      "\n",
      " [[ 31  31  35]\n",
      "  [ 32  27  34]\n",
      "  [ 34  30  34]\n",
      "  ...\n",
      "  [161 152 111]\n",
      "  [185 175 141]\n",
      "  [184 171 127]]]\n",
      "[[[119 212 241]\n",
      "  [122 215 242]\n",
      "  [120 211 240]\n",
      "  ...\n",
      "  [212 228 237]\n",
      "  [210 225 237]\n",
      "  [207 227 236]]\n",
      "\n",
      " [[119 214 242]\n",
      "  [122 215 241]\n",
      "  [120 211 240]\n",
      "  ...\n",
      "  [209 226 235]\n",
      "  [208 223 235]\n",
      "  [204 225 234]]\n",
      "\n",
      " [[119 211 240]\n",
      "  [127 216 242]\n",
      "  [121 210 240]\n",
      "  ...\n",
      "  [220 235 242]\n",
      "  [216 230 240]\n",
      "  [210 229 236]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 55  52  51]\n",
      "  [ 51  48  48]\n",
      "  [ 68  63  65]\n",
      "  ...\n",
      "  [  4   9  14]\n",
      "  [  7  13  24]\n",
      "  [  6  13  18]]\n",
      "\n",
      " [[ 61  54  55]\n",
      "  [ 62  57  55]\n",
      "  [ 75  67  68]\n",
      "  ...\n",
      "  [  5  10  17]\n",
      "  [  7  13  23]\n",
      "  [  7  13  19]]\n",
      "\n",
      " [[ 69  66  58]\n",
      "  [ 69  62  55]\n",
      "  [ 84  82  73]\n",
      "  ...\n",
      "  [  8  17  23]\n",
      "  [ 12  22  35]\n",
      "  [ 11  23  28]]]\n",
      "[[[ 68  75  47]\n",
      "  [ 72  82  52]\n",
      "  [ 67  71  51]\n",
      "  ...\n",
      "  [248 254 254]\n",
      "  [248 254 254]\n",
      "  [249 254 254]]\n",
      "\n",
      " [[ 71  74  43]\n",
      "  [ 72  79  47]\n",
      "  [ 75  73  50]\n",
      "  ...\n",
      "  [249 254 254]\n",
      "  [248 254 254]\n",
      "  [250 254 254]]\n",
      "\n",
      " [[ 77  80  52]\n",
      "  [ 84  86  57]\n",
      "  [ 78  77  58]\n",
      "  ...\n",
      "  [249 254 254]\n",
      "  [249 254 254]\n",
      "  [249 254 254]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 43  51  32]\n",
      "  [ 41  53  33]\n",
      "  [ 50  53  36]\n",
      "  ...\n",
      "  [149  81  42]\n",
      "  [161  99  49]\n",
      "  [155  93  44]]\n",
      "\n",
      " [[ 39  47  35]\n",
      "  [ 41  52  36]\n",
      "  [ 46  46  34]\n",
      "  ...\n",
      "  [120  57  29]\n",
      "  [136  80  39]\n",
      "  [133  78  36]]\n",
      "\n",
      " [[ 50  62  38]\n",
      "  [ 49  64  41]\n",
      "  [ 57  59  39]\n",
      "  ...\n",
      "  [136  70  35]\n",
      "  [150  95  45]\n",
      "  [149  93  44]]]\n",
      "[[[125 112 109]\n",
      "  [130 118 118]\n",
      "  [125 118 117]\n",
      "  ...\n",
      "  [105  98 118]\n",
      "  [111  98 115]\n",
      "  [106  92 109]]\n",
      "\n",
      " [[120 111 109]\n",
      "  [123 113 115]\n",
      "  [121 115 115]\n",
      "  ...\n",
      "  [113 106 129]\n",
      "  [117 105 125]\n",
      "  [113 100 120]]\n",
      "\n",
      " [[126 118 117]\n",
      "  [133 124 126]\n",
      "  [129 123 123]\n",
      "  ...\n",
      "  [ 96  97 121]\n",
      "  [105 103 126]\n",
      "  [102  98 119]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[159  80  60]\n",
      "  [162  79  63]\n",
      "  [154  76  59]\n",
      "  ...\n",
      "  [209 232 240]\n",
      "  [201 233 243]\n",
      "  [193 233 243]]\n",
      "\n",
      " [[152  71  55]\n",
      "  [158  74  58]\n",
      "  [148  71  56]\n",
      "  ...\n",
      "  [214 235 242]\n",
      "  [208 236 245]\n",
      "  [201 237 244]]\n",
      "\n",
      " [[154  76  57]\n",
      "  [159  75  61]\n",
      "  [152  74  57]\n",
      "  ...\n",
      "  [221 239 244]\n",
      "  [216 239 246]\n",
      "  [208 240 246]]]\n",
      "[[[205 230 242]\n",
      "  [210 233 243]\n",
      "  [212 233 243]\n",
      "  ...\n",
      "  [180 226 242]\n",
      "  [184 227 243]\n",
      "  [182 229 244]]\n",
      "\n",
      " [[205 231 242]\n",
      "  [211 234 243]\n",
      "  [212 233 242]\n",
      "  ...\n",
      "  [177 225 241]\n",
      "  [178 225 242]\n",
      "  [178 228 242]]\n",
      "\n",
      " [[206 231 242]\n",
      "  [215 235 244]\n",
      "  [216 236 244]\n",
      "  ...\n",
      "  [187 230 244]\n",
      "  [188 229 244]\n",
      "  [184 230 243]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 35  92 126]\n",
      "  [ 35  89 123]\n",
      "  [ 33  86 125]\n",
      "  ...\n",
      "  [119  79  41]\n",
      "  [160 109  69]\n",
      "  [163 118  68]]\n",
      "\n",
      " [[ 33  88 119]\n",
      "  [ 34  93 125]\n",
      "  [ 33  88 121]\n",
      "  ...\n",
      "  [106  74  35]\n",
      "  [148 101  59]\n",
      "  [153 116  64]]\n",
      "\n",
      " [[ 38 102 131]\n",
      "  [ 39  98 128]\n",
      "  [ 39 100 134]\n",
      "  ...\n",
      "  [116  82  38]\n",
      "  [158 113  62]\n",
      "  [164 122  65]]]\n",
      "[[[ 89 133 151]\n",
      "  [ 97 140 158]\n",
      "  [ 90 140 158]\n",
      "  ...\n",
      "  [206 133  80]\n",
      "  [212 142  87]\n",
      "  [212 143  87]]\n",
      "\n",
      " [[ 81 125 140]\n",
      "  [ 87 127 145]\n",
      "  [ 81 129 147]\n",
      "  ...\n",
      "  [203 132  77]\n",
      "  [210 139  83]\n",
      "  [210 142  84]]\n",
      "\n",
      " [[ 86 122 135]\n",
      "  [ 96 132 144]\n",
      "  [ 89 131 144]\n",
      "  ...\n",
      "  [204 132  78]\n",
      "  [210 139  85]\n",
      "  [211 142  85]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[232 245 250]\n",
      "  [235 247 250]\n",
      "  [236 247 250]\n",
      "  ...\n",
      "  [117 113 128]\n",
      "  [109 108 127]\n",
      "  [100 100 115]]\n",
      "\n",
      " [[232 245 250]\n",
      "  [235 247 250]\n",
      "  [236 247 250]\n",
      "  ...\n",
      "  [107 108 123]\n",
      "  [102 101 120]\n",
      "  [ 93  94 110]]\n",
      "\n",
      " [[234 247 250]\n",
      "  [237 248 251]\n",
      "  [238 248 251]\n",
      "  ...\n",
      "  [126 129 137]\n",
      "  [117 118 132]\n",
      "  [107 109 120]]]\n",
      "[[[ 50  53  51]\n",
      "  [ 54  55  59]\n",
      "  [ 51  54  53]\n",
      "  ...\n",
      "  [250 252 253]\n",
      "  [249 251 253]\n",
      "  [250 252 253]]\n",
      "\n",
      " [[ 50  55  52]\n",
      "  [ 53  53  56]\n",
      "  [ 51  55  54]\n",
      "  ...\n",
      "  [251 253 253]\n",
      "  [250 251 253]\n",
      "  [251 253 253]]\n",
      "\n",
      " [[ 51  53  53]\n",
      "  [ 55  56  60]\n",
      "  [ 55  57  58]\n",
      "  ...\n",
      "  [251 253 254]\n",
      "  [250 252 253]\n",
      "  [250 252 253]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 40  77  89]\n",
      "  [ 40  75  91]\n",
      "  [ 41  81  96]\n",
      "  ...\n",
      "  [197 193 192]\n",
      "  [202 196 197]\n",
      "  [194 191 188]]\n",
      "\n",
      " [[ 41  72  85]\n",
      "  [ 43  80  95]\n",
      "  [ 43  77  92]\n",
      "  ...\n",
      "  [191 192 190]\n",
      "  [202 197 199]\n",
      "  [193 194 189]]\n",
      "\n",
      " [[ 43  82  88]\n",
      "  [ 45  79  92]\n",
      "  [ 45  86  97]\n",
      "  ...\n",
      "  [199 200 192]\n",
      "  [209 205 202]\n",
      "  [201 202 194]]]\n",
      "[[[206 225 237]\n",
      "  [210 229 240]\n",
      "  [214 229 239]\n",
      "  ...\n",
      "  [228 233 238]\n",
      "  [227 229 237]\n",
      "  [224 229 234]]\n",
      "\n",
      " [[206 226 237]\n",
      "  [211 230 238]\n",
      "  [214 230 239]\n",
      "  ...\n",
      "  [227 231 236]\n",
      "  [226 228 234]\n",
      "  [223 229 233]]\n",
      "\n",
      " [[209 226 237]\n",
      "  [216 231 241]\n",
      "  [219 232 240]\n",
      "  ...\n",
      "  [236 241 244]\n",
      "  [233 236 241]\n",
      "  [229 234 236]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 36  97 136]\n",
      "  [ 38  99 138]\n",
      "  [ 39  99 141]\n",
      "  ...\n",
      "  [ 84  94 114]\n",
      "  [110 124 140]\n",
      "  [105 113 131]]\n",
      "\n",
      " [[ 35  93 133]\n",
      "  [ 39 105 144]\n",
      "  [ 40 103 143]\n",
      "  ...\n",
      "  [ 66  90 108]\n",
      "  [ 98 113 131]\n",
      "  [ 88 105 122]]\n",
      "\n",
      " [[ 40 106 141]\n",
      "  [ 43 106 143]\n",
      "  [ 44 109 147]\n",
      "  ...\n",
      "  [ 77  97 112]\n",
      "  [107 128 140]\n",
      "  [ 98 115 128]]]\n",
      "[[[182 233 247]\n",
      "  [186 235 248]\n",
      "  [189 233 247]\n",
      "  ...\n",
      "  [180 231 246]\n",
      "  [182 232 247]\n",
      "  [181 235 247]]\n",
      "\n",
      " [[182 234 247]\n",
      "  [188 236 248]\n",
      "  [187 234 247]\n",
      "  ...\n",
      "  [177 231 245]\n",
      "  [177 230 246]\n",
      "  [178 234 247]]\n",
      "\n",
      " [[184 233 247]\n",
      "  [192 236 248]\n",
      "  [193 234 247]\n",
      "  ...\n",
      "  [191 235 247]\n",
      "  [187 233 247]\n",
      "  [187 236 247]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 27  41  26]\n",
      "  [ 22  41  23]\n",
      "  [ 27  34  24]\n",
      "  ...\n",
      "  [ 74  80  88]\n",
      "  [ 82  94 104]\n",
      "  [ 78  87  93]]\n",
      "\n",
      " [[ 23  34  23]\n",
      "  [ 21  36  21]\n",
      "  [ 24  30  20]\n",
      "  ...\n",
      "  [ 67  77  89]\n",
      "  [ 80  83  96]\n",
      "  [ 71  82  88]]\n",
      "\n",
      " [[ 27  45  26]\n",
      "  [ 24  44  24]\n",
      "  [ 29  40  24]\n",
      "  ...\n",
      "  [ 76  90  92]\n",
      "  [ 88 102 106]\n",
      "  [ 82  96  95]]]\n",
      "[[[117 191 224]\n",
      "  [121 194 225]\n",
      "  [122 195 225]\n",
      "  ...\n",
      "  [124 193 221]\n",
      "  [126 194 224]\n",
      "  [127 198 224]]\n",
      "\n",
      " [[118 193 224]\n",
      "  [121 193 224]\n",
      "  [120 194 224]\n",
      "  ...\n",
      "  [120 187 220]\n",
      "  [120 189 222]\n",
      "  [122 193 223]]\n",
      "\n",
      " [[116 190 223]\n",
      "  [123 194 225]\n",
      "  [122 195 225]\n",
      "  ...\n",
      "  [124 193 222]\n",
      "  [125 193 224]\n",
      "  [126 197 223]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[237 135  66]\n",
      "  [237 140  68]\n",
      "  [235 125  63]\n",
      "  ...\n",
      "  [ 32  42  43]\n",
      "  [ 39  53  55]\n",
      "  [ 35  47  45]]\n",
      "\n",
      " [[235 131  61]\n",
      "  [236 140  67]\n",
      "  [234 126  61]\n",
      "  ...\n",
      "  [ 28  40  42]\n",
      "  [ 36  46  48]\n",
      "  [ 32  43  43]]\n",
      "\n",
      " [[239 148  74]\n",
      "  [240 152  76]\n",
      "  [237 141  72]\n",
      "  ...\n",
      "  [ 35  52  46]\n",
      "  [ 43  60  57]\n",
      "  [ 40  56  47]]]\n",
      "[[[146 215 239]\n",
      "  [150 217 241]\n",
      "  [153 217 239]\n",
      "  ...\n",
      "  [145 211 236]\n",
      "  [149 213 237]\n",
      "  [149 216 238]]\n",
      "\n",
      " [[144 216 240]\n",
      "  [150 217 240]\n",
      "  [149 216 239]\n",
      "  ...\n",
      "  [140 208 234]\n",
      "  [142 210 236]\n",
      "  [143 213 236]]\n",
      "\n",
      " [[146 214 239]\n",
      "  [155 219 241]\n",
      "  [156 217 239]\n",
      "  ...\n",
      "  [150 214 236]\n",
      "  [151 212 237]\n",
      "  [149 215 236]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 99  62  63]\n",
      "  [ 99  61  64]\n",
      "  [ 97  61  64]\n",
      "  ...\n",
      "  [ 32  34  46]\n",
      "  [ 42  46  64]\n",
      "  [ 37  40  50]]\n",
      "\n",
      " [[ 83  52  55]\n",
      "  [ 90  58  59]\n",
      "  [ 85  53  54]\n",
      "  ...\n",
      "  [ 26  32  47]\n",
      "  [ 35  38  57]\n",
      "  [ 31  36  49]]\n",
      "\n",
      " [[ 96  64  60]\n",
      "  [100  62  62]\n",
      "  [ 97  63  62]\n",
      "  ...\n",
      "  [ 37  46  55]\n",
      "  [ 49  60  74]\n",
      "  [ 44  53  59]]]\n",
      "[[[ 48  33  28]\n",
      "  [ 51  35  33]\n",
      "  [ 46  29  26]\n",
      "  ...\n",
      "  [182 246 253]\n",
      "  [183 246 253]\n",
      "  [186 248 253]]\n",
      "\n",
      " [[ 50  33  28]\n",
      "  [ 50  33  31]\n",
      "  [ 50  30  27]\n",
      "  ...\n",
      "  [183 247 253]\n",
      "  [181 246 253]\n",
      "  [186 248 253]]\n",
      "\n",
      " [[ 48  31  26]\n",
      "  [ 52  33  30]\n",
      "  [ 48  28  25]\n",
      "  ...\n",
      "  [192 247 253]\n",
      "  [188 246 253]\n",
      "  [193 248 253]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 38  92 110]\n",
      "  [ 45  97 121]\n",
      "  [ 40  95 114]\n",
      "  ...\n",
      "  [124 115 120]\n",
      "  [128 122 124]\n",
      "  [118 112 116]]\n",
      "\n",
      " [[ 38  85 104]\n",
      "  [ 45  99 121]\n",
      "  [ 41  94 113]\n",
      "  ...\n",
      "  [116 113 119]\n",
      "  [121 112 118]\n",
      "  [113 111 112]]\n",
      "\n",
      " [[ 37  90 107]\n",
      "  [ 44  92 115]\n",
      "  [ 39  94 111]\n",
      "  ...\n",
      "  [129 127 125]\n",
      "  [136 133 130]\n",
      "  [124 124 120]]]\n",
      "[[[164 227 245]\n",
      "  [166 229 246]\n",
      "  [168 227 245]\n",
      "  ...\n",
      "  [198 228 241]\n",
      "  [201 227 241]\n",
      "  [198 229 241]]\n",
      "\n",
      " [[164 228 245]\n",
      "  [167 229 245]\n",
      "  [167 228 245]\n",
      "  ...\n",
      "  [196 227 239]\n",
      "  [198 227 240]\n",
      "  [195 228 240]]\n",
      "\n",
      " [[165 226 244]\n",
      "  [171 229 246]\n",
      "  [169 227 244]\n",
      "  ...\n",
      "  [205 233 243]\n",
      "  [204 230 243]\n",
      "  [199 231 241]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[131 106  91]\n",
      "  [142 117 105]\n",
      "  [139 119 105]\n",
      "  ...\n",
      "  [143 115  92]\n",
      "  [163 140 120]\n",
      "  [157 133 108]]\n",
      "\n",
      " [[126  99  88]\n",
      "  [140 117 102]\n",
      "  [140 118 105]\n",
      "  ...\n",
      "  [125 109  81]\n",
      "  [151 124 103]\n",
      "  [143 129  98]]\n",
      "\n",
      " [[128 106  88]\n",
      "  [139 113  97]\n",
      "  [136 118 100]\n",
      "  ...\n",
      "  [130 112  79]\n",
      "  [155 138 108]\n",
      "  [150 134  99]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[160 231 248]\n",
      "  [161 232 248]\n",
      "  [167 232 248]\n",
      "  ...\n",
      "  [158 230 247]\n",
      "  [161 230 247]\n",
      "  [162 233 248]]\n",
      "\n",
      " [[161 232 248]\n",
      "  [163 233 248]\n",
      "  [167 233 248]\n",
      "  ...\n",
      "  [154 230 247]\n",
      "  [154 229 247]\n",
      "  [156 232 247]]\n",
      "\n",
      " [[159 230 247]\n",
      "  [163 232 248]\n",
      "  [168 232 248]\n",
      "  ...\n",
      "  [156 231 247]\n",
      "  [157 230 248]\n",
      "  [159 233 248]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 67  68  69]\n",
      "  [ 70  66  72]\n",
      "  [ 67  66  69]\n",
      "  ...\n",
      "  [214 200 191]\n",
      "  [227 212 217]\n",
      "  [226 221 216]]\n",
      "\n",
      " [[ 66  56  63]\n",
      "  [ 72  62  69]\n",
      "  [ 70  60  64]\n",
      "  ...\n",
      "  [198 181 172]\n",
      "  [218 199 200]\n",
      "  [215 205 196]]\n",
      "\n",
      " [[ 71  67  69]\n",
      "  [ 76  67  73]\n",
      "  [ 72  68  69]\n",
      "  ...\n",
      "  [211 197 180]\n",
      "  [226 209 204]\n",
      "  [223 215 201]]]\n",
      "[[[166 223 242]\n",
      "  [169 226 244]\n",
      "  [171 224 242]\n",
      "  ...\n",
      "  [217 231 241]\n",
      "  [218 229 240]\n",
      "  [215 230 239]]\n",
      "\n",
      " [[165 224 243]\n",
      "  [172 228 243]\n",
      "  [169 223 242]\n",
      "  ...\n",
      "  [216 231 239]\n",
      "  [217 228 238]\n",
      "  [214 230 238]]\n",
      "\n",
      " [[165 223 242]\n",
      "  [175 228 244]\n",
      "  [172 224 242]\n",
      "  ...\n",
      "  [226 238 244]\n",
      "  [222 233 242]\n",
      "  [219 233 239]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 76  55  55]\n",
      "  [ 78  58  62]\n",
      "  [ 80  53  58]\n",
      "  ...\n",
      "  [ 68  39  36]\n",
      "  [ 94  59  57]\n",
      "  [ 95  60  54]]\n",
      "\n",
      " [[ 61  45  43]\n",
      "  [ 68  55  53]\n",
      "  [ 72  52  50]\n",
      "  ...\n",
      "  [ 51  32  29]\n",
      "  [ 78  48  44]\n",
      "  [ 79  52  47]]\n",
      "\n",
      " [[ 64  50  45]\n",
      "  [ 69  52  51]\n",
      "  [ 69  49  48]\n",
      "  ...\n",
      "  [ 62  39  32]\n",
      "  [ 89  60  52]\n",
      "  [ 91  63  52]]]\n",
      "[[[170 219 239]\n",
      "  [176 222 240]\n",
      "  [181 223 239]\n",
      "  ...\n",
      "  [124 209 237]\n",
      "  [128 211 239]\n",
      "  [128 216 240]]\n",
      "\n",
      " [[171 221 239]\n",
      "  [177 223 239]\n",
      "  [180 223 239]\n",
      "  ...\n",
      "  [119 207 236]\n",
      "  [121 209 239]\n",
      "  [123 213 239]]\n",
      "\n",
      " [[171 219 238]\n",
      "  [180 224 240]\n",
      "  [186 226 241]\n",
      "  ...\n",
      "  [126 209 237]\n",
      "  [125 208 239]\n",
      "  [126 213 238]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 85  64  46]\n",
      "  [ 86  63  48]\n",
      "  [ 84  62  47]\n",
      "  ...\n",
      "  [ 76  48  43]\n",
      "  [101  67  65]\n",
      "  [ 95  64  56]]\n",
      "\n",
      " [[ 76  54  41]\n",
      "  [ 81  59  44]\n",
      "  [ 79  54  41]\n",
      "  ...\n",
      "  [ 58  43  38]\n",
      "  [ 84  57  54]\n",
      "  [ 81  59  53]]\n",
      "\n",
      " [[ 84  66  43]\n",
      "  [ 88  64  46]\n",
      "  [ 86  66  46]\n",
      "  ...\n",
      "  [ 77  58  46]\n",
      "  [106  79  68]\n",
      "  [101  77  61]]]\n",
      "[[[ 23  63  86]\n",
      "  [ 25  67  92]\n",
      "  [ 21  54  76]\n",
      "  ...\n",
      "  [ 97 137 157]\n",
      "  [ 93 145 165]\n",
      "  [106 157 174]]\n",
      "\n",
      " [[ 23  63  86]\n",
      "  [ 24  66  87]\n",
      "  [ 22  54  77]\n",
      "  ...\n",
      "  [ 91 127 147]\n",
      "  [ 85 136 155]\n",
      "  [ 98 145 164]]\n",
      "\n",
      " [[ 21  60  84]\n",
      "  [ 25  66  91]\n",
      "  [ 20  51  76]\n",
      "  ...\n",
      "  [ 97 127 143]\n",
      "  [ 93 132 150]\n",
      "  [103 142 157]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 74  84  73]\n",
      "  [ 80  92  80]\n",
      "  [ 80  95  87]\n",
      "  ...\n",
      "  [187 153  98]\n",
      "  [187 154  97]\n",
      "  [188 153  98]]\n",
      "\n",
      " [[ 75  79  71]\n",
      "  [ 83  92  78]\n",
      "  [ 85  92  85]\n",
      "  ...\n",
      "  [187 155  99]\n",
      "  [191 156 101]\n",
      "  [191 158 101]]\n",
      "\n",
      " [[ 73  84  68]\n",
      "  [ 81  93  75]\n",
      "  [ 88 102  86]\n",
      "  ...\n",
      "  [196 168 105]\n",
      "  [200 170 107]\n",
      "  [199 167 105]]]\n"
     ]
    }
   ],
   "source": [
    "# try on a test image\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "save_models = 'save_models/V_Urban_Launcher_Unet/V5_256/'\n",
    "image_shape=(1024,1024,3)\n",
    "\n",
    "num = 12\n",
    "\n",
    "# define the models\n",
    "generator = define_generator(image_shape)\n",
    "#load initial weights\n",
    "generator.load_weights(save_models+'genX_model_color_epoch_'+str(num)+'.h5') #4,5,13,25\n",
    "\n",
    "doss_image = 'test/val_set/org_1024'\n",
    "doss_gen = 'test/val_set/V_Urban_Launcher_Unet/V5/V5/gen_'+str(num)  #V17/gen_50'\n",
    "#doss_image ='dataset_color_transfer/test_sans_batch/V5_dts/new'\n",
    "#doss_gen = 'dataset_color_transfer/test_sans_batch/V5_dts/old'\n",
    "\n",
    "IMAGE=os.listdir(doss_image)\n",
    "IMAGE.sort()\n",
    "\n",
    "for file in IMAGE:\n",
    "    old = os.path.join(doss_image,file)\n",
    "    new = os.path.join(doss_gen,file+'_CGAN_'+str(num)+'.jpg') \n",
    "    #new = os.path.join(doss_gen,file) \n",
    "    X1=[]\n",
    "    # now read the input image files (difference from the code it is derived from that preloads all images)\n",
    "    pixels = Image.open(old).convert(\"RGB\")\n",
    "    pixels = np.array(pixels)\n",
    "    pixels = pixels[np.newaxis, ...]\n",
    "    #\n",
    "    img_fake = generator(pixels, training=False)\n",
    "    #print(tf.shape(img_fake))\n",
    "    img_fake = img_fake[0].numpy() * 127.5 + 127.5\n",
    "    img_fake= img_fake.astype(np.uint8)\n",
    "    print(img_fake)\n",
    "    image_reco=Image.fromarray(img_fake)\n",
    "    #image_reco.show()\n",
    "    #cv2.imwrite('test/image_00000168_gan_color.png',img.astype(int))\n",
    "    image_reco = image_reco.save(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source**\n",
    "\n",
    "https://github.com/soumith/ganhacks/issues/14\n",
    "https://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/\n",
    "\n",
    "I think the discriminator got too strong relative to the generator. Beyond this point, the generator finds it almost impossible to fool the discriminator, hence the increase in it's loss. I'm facing a similar problem.\n",
    "\n",
    "Probably, the problem is that the discriminator overfit. One of the reasons leading to this is following thing:\n",
    "discriminator may \"notice\" that images from true distribution is a matrix of numbers of the form n/255. So, adding gaussian noise to the input images may help to avoid the problem.\n",
    "It helps in my case.\n",
    "\n",
    "Probably, the problem is that the discriminator overfit. One of the reasons leading to this is following thing:\n",
    "discriminator may \"notice\" that images from true distribution is a matrix of numbers of the form n/255. So, adding gaussian noise to the input images may help to avoid the problem.\n",
    "It helps in my case.\n",
    "\n",
    "https://imatge.upc.edu/web/sites/default/files/pub/xTarres21.pdf\n",
    "\n",
    "Dropout :\n",
    "https://saturncloud.io/blog/how-to-properly-use-dropout-in-tensorflow-a-guide-for-data-scientists/\n",
    "\n",
    "Filters :\n",
    "https://datascience.stackexchange.com/questions/55545/in-cnn-why-do-we-increase-the-number-of-filters-in-deeper-convolution-layers-fo\n",
    "\n",
    "Loss study :\n",
    "https://arxiv.org/pdf/2204.02980.pdf\n",
    "\n",
    "http://cs231n.stanford.edu/reports/2017/pdfs/302.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1703.10593.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1603.08511.pdf\n",
    "\n",
    "\n",
    "\n",
    "Training indicating whether the layer should behave in training mode or in inference mode.\n",
    "\n",
    "    training=True: The layer will normalize its inputs using the mean and variance of the current batch of inputs.\n",
    "\n",
    "    training=False: The layer will normalize its inputs using the mean and variance of its moving statistics, learned during training.\n",
    "\n",
    "Usually in inference mode training=False, but in some networks such as pix2pix_cGAN At both times of inference and training, training=True.\n",
    "\n",
    "Seed counter tf : https://saturncloud.io/blog/tensorflow-different-results-with-the-same-random-seed/\n",
    "\n",
    "\n",
    "**CYCLE GAN :** \n",
    "\n",
    "https://www.tensorflow.org/tutorials/generative/cyclegan\n",
    "https://arxiv.org/pdf/1703.10593.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimizer weights\n",
    "#symbolic_weights = getattr(discriminator.optimizer, 'weights')\n",
    "#weight_values = K.batch_get_value(symbolic_weights)  # or symbolic_weights.numpy()\n",
    "# Then store .npy\n",
    "#\n",
    "# Load optimizer weights\n",
    "#weight_values = np.load(...)\n",
    "#model.optimizer.set_weights(weight_values)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
